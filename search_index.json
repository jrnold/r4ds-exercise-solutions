[
["index.html", "R for Data Science: Exercise Solutions Welcome Acknowledgments License", " R for Data Science: Exercise Solutions Jeffrey B. Arnold July 19, 2020 Welcome This book contains the exercise solutions for the book R for Data Science, by Hadley Wickham and Garret Grolemund (Wickham and Grolemund 2017). R for Data Science itself is available online at r4ds.had.co.nz, and physical copy is published by O’Reilly Media and available from amazon. Acknowledgments These solutions have benefited from many contributors. A special thanks to: Garrett Grolemund and Hadley Wickham for writing the truly fantastic R for Data Science, without whom these solutions would not exist—literally. @dongzhuoer and @cfgauss for careful readings of the book and noticing numerous issues and proposing fixes. Thank you to all of those who contributed issues or pull-requests on GitHub (in alphabetical order): @adamblake, @benherbertson, @bhishanpdl, @bob100000000000, @carajoos, @chrisyeh96, @clemonsa, @daczarne, @dcgreaves, @decoursin, @dependabot[bot], @dongzhuoer, @dvanic, @edavishydro, @eric-k-zhu, @GoldbergData, @gvwilson, @henrikmidtiby, @ihagerman, @JamesCuster, @jdblischak, @jhoeting, @jlbeaudry, @jmclawson, @kxchia1, @liuminzhao, @lopierra, @martinruhle, @matthewlock91, @mgeard, @mjones01, @mroviras, @mugpeng, @mvhone, @neander09, @nickcorona, @nielsenmarkus11, @nzxwang, @qichun-dai, @r2ressler, @RandallEW, @rbjanis, @ricardosasso, @Shurakai, @TheMksConnection, @timothydobbins, @tinhb92, @vzei, @xiaoouwang, @xinrui112, and@zidra Thank you to all of you who contributed annotations on hypothes.is (in alphabetical order): @electricdinosaurs, and @inkish. For another set of solutions for and notes on R for Data Science see Yet Another ‘R for Data Science’ Study Guide by Bryan Shalloway. License This work is licensed under a Creative Commons Attribution 4.0 International License. References "],
["introduction.html", "1 Introduction How this book is organized Prerequisites Bugs/Contributing Colophon", " 1 Introduction How this book is organized The book is divided into sections in with the same numbers and titles as those in R for Data Science. Not all sections have exercises. Those sections without exercises have placeholder text indicating that there are no exercises. The text for each exercise is followed by the solution. Like R for Data Science, packages used in each chapter are loaded in a code chunk at the start of the chapter in a section titled “Prerequisites”. If exercises depend on code in a section of R for Data Science it is either provided before the exercises or within the exercise solution. If a package is used infrequently in solutions it may not be loaded, and functions using it will be called using the package name followed by two colons, as in dplyr::mutate() (see the R for Data Science Introduction). The double colon may also be used to be explicit about the package from which a function comes. Prerequisites This book is a complement to, not a substitute of, R for Data Science. It only provides the exercise solutions for it. See the R for Data Science prerequisites. Additional, the solutions use several packages that are not used in R4DS. You can install all packages required to run the code in this book with the following line of code. devtools::install_github(&quot;jrnold/r4ds-exercise-solutions&quot;) Bugs/Contributing If you find any typos, errors in the solutions, have an alternative solution, or think the solution could be improved, I would love your contributions. The best way to contribute is through GitHub. Please open an issue at https://github.com/jrnold/r4ds-exercise-solutions/issues or a pull request at https://github.com/jrnold/r4ds-exercise-solutions/pulls. Colophon HTML and PDF versions of this book are available at https://jrnold.github.io/r4ds-exercise-solutions. The book is powered by bookdown which makes it easy to turn R markdown files into HTML, PDF, and EPUB. The source of this book is available on GitHub at https://github.com/jrnold/r4ds-exercise-solutions. This book was built from commit f0d0f0d. This book was built with these R packages. devtools::session_info(&quot;r4ds.exercise.solutions&quot;) #&gt; ─ Session info ─────────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 4.0.0 (2020-04-24) #&gt; os Ubuntu 16.04.6 LTS #&gt; system x86_64, linux-gnu #&gt; ui X11 #&gt; language en_US.UTF-8 #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz UTC #&gt; date 2020-07-19 #&gt; #&gt; ─ Packages ─────────────────────────────────────────────────────────────────── #&gt; ! package * version date lib source #&gt; R r4ds.exercise.solutions &lt;NA&gt; &lt;NA&gt; [?] &lt;NA&gt; #&gt; #&gt; [1] /home/travis/R/Library #&gt; [2] /usr/local/lib/R/site-library #&gt; [3] /home/travis/R-bin/lib/R/library #&gt; #&gt; R ── Package was removed from disk. "],
["explore-intro.html", "2 Introduction", " 2 Introduction No exercises "],
["data-visualisation.html", "3 Data visualisation 3.1 Introduction 3.2 First steps 3.3 Aesthetic mappings 3.4 Common problems 3.5 Facets 3.6 Geometric objects 3.7 Statistical transformations 3.8 Position adjustments 3.9 Coordinate systems 3.10 The layered grammar of graphics", " 3 Data visualisation 3.1 Introduction library(&quot;tidyverse&quot;) 3.2 First steps Exercise 3.2.1 Run ggplot(data = mpg) what do you see? ggplot(data = mpg) This code creates an empty plot. The ggplot() function creates the background of the plot, but since no layers were specified with geom function, nothing is drawn. Exercise 3.2.2 How many rows are in mpg? How many columns? There are 234 rows and 11 columns in the mpg data frame. nrow(mpg) #&gt; [1] 234 ncol(mpg) #&gt; [1] 11 The glimpse() function also displays the number of rows and columns in a data frame. glimpse(mpg) #&gt; Rows: 234 #&gt; Columns: 11 #&gt; $ manufacturer &lt;chr&gt; &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, … #&gt; $ model &lt;chr&gt; &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4 quattro&quot;, … #&gt; $ displ &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2… #&gt; $ year &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 20… #&gt; $ cyl &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8,… #&gt; $ trans &lt;chr&gt; &quot;auto(l5)&quot;, &quot;manual(m5)&quot;, &quot;manual(m6)&quot;, &quot;auto(av)&quot;, &quot;aut… #&gt; $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;… #&gt; $ cty &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, … #&gt; $ hwy &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, … #&gt; $ fl &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;… #&gt; $ class &lt;chr&gt; &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;… Exercise 3.2.3 What does the drv variable describe? Read the help for ?mpg to find out. The drv variable is a categorical variable which categorizes cars into front-wheels, rear-wheels, or four-wheel drive.1 Value Description &quot;f&quot; front-wheel drive &quot;r&quot; rear-wheel drive &quot;4&quot; four-wheel drive Exercise 3.2.4 Make a scatter plot of hwy vs. cyl. ggplot(mpg, aes(x = cyl, y = hwy)) + geom_point() Exercise 3.2.5 What happens if you make a scatter plot of class vs drv? Why is the plot not useful? The resulting scatterplot has only a few points. ggplot(mpg, aes(x = class, y = drv)) + geom_point() A scatter plot is not a useful display of these variables since both drv and class are categorical variables. Since categorical variables typically take a small number of values, there are a limited number of unique combinations of (x, y) values that can be displayed. In this data, drv takes 3 values and class takes 7 values, meaning that there are only 21 values that could be plotted on a scatterplot of drv vs. class. In this data, there 12 values of (drv, class) are observed. count(mpg, drv, class) #&gt; # A tibble: 12 x 3 #&gt; drv class n #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 4 compact 12 #&gt; 2 4 midsize 3 #&gt; 3 4 pickup 33 #&gt; 4 4 subcompact 4 #&gt; 5 4 suv 51 #&gt; 6 f compact 35 #&gt; # … with 6 more rows A simple scatter plot does not show how many observations there are for each (x, y) value. As such, scatterplots work best for plotting a continuous x and a continuous y variable, and when all (x, y) values are unique. Warning: The following code uses functions introduced in a later section. Come back to this after reading section 7.5.2, which introduces methods for plotting two categorical variables. The first is geom_count() which is similar to a scatterplot but uses the size of the points to show the number of observations at an (x, y) point. ggplot(mpg, aes(x = class, y = drv)) + geom_count() The second is geom_tile() which uses a color scale to show the number of observations with each (x, y) value. mpg %&gt;% count(class, drv) %&gt;% ggplot(aes(x = class, y = drv)) + geom_tile(mapping = aes(fill = n)) In the previous plot, there are many missing tiles. These missing tiles represent unobserved combinations of class and drv values. These missing values are not unknown, but represent values of (class, drv) where n = 0. The complete() function in the tidyr package adds new rows to a data frame for missing combinations of columns. The following code adds rows for missing combinations of class and drv and uses the fill argument to set n = 0 for those new rows. mpg %&gt;% count(class, drv) %&gt;% complete(class, drv, fill = list(n = 0)) %&gt;% ggplot(aes(x = class, y = drv)) + geom_tile(mapping = aes(fill = n)) 3.3 Aesthetic mappings Exercise 3.3.1 What’s gone wrong with this code? Why are the points not blue? ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = &quot;blue&quot;)) The argumentcolour = &quot;blue&quot; is included within the mapping argument, and as such, it is treated as an aesthetic, which is a mapping between a variable and a value. In the expression, colour = &quot;blue&quot;, &quot;blue&quot; is interpreted as a categorical variable which only takes a single value &quot;blue&quot;. If this is confusing, consider how colour = 1:234 and colour = 1 are interpreted by aes(). The following code does produces the expected result. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), colour = &quot;blue&quot;) Exercise 3.3.2 Which variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg? The following list contains the categorical variables in mpg: manufacturer model trans drv fl class The following list contains the continuous variables in mpg: displ year cyl cty hwy In the printed data frame, angled brackets at the top of each column provide type of each variable. mpg #&gt; # A tibble: 234 x 11 #&gt; manufacturer model displ year cyl trans drv cty hwy fl class #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compa… #&gt; 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compa… #&gt; 3 audi a4 2 2008 4 manual(m6) f 20 31 p compa… #&gt; 4 audi a4 2 2008 4 auto(av) f 21 30 p compa… #&gt; 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compa… #&gt; 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compa… #&gt; # … with 228 more rows Those with &lt;chr&gt; above their columns are categorical, while those with &lt;dbl&gt; or &lt;int&gt; are continuous. The exact meaning of these types will be discussed in “Chapter 15: Vectors”. glimpse() is another function that concisely displays the type of each column in the data frame: glimpse(mpg) #&gt; Rows: 234 #&gt; Columns: 11 #&gt; $ manufacturer &lt;chr&gt; &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, … #&gt; $ model &lt;chr&gt; &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4 quattro&quot;, … #&gt; $ displ &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2… #&gt; $ year &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 20… #&gt; $ cyl &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8,… #&gt; $ trans &lt;chr&gt; &quot;auto(l5)&quot;, &quot;manual(m5)&quot;, &quot;manual(m6)&quot;, &quot;auto(av)&quot;, &quot;aut… #&gt; $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;… #&gt; $ cty &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, … #&gt; $ hwy &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, … #&gt; $ fl &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;… #&gt; $ class &lt;chr&gt; &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;… For those lists, I considered any variable that was non-numeric was considered categorical and any variable that was numeric was considered continuous. This largely corresponds to the heuristics ggplot() uses for will interpreting variables as discrete or continuous. However, this definition of continuous vs. categorical misses several important cases. Of the numeric variables, year and cyl (cylinders) clearly take on discrete values. The variables cty and hwy are stored as integers (int) so they only take on a discrete values. Even though displ has In some sense, due to measurement and computational constraints all numeric variables are discrete (). But unlike the categorical variables, it is possible to add and subtract these numeric variables in a meaningful way. The typology of levels of measurement is one such typology of data types. In this case the R data types largely encode the semantics of the variables; e.g. integer variables are stored as integers, categorical variables with no order are stored as character vectors and so on. However, that is not always the case. Instead, the data could have stored the categorical class variable as an integer with values 1–7, where the documentation would note that 1 = “compact”, 2 = “midsize”, and so on.2 Even though this integer vector could be added, multiplied, subtracted, and divided, those operations would be meaningless. Fundamentally, categorizing variables as “discrete”, “continuous”, “ordinal”, “nominal”, “categorical”, etc. is about specifying what operations can be performed on the variables. Discrete variables support counting and calculating the mode. Variables with an ordering support sorting and calculating quantiles. Variables that have an interval scale support addition and subtraction and operations such as taking the mean that rely on these primitives. In this way, the types of data or variables types is an information class system, something that is beyond the scope of R4DS but discussed in Advanced R. Exercise 3.3.3 Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables? The variable cty, city highway miles per gallon, is a continuous variable. ggplot(mpg, aes(x = displ, y = hwy, colour = cty)) + geom_point() Instead of using discrete colors, the continuous variable uses a scale that varies from a light to dark blue color. ggplot(mpg, aes(x = displ, y = hwy, size = cty)) + geom_point() When mapped to size, the sizes of the points vary continuously as a function of their size. ggplot(mpg, aes(x = displ, y = hwy, shape = cty)) + geom_point() #&gt; Error: A continuous variable can not be mapped to shape When a continuous value is mapped to shape, it gives an error. Though we could split a continuous variable into discrete categories and use a shape aesthetic, this would conceptually not make sense. A numeric variable has an order, but shapes do not. It is clear that smaller points correspond to smaller values, or once the color scale is given, which colors correspond to larger or smaller values. But it is not clear whether a square is greater or less than a circle. Exercise 3.3.4 What happens if you map the same variable to multiple aesthetics? ggplot(mpg, aes(x = displ, y = hwy, colour = hwy, size = displ)) + geom_point() In the above plot, hwy is mapped to both location on the y-axis and color, and displ is mapped to both location on the x-axis and size. The code works and produces a plot, even if it is a bad one. Mapping a single variable to multiple aesthetics is redundant. Because it is redundant information, in most cases avoid mapping a single variable to multiple aesthetics. Exercise 3.3.5 What does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point) Stroke changes the size of the border for shapes (21-25). These are filled shapes in which the color and size of the border can differ from that of the filled interior of the shape. For example ggplot(mtcars, aes(wt, mpg)) + geom_point(shape = 21, colour = &quot;black&quot;, fill = &quot;white&quot;, size = 5, stroke = 5) Exercise 3.3.6 What happens if you map an aesthetic to something other than a variable name, like aes(colour = displ &lt; 5)? ggplot(mpg, aes(x = displ, y = hwy, colour = displ &lt; 5)) + geom_point() Aesthetics can also be mapped to expressions like displ &lt; 5. The ggplot() function behaves as if a temporary variable was added to the data with values equal to the result of the expression. In this case, the result of displ &lt; 5 is a logical variable which takes values of TRUE or FALSE. This also explains why, in Exercise 3.3.1, the expression colour = &quot;blue&quot; created a categorical variable with only one category: “blue”. 3.4 Common problems No exercises 3.5 Facets Exercise 3.5.1 What happens if you facet on a continuous variable? Let’s see. ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() + facet_grid(. ~ cty) The continuous variable is converted to a categorical variable, and the plot contains a facet for each distinct value. Exercise 3.5.2 What do the empty cells in plot with facet_grid(drv ~ cyl) mean? How do they relate to this plot? ggplot(data = mpg) + geom_point(mapping = aes(x = drv, y = cyl)) ggplot(data = mpg) + geom_point(mapping = aes(x = hwy, y = cty)) + facet_grid(drv ~ cyl) The empty cells (facets) in this plot are combinations of drv and cyl that have no observations. These are the same locations in the scatter plot of drv and cyl that have no points. ggplot(data = mpg) + geom_point(mapping = aes(x = drv, y = cyl)) Exercise 3.5.3 What plots does the following code make? What does . do? The symbol . ignores that dimension when faceting. For example, drv ~ . facet by values of drv on the y-axis. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(drv ~ .) While, . ~ cyl will facet by values of cyl on the x-axis. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(. ~ cyl) Exercise 3.5.4 Take the first faceted plot in this section: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_wrap(~class, nrow = 2) What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset? In the following plot the class variable is mapped to color. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = class)) Advantages of encoding class with facets instead of color include the ability to encode more distinct categories. For me, it is difficult to distinguish between the colors of &quot;midsize&quot; and &quot;minivan&quot;. Given human visual perception, the max number of colors to use when encoding unordered categorical (qualitative) data is nine, and in practice, often much less than that. Displaying observations from different categories on different scales makes it difficult to directly compare values of observations across categories. However, it can make it easier to compare the shape of the relationship between the x and y variables across categories. Disadvantages of encoding the class variable with facets instead of the color aesthetic include the difficulty of comparing the values of observations between categories since the observations for each category are on different plots. Using the same x- and y-scales for all facets makes it easier to compare values of observations across categories, but it is still more difficult than if they had been displayed on the same plot. Since encoding class within color also places all points on the same plot, it visualizes the unconditional relationship between the x and y variables; with facets, the unconditional relationship is no longer visualized since the points are spread across multiple plots. The benefit of encoding a variable with facetting over encoding it with color increase in both the number of points and the number of categories. With a large number of points, there is often overlap. It is difficult to handle overlapping points with different colors color. Jittering will still work with color. But jittering will only work well if there are few points and the classes do not overlap much, otherwise, the colors of areas will no longer be distinct, and it will be hard to pick out the patterns of different categories visually. Transparency (alpha) does not work well with colors since the mixing of overlapping transparent colors will no longer represent the colors of the categories. Binning methods already use color to encode the density of points in the bin, so color cannot be used to encode categories. As the number of categories increases, the difference between colors decreases, to the point that the color of categories will no longer be visually distinct. Exercise 3.5.5 Read ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol variables? The arguments nrow (ncol) determines the number of rows (columns) to use when laying out the facets. It is necessary since facet_wrap() only facets on one variable. The nrow and ncol arguments are unnecessary for facet_grid() since the number of unique values of the variables specified in the function determines the number of rows and columns. Exercise 3.5.6 When using facet_grid() you should usually put the variable with more unique levels in the columns. Why? There will be more space for columns if the plot is laid out horizontally (landscape). 3.6 Geometric objects Exercise 3.6.1 What geom would you use to draw a line chart? A boxplot? A histogram? An area chart? line chart: geom_line() boxplot: geom_boxplot() histogram: geom_histogram() area chart: geom_area() Exercise 3.6.2 Run this code in your head and predict what the output will look like. Then, run the code in R and check your predictions. ggplot(data = mpg, mapping = aes(x = displ, y = hwy, colour = drv)) + geom_point() + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; This code produces a scatter plot with displ on the x-axis, hwy on the y-axis, and the points colored by drv. There will be a smooth line, without standard errors, fit through each drv group. ggplot(data = mpg, mapping = aes(x = displ, y = hwy, colour = drv)) + geom_point() + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Exercise 3.6.3 What does show.legend = FALSE do? What happens if you remove it? Why do you think I used it earlier in the chapter? The theme option show.legend = FALSE hides the legend box. Consider this example earlier in the chapter. ggplot(data = mpg) + geom_smooth( mapping = aes(x = displ, y = hwy, colour = drv), show.legend = FALSE ) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; In that plot, there is no legend. Removing the show.legend argument or setting show.legend = TRUE will result in the plot having a legend displaying the mapping between colors and drv. ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, colour = drv)) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; In the chapter, the legend is suppressed because with three plots, adding a legend to only the last plot would make the sizes of plots different. Different sized plots would make it more difficult to see how arguments change the appearance of the plots. The purpose of those plots is to show the difference between no groups, using a group aesthetic, and using a color aesthetic, which creates implicit groups. In that example, the legend isn’t necessary since looking up the values associated with each color isn’t necessary to make that point. Exercise 3.6.4 What does the se argument to geom_smooth() do? It adds standard error bands to the lines. ggplot(data = mpg, mapping = aes(x = displ, y = hwy, colour = drv)) + geom_point() + geom_smooth(se = TRUE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; By default se = TRUE: ggplot(data = mpg, mapping = aes(x = displ, y = hwy, colour = drv)) + geom_point() + geom_smooth() #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Exercise 3.6.5 Will these two graphs look different? Why/why not? ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point() + geom_smooth() ggplot() + geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy)) No. Because both geom_point() and geom_smooth() will use the same data and mappings. They will inherit those options from the ggplot() object, so the mappings don’t need to specified again. ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point() + geom_smooth() #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot() + geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy)) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Exercise 3.6.6 Recreate the R code necessary to generate the following graphs. The following code will generate those plots. ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth(se = FALSE) ggplot(mpg, aes(x = displ, y = hwy)) + geom_smooth(mapping = aes(group = drv), se = FALSE) + geom_point() ggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + geom_point() + geom_smooth(se = FALSE) ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(aes(colour = drv)) + geom_smooth(se = FALSE) ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(aes(colour = drv)) + geom_smooth(aes(linetype = drv), se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(size = 4, color = &quot;white&quot;) + geom_point(aes(colour = drv)) 3.7 Statistical transformations Exercise 3.7.1 What is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function? The “previous plot” referred to in the question is the following. ggplot(data = diamonds) + stat_summary( mapping = aes(x = cut, y = depth), fun.min = min, fun.max = max, fun = median ) The arguments fun.ymin, fun.ymax, and fun.y have been deprecated and replaced with fun.min, fun.max, and fun in ggplot2 v 3.3.0. The default geom for stat_summary() is geom_pointrange(). The default stat for geom_pointrange() is identity() but we can add the argument stat = &quot;summary&quot; to use stat_summary() instead of stat_identity(). ggplot(data = diamonds) + geom_pointrange( mapping = aes(x = cut, y = depth), stat = &quot;summary&quot; ) #&gt; No summary function supplied, defaulting to `mean_se()` The resulting message says that stat_summary() uses the mean and sd to calculate the middle point and endpoints of the line. However, in the original plot the min and max values were used for the endpoints. To recreate the original plot we need to specify values for fun.min, fun.max, and fun. ggplot(data = diamonds) + geom_pointrange( mapping = aes(x = cut, y = depth), stat = &quot;summary&quot;, fun.min = min, fun.max = max, fun = median ) Exercise 3.7.2 What does geom_col() do? How is it different to geom_bar()? The geom_col() function has different default stat than geom_bar(). The default stat of geom_col() is stat_identity(), which leaves the data as is. The geom_col() function expects that the data contains x values and y values which represent the bar height. The default stat of geom_bar() is stat_count(). The geom_bar() function only expects an x variable. The stat, stat_count(), preprocesses input data by counting the number of observations for each value of x. The y aesthetic uses the values of these counts. Exercise 3.7.3 Most geoms and stats come in pairs that are almost always used in concert. Read through the documentation and make a list of all the pairs. What do they have in common? The following tables lists the pairs of geoms and stats that are almost always used in concert. Complementary geoms and stats geom stat geom_bar() stat_count() geom_bin2d() stat_bin_2d() geom_boxplot() stat_boxplot() geom_contour_filled() stat_contour_filled() geom_contour() stat_contour() geom_count() stat_sum() geom_density_2d() stat_density_2d() geom_density() stat_density() geom_dotplot() stat_bindot() geom_function() stat_function() geom_sf() stat_sf() geom_sf() stat_sf() geom_smooth() stat_smooth() geom_violin() stat_ydensity() geom_hex() stat_bin_hex() geom_qq_line() stat_qq_line() geom_qq() stat_qq() geom_quantile() stat_quantile() These pairs of geoms and stats tend to have their names in common, such stat_smooth() and geom_smooth() and be documented on the same help page. The pairs of geoms and stats that are used in concert often have each other as the default stat (for a geom) or geom (for a stat). The following tables contain the geoms and stats in ggplot2 and their defaults as of version 3.3.0. Many geoms have stat_identity() as the default stat. ggplot2 geom layers and their default stats. geom default stat shared docs geom_abline() stat_identity() geom_area() stat_identity() geom_bar() stat_count() x geom_bin2d() stat_bin_2d() x geom_blank() None geom_boxplot() stat_boxplot() x geom_col() stat_identity() geom_count() stat_sum() x geom_countour_filled() stat_countour_filled() x geom_countour() stat_countour() x geom_crossbar() stat_identity() geom_curve() stat_identity() geom_density_2d_filled() stat_density_2d_filled() x geom_density_2d() stat_density_2d() x geom_density() stat_density() x geom_dotplot() stat_bindot() x geom_errorbar() stat_identity() geom_errorbarh() stat_identity() geom_freqpoly() stat_bin() x geom_function() stat_function() x geom_hex() stat_bin_hex() x geom_histogram() stat_bin() x geom_hline() stat_identity() geom_jitter() stat_identity() geom_label() stat_identity() geom_line() stat_identity() geom_linerange() stat_identity() geom_map() stat_identity() geom_path() stat_identity() geom_point() stat_identity() geom_pointrange() stat_identity() geom_polygon() stat_identity() geom_qq_line() stat_qq_line() x geom_qq() stat_qq() x geom_quantile() stat_quantile() x geom_raster() stat_identity() geom_rect() stat_identity() geom_ribbon() stat_identity() geom_rug() stat_identity() geom_segment() stat_identity() geom_sf_label() stat_sf_coordinates() x geom_sf_text() stat_sf_coordinates() x geom_sf() stat_sf() x geom_smooth() stat_smooth() x geom_spoke() stat_identity() geom_step() stat_identity() geom_text() stat_identity() geom_tile() stat_identity() geom_violin() stat_ydensity() x geom_vline() stat_identity() ggplot2 stat layers and their default geoms. stat default geom shared docs stat_bin_2d() geom_tile() stat_bin_hex() geom_hex() x stat_bin() geom_bar() x stat_boxplot() geom_boxplot() x stat_count() geom_bar() x stat_countour_filled() geom_contour_filled() x stat_countour() geom_contour() x stat_density_2d_filled() geom_density_2d() x stat_density_2d() geom_density_2d() x stat_density() geom_area() stat_ecdf() geom_step() stat_ellipse() geom_path() stat_function() geom_function() x stat_function() geom_path() stat_identity() geom_point() stat_qq_line() geom_path() stat_qq() geom_point() stat_quantile() geom_quantile() x stat_sf_coordinates() geom_point() stat_sf() geom_rect() stat_smooth() geom_smooth() x stat_sum() geom_point() stat_summary_2d() geom_tile() stat_summary_bin() geom_pointrange() stat_summary_hex() geom_hex() stat_summary() geom_pointrange() stat_unique() geom_point() stat_ydensity() geom_violin() x Exercise 3.7.4 What variables does stat_smooth() compute? What parameters control its behavior? The function stat_smooth() calculates the following variables: y: predicted value ymin: lower value of the confidence interval ymax: upper value of the confidence interval se: standard error The “Computed Variables” section of the stat_smooth() documentation contains these variables. The parameters that control the behavior of stat_smooth() include: method: This is the method used to compute the smoothing line. If NULL, a default method is used based on the sample size: stats::loess() when there are less than 1,000 observations in a group, and mgcv::gam() with formula = y ~ s(x, bs = &quot;CS) otherwise. Alternatively, the user can provide a character vector with a function name, e.g. &quot;lm&quot;, &quot;loess&quot;, or a function, e.g. MASS::rlm. formula: When providing a custom method argument, the formula to use. The default is y ~ x. For example, to use the line implied by lm(y ~ x + I(x ^ 2) + I(x ^ 3)), use method = &quot;lm&quot; or method = lm and formula = y ~ x + I(x ^ 2) + I(x ^ 3). method.arg(): Arguments other than than the formula, which is already specified in the formula argument, to pass to the function inmethod`. se: If TRUE, display standard error bands, if FALSE only display the line. na.rm: If FALSE, missing values are removed with a warning, if TRUE the are silently removed. The default is FALSE in order to make debugging easier. If missing values are known to be in the data, then can be ignored, but if missing values are not anticipated this warning can help catch errors. TODO: Plots with examples illustrating the uses of these arguments. Exercise 3.7.5 In our proportion bar chart, we need to set group = 1 Why? In other words, what is the problem with these two graphs? If group = 1 is not included, then all the bars in the plot will have the same height, a height of 1. The function geom_bar() assumes that the groups are equal to the x values since the stat computes the counts within the group. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop..)) The problem with these two plots is that the proportions are calculated within the groups. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop..)) ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = color, y = ..prop..)) The following code will produce the intended stacked bar charts for the case with no fill aesthetic. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop.., group = 1)) With the fill aesthetic, the heights of the bars need to be normalized. ggplot(data = diamonds) + geom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill = color)) 3.8 Position adjustments Exercise 3.8.1 What is the problem with this plot? How could you improve it? ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() There is overplotting because there are multiple observations for each combination of cty and hwy values. I would improve the plot by using a jitter position adjustment to decrease overplotting. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point(position = &quot;jitter&quot;) The relationship between cty and hwy is clear even without jittering the points but jittering shows the locations where there are more observations. Exercise 3.8.2 What parameters to geom_jitter() control the amount of jittering? From the geom_jitter() documentation, there are two arguments to jitter: width controls the amount of horizontal displacement, and height controls the amount of vertical displacement. The defaults values of width and height will introduce noise in both directions. Here is what the plot looks like with the default values of height and width. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point(position = position_jitter()) However, we can change these parameters. Here are few a examples to understand how these parameters affect the amount of jittering. Whenwidth = 0 there is no horizontal jitter. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_jitter(width = 0) When width = 20, there is too much horizontal jitter. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_jitter(width = 20) When height = 0, there is no vertical jitter. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_jitter(height = 0) When height = 15, there is too much vertical jitter. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_jitter(height = 15) When width = 0 and height = 0, there is neither horizontal or vertical jitter, and the plot produced is identical to the one produced with geom_point(). ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_jitter(height = 0, width = 0) Note that the height and width arguments are in the units of the data. Thus height = 1 (width = 1) corresponds to different relative amounts of jittering depending on the scale of the y (x) variable. The default values of height and width are defined to be 80% of the resolution() of the data, which is the smallest non-zero distance between adjacent values of a variable. When x and y are discrete variables, their resolutions are both equal to 1, and height = 0.4 and width = 0.4 since the jitter moves points in both positive and negative directions. The default values of height and width in geom_jitter() are non-zero, so unless both height and width are explicitly set set 0, there will be some jitter. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_jitter() Exercise 3.8.3 Compare and contrast geom_jitter() with geom_count(). The geom geom_jitter() adds random variation to the locations points of the graph. In other words, it “jitters” the locations of points slightly. This method reduces overplotting since two points with the same location are unlikely to have the same random variation. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_jitter() However, the reduction in overlapping comes at the cost of slightly changing the x and y values of the points. The geom geom_count() sizes the points relative to the number of observations. Combinations of (x, y) values with more observations will be larger than those with fewer observations. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_count() The geom_count() geom does not change x and y coordinates of the points. However, if the points are close together and counts are large, the size of some points can itself create overplotting. For example, in the following example, a third variable mapped to color is added to the plot. In this case, geom_count() is less readable than geom_jitter() when adding a third variable as a color aesthetic. ggplot(data = mpg, mapping = aes(x = cty, y = hwy, color = class)) + geom_jitter() ggplot(data = mpg, mapping = aes(x = cty, y = hwy, color = class)) + geom_count() Combining geom_count() with jitter, which is specified with the position argument to geom_count() rather than its own geom, helps overplotting a little. ggplot(data = mpg, mapping = aes(x = cty, y = hwy, color = class)) + geom_count(position = &quot;jitter&quot;) But as this example shows, unfortunately, there is no universal solution to overplotting. The costs and benefits of different approaches will depend on the structure of the data and the goal of the data scientist. Exercise 3.8.4 What’s the default position adjustment for geom_boxplot()? Create a visualization of the mpg dataset that demonstrates it. The default position for geom_boxplot() is &quot;dodge2&quot;, which is a shortcut for position_dodge2. This position adjustment does not change the vertical position of a geom but moves the geom horizontally to avoid overlapping other geoms. See the documentation for position_dodge2() for additional discussion on how it works. When we add colour = class to the box plot, the different levels of the drv variable are placed side by side, i.e., dodged. ggplot(data = mpg, aes(x = drv, y = hwy, colour = class)) + geom_boxplot() If position_identity() is used the boxplots overlap. ggplot(data = mpg, aes(x = drv, y = hwy, colour = class)) + geom_boxplot(position = &quot;identity&quot;) 3.9 Coordinate systems Exercise 3.9.1 Turn a stacked bar chart into a pie chart using coord_polar(). A pie chart is a stacked bar chart with the addition of polar coordinates. Take this stacked bar chart with a single category. ggplot(mpg, aes(x = factor(1), fill = drv)) + geom_bar() Now add coord_polar(theta=&quot;y&quot;) to create pie chart. ggplot(mpg, aes(x = factor(1), fill = drv)) + geom_bar(width = 1) + coord_polar(theta = &quot;y&quot;) The argument theta = &quot;y&quot; maps y to the angle of each section. If coord_polar() is specified without theta = &quot;y&quot;, then the resulting plot is called a bulls-eye chart. ggplot(mpg, aes(x = factor(1), fill = drv)) + geom_bar(width = 1) + coord_polar() Exercise 3.9.2 What does labs() do? Read the documentation. The labs function adds axis titles, plot titles, and a caption to the plot. ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() + coord_flip() + labs(y = &quot;Highway MPG&quot;, x = &quot;Class&quot;, title = &quot;Highway MPG by car class&quot;, subtitle = &quot;1999-2008&quot;, caption = &quot;Source: http://fueleconomy.gov&quot;) The arguments to labs() are optional, so you can add as many or as few of these as are needed. ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() + coord_flip() + labs(y = &quot;Highway MPG&quot;, x = &quot;Year&quot;, title = &quot;Highway MPG by car class&quot;) The labs() function is not the only function that adds titles to plots. The xlab(), ylab(), and x- and y-scale functions can add axis titles. The ggtitle() function adds plot titles. Exercise 3.9.3 What’s the difference between coord_quickmap() and coord_map()? The coord_map() function uses map projections to project the three-dimensional Earth onto a two-dimensional plane. By default, coord_map() uses the Mercator projection. This projection is applied to all the geoms in the plot. The coord_quickmap() function uses an approximate but faster map projection. This approximation ignores the curvature of Earth and adjusts the map for the latitude/longitude ratio. The coord_quickmap() project is faster than coord_map() both because the projection is computationally easier, and unlike coord_map(), the coordinates of the individual geoms do not need to be transformed. See the coord_map() documentation for more information on these functions and some examples. Exercise 3.9.4 What does the plot below tell you about the relationship between city and highway mpg? Why is coord_fixed() important? What does geom_abline() do? The function coord_fixed() ensures that the line produced by geom_abline() is at a 45-degree angle. A 45-degree line makes it easy to compare the highway and city mileage to the case in which city and highway MPG were equal. p &lt;- ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() + geom_abline() p + coord_fixed() If we didn’t include coord_fixed(), then the line would no longer have an angle of 45 degrees. p On average, humans are best able to perceive differences in angles relative to 45 degrees. See Cleveland (1993b), Cleveland (1994),Cleveland (1993a), Cleveland, McGill, and McGill (1988), Heer and Agrawala (2006) for discussion on how the aspect ratio of a plot affects perception of the values it encodes, evidence that 45-degrees is generally the optimal aspect ratio, and methods to calculate the optimal aspect ratio of a plot. The function ggthemes::bank_slopes() will calculate the optimal aspect ratio to bank slopes to 45-degrees. 3.10 The layered grammar of graphics No exercises References "],
["workflow-basics.html", "4 Workflow: basics Exercise 4.1 Exercise 4.2 Exercise 4.3", " 4 Workflow: basics library(&quot;tidyverse&quot;) Exercise 4.1 Why does this code not work? my_variable &lt;- 10 my_varıable #&gt; Error in eval(expr, envir, enclos): object &#39;my_varıable&#39; not found The variable being printed is my_varıable, not my_variable: the seventh character is “ı” (“LATIN SMALL LETTER DOTLESS I”), not “i”. While it wouldn’t have helped much in this case, the importance of distinguishing characters in code is reasons why fonts which clearly distinguish similar characters are preferred in programming. It is especially important to distinguish between two sets of similar looking characters: the numeral zero (0), the Latin small letter O (o), and the Latin capital letter O (O), the numeral one (1), the Latin small letter I (i), the Latin capital letter I (I), and Latin small letter L (l). In these fonts, zero and the Latin letter O are often distinguished by using a glyph for zero that uses either a dot in the interior or a slash through it. Some examples of fonts with dotted or slashed zero glyphs are Consolas, Deja Vu Sans Mono, Monaco, Menlo, Source Sans Pro, and FiraCode. Error messages of the form &quot;object '...' not found&quot; mean exactly what they say. R cannot find an object with that name. Unfortunately, the error does not tell you why that object cannot be found, because R does not know the reason that the object does not exist. The most common scenarios in which I encounter this error message are I forgot to create the object, or an error prevented the object from being created. I made a typo in the object’s name, either when using it or when I created it (as in the example above), or I forgot what I had originally named it. If you find yourself often writing the wrong name for an object, it is a good indication that the original name was not a good one. I forgot to load the package that contains the object using library(). Exercise 4.2 Tweak each of the following R commands so that they run correctly: ggplot(dota = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) fliter(mpg, cyl = 8) filter(diamond, carat &gt; 3) ggplot(dota = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) #&gt; Error in FUN(X[[i]], ...): object &#39;displ&#39; not found The error message is argument &quot;data&quot; is missing, with no default. This error is a result of a typo, dota instead of data. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) fliter(mpg, cyl = 8) #&gt; Error in fliter(mpg, cyl = 8): could not find function &quot;fliter&quot; R could not find the function fliter() because we made a typo: fliter instead of filter. filter(mpg, cyl = 8) #&gt; Error: Problem with `filter()` input `..1`. #&gt; ✖ Input `..1` is named. #&gt; ℹ This usually means that you&#39;ve used `=` instead of `==`. #&gt; ℹ Did you mean `cyl == 8`? We aren’t done yet. But the error message gives a suggestion. Let’s follow it. filter(mpg, cyl == 8) #&gt; # A tibble: 70 x 11 #&gt; manufacturer model displ year cyl trans drv cty hwy fl class #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 audi a6 quattro 4.2 2008 8 auto(… 4 16 23 p mids… #&gt; 2 chevrolet c1500 sub… 5.3 2008 8 auto(… r 14 20 r suv #&gt; 3 chevrolet c1500 sub… 5.3 2008 8 auto(… r 11 15 e suv #&gt; 4 chevrolet c1500 sub… 5.3 2008 8 auto(… r 14 20 r suv #&gt; 5 chevrolet c1500 sub… 5.7 1999 8 auto(… r 13 17 r suv #&gt; 6 chevrolet c1500 sub… 6 2008 8 auto(… r 12 17 r suv #&gt; # … with 64 more rows filter(diamond, carat &gt; 3) #&gt; Error in filter(diamond, carat &gt; 3): object &#39;diamond&#39; not found R says it can’t find the object diamond. This is a typo; the data frame is named diamonds. filter(diamonds, carat &gt; 3) #&gt; # A tibble: 32 x 10 #&gt; carat cut color clarity depth table price x y z #&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3.01 Premium I I1 62.7 58 8040 9.1 8.97 5.67 #&gt; 2 3.11 Fair J I1 65.9 57 9823 9.15 9.02 5.98 #&gt; 3 3.01 Premium F I1 62.2 56 9925 9.24 9.13 5.73 #&gt; 4 3.05 Premium E I1 60.9 58 10453 9.26 9.25 5.66 #&gt; 5 3.02 Fair I I1 65.2 56 10577 9.11 9.02 5.91 #&gt; 6 3.01 Fair H I1 56.1 62 10761 9.54 9.38 5.31 #&gt; # … with 26 more rows How did I know? I started typing in diamond and RStudio completed it to diamonds. Since diamonds includes the variable carat and the code works, that appears to have been the problem. Exercise 4.3 Press Alt + Shift + K. What happens? How can you get to the same place using the menus? This gives a menu with keyboard shortcuts. This can be found in the menu under Tools -&gt; Keyboard Shortcuts Help. "],
["transform.html", "5 Data transformation 5.1 Introduction 5.2 Filter rows with filter() 5.3 Arrange rows with arrange() 5.4 Select columns with select() 5.5 Add new variables with mutate() 5.6 Grouped summaries with summarise() 5.7 Grouped mutates (and filters)", " 5 Data transformation 5.1 Introduction library(&quot;nycflights13&quot;) library(&quot;tidyverse&quot;) 5.2 Filter rows with filter() Exercise 5.2.1 Find all flights that Had an arrival delay of two or more hours Flew to Houston (IAH or HOU) Were operated by United, American, or Delta Departed in summer (July, August, and September) Arrived more than two hours late, but didn’t leave late Were delayed by at least an hour, but made up over 30 minutes in flight Departed between midnight and 6 am (inclusive) The answer to each part follows. Since the arr_delay variable is measured in minutes, find flights with an arrival delay of 120 or more minutes. filter(flights, arr_delay &gt;= 120) #&gt; # A tibble: 10,200 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 811 630 101 1047 830 #&gt; 2 2013 1 1 848 1835 853 1001 1950 #&gt; 3 2013 1 1 957 733 144 1056 853 #&gt; 4 2013 1 1 1114 900 134 1447 1222 #&gt; 5 2013 1 1 1505 1310 115 1638 1431 #&gt; 6 2013 1 1 1525 1340 105 1831 1626 #&gt; # … with 10,194 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The flights that flew to Houston are those flights where the destination (dest) is either “IAH” or “HOU”. filter(flights, dest == &quot;IAH&quot; | dest == &quot;HOU&quot;) #&gt; # A tibble: 9,313 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 623 627 -4 933 932 #&gt; 4 2013 1 1 728 732 -4 1041 1038 #&gt; 5 2013 1 1 739 739 0 1104 1038 #&gt; 6 2013 1 1 908 908 0 1228 1219 #&gt; # … with 9,307 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; However, using %in% is more compact and would scale to cases where there were more than two airports we were interested in. filter(flights, dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)) #&gt; # A tibble: 9,313 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 623 627 -4 933 932 #&gt; 4 2013 1 1 728 732 -4 1041 1038 #&gt; 5 2013 1 1 739 739 0 1104 1038 #&gt; 6 2013 1 1 908 908 0 1228 1219 #&gt; # … with 9,307 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; In the flights dataset, the column carrier indicates the airline, but it uses two-character carrier codes. We can find the carrier codes for the airlines in the airlines dataset. Since the carrier code dataset only has 16 rows, and the names of the airlines in that dataset are not exactly “United”, “American”, or “Delta”, it is easiest to manually look up their carrier codes in that data. airlines #&gt; # A tibble: 16 x 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 9E Endeavor Air Inc. #&gt; 2 AA American Airlines Inc. #&gt; 3 AS Alaska Airlines Inc. #&gt; 4 B6 JetBlue Airways #&gt; 5 DL Delta Air Lines Inc. #&gt; 6 EV ExpressJet Airlines Inc. #&gt; # … with 10 more rows The carrier code for Delta is &quot;DL&quot;, for American is &quot;AA&quot;, and for United is &quot;UA&quot;. Using these carriers codes, we check whether carrier is one of those. filter(flights, carrier %in% c(&quot;AA&quot;, &quot;DL&quot;, &quot;UA&quot;)) #&gt; # A tibble: 139,504 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 554 600 -6 812 837 #&gt; 5 2013 1 1 554 558 -4 740 728 #&gt; 6 2013 1 1 558 600 -2 753 745 #&gt; # … with 139,498 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The variable month has the month, and it is numeric. So, the summer flights are those that departed in months 7 (July), 8 (August), and 9 (September). filter(flights, month &gt;= 7, month &lt;= 9) #&gt; # A tibble: 86,326 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 7 1 1 2029 212 236 2359 #&gt; 2 2013 7 1 2 2359 3 344 344 #&gt; 3 2013 7 1 29 2245 104 151 1 #&gt; 4 2013 7 1 43 2130 193 322 14 #&gt; 5 2013 7 1 44 2150 174 300 100 #&gt; 6 2013 7 1 46 2051 235 304 2358 #&gt; # … with 86,320 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The %in% operator is an alternative. If the : operator is used to specify the integer range, the expression is readable and compact. filter(flights, month %in% 7:9) #&gt; # A tibble: 86,326 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 7 1 1 2029 212 236 2359 #&gt; 2 2013 7 1 2 2359 3 344 344 #&gt; 3 2013 7 1 29 2245 104 151 1 #&gt; 4 2013 7 1 43 2130 193 322 14 #&gt; 5 2013 7 1 44 2150 174 300 100 #&gt; 6 2013 7 1 46 2051 235 304 2358 #&gt; # … with 86,320 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; We could also use the | operator. However, the | does not scale to many choices. Even with only three choices, it is quite verbose. filter(flights, month == 7 | month == 8 | month == 9) #&gt; # A tibble: 86,326 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 7 1 1 2029 212 236 2359 #&gt; 2 2013 7 1 2 2359 3 344 344 #&gt; 3 2013 7 1 29 2245 104 151 1 #&gt; 4 2013 7 1 43 2130 193 322 14 #&gt; 5 2013 7 1 44 2150 174 300 100 #&gt; 6 2013 7 1 46 2051 235 304 2358 #&gt; # … with 86,320 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; We can also use the between() function as shown in Exercise 5.2.2. Flights that arrived more than two hours late, but didn’t leave late will have an arrival delay of more than 120 minutes (arr_delay &gt; 120) and a non-positive departure delay (dep_delay &lt;= 0). filter(flights, arr_delay &gt; 120, dep_delay &lt;= 0) #&gt; # A tibble: 29 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 27 1419 1420 -1 1754 1550 #&gt; 2 2013 10 7 1350 1350 0 1736 1526 #&gt; 3 2013 10 7 1357 1359 -2 1858 1654 #&gt; 4 2013 10 16 657 700 -3 1258 1056 #&gt; 5 2013 11 1 658 700 -2 1329 1015 #&gt; 6 2013 3 18 1844 1847 -3 39 2219 #&gt; # … with 23 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, #&gt; # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Were delayed by at least an hour, but made up over 30 minutes in flight. If a flight was delayed by at least an hour, then dep_delay &gt;= 60. If the flight didn’t make up any time in the air, then its arrival would be delayed by the same amount as its departure, meaning dep_delay == arr_delay, or alternatively, dep_delay - arr_delay == 0. If it makes up over 30 minutes in the air, then the arrival delay must be at least 30 minutes less than the departure delay, which is stated as dep_delay - arr_delay &gt; 30. filter(flights, dep_delay &gt;= 60, dep_delay - arr_delay &gt; 30) #&gt; # A tibble: 1,844 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 2205 1720 285 46 2040 #&gt; 2 2013 1 1 2326 2130 116 131 18 #&gt; 3 2013 1 3 1503 1221 162 1803 1555 #&gt; 4 2013 1 3 1839 1700 99 2056 1950 #&gt; 5 2013 1 3 1850 1745 65 2148 2120 #&gt; 6 2013 1 3 1941 1759 102 2246 2139 #&gt; # … with 1,838 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Finding flights that departed between midnight and 6 a.m. is complicated by the way in which times are represented in the data. In dep_time, midnight is represented by 2400, not 0. You can verify this by checking the minimum and maximum of dep_time. summary(flights$dep_time) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; 1 907 1401 1349 1744 2400 8255 This is an example of why it is always good to check the summary statistics of your data. Unfortunately, this means we cannot simply check that dep_time &lt; 600, because we also have to consider the special case of midnight. filter(flights, dep_time &lt;= 600 | dep_time == 2400) #&gt; # A tibble: 9,373 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; # … with 9,367 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Alternatively, we could use the modulo operator, %%. The modulo operator returns the remainder of division. Let’s see how this affects our times. c(600, 1200, 2400) %% 2400 #&gt; [1] 600 1200 0 Since 2400 %% 2400 == 0 and all other times are left unchanged, we can compare the result of the modulo operation to 600, filter(flights, dep_time %% 2400 &lt;= 600) #&gt; # A tibble: 9,373 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; # … with 9,367 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; This filter expression is more compact, but its readability depends on the familiarity of the reader with modular arithmetic. Exercise 5.2.2 Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer the previous challenges? The expression between(x, left, right) is equivalent to x &gt;= left &amp; x &lt;= right. Of the answers in the previous question, we could simplify the statement of departed in summer (month &gt;= 7 &amp; month &lt;= 9) using the between() function. filter(flights, between(month, 7, 9)) #&gt; # A tibble: 86,326 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 7 1 1 2029 212 236 2359 #&gt; 2 2013 7 1 2 2359 3 344 344 #&gt; 3 2013 7 1 29 2245 104 151 1 #&gt; 4 2013 7 1 43 2130 193 322 14 #&gt; 5 2013 7 1 44 2150 174 300 100 #&gt; 6 2013 7 1 46 2051 235 304 2358 #&gt; # … with 86,320 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Exercise 5.2.3 How many flights have a missing dep_time? What other variables are missing? What might these rows represent? Find the rows of flights with a missing departure time (dep_time) using the is.na() function. filter(flights, is.na(dep_time)) #&gt; # A tibble: 8,255 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 NA 1630 NA NA 1815 #&gt; 2 2013 1 1 NA 1935 NA NA 2240 #&gt; 3 2013 1 1 NA 1500 NA NA 1825 #&gt; 4 2013 1 1 NA 600 NA NA 901 #&gt; 5 2013 1 2 NA 1540 NA NA 1747 #&gt; 6 2013 1 2 NA 1620 NA NA 1746 #&gt; # … with 8,249 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Notably, the arrival time (arr_time) is also missing for these rows. These seem to be cancelled flights. The output of the function summary() includes the number of missing values for all non-character variables. summary(flights) #&gt; year month day dep_time sched_dep_time #&gt; Min. :2013 Min. : 1.00 Min. : 1.0 Min. : 1 Min. : 106 #&gt; 1st Qu.:2013 1st Qu.: 4.00 1st Qu.: 8.0 1st Qu.: 907 1st Qu.: 906 #&gt; Median :2013 Median : 7.00 Median :16.0 Median :1401 Median :1359 #&gt; Mean :2013 Mean : 6.55 Mean :15.7 Mean :1349 Mean :1344 #&gt; 3rd Qu.:2013 3rd Qu.:10.00 3rd Qu.:23.0 3rd Qu.:1744 3rd Qu.:1729 #&gt; Max. :2013 Max. :12.00 Max. :31.0 Max. :2400 Max. :2359 #&gt; NA&#39;s :8255 #&gt; dep_delay arr_time sched_arr_time arr_delay carrier #&gt; Min. : -43 Min. : 1 Min. : 1 Min. : -86 Length:336776 #&gt; 1st Qu.: -5 1st Qu.:1104 1st Qu.:1124 1st Qu.: -17 Class :character #&gt; Median : -2 Median :1535 Median :1556 Median : -5 Mode :character #&gt; Mean : 13 Mean :1502 Mean :1536 Mean : 7 #&gt; 3rd Qu.: 11 3rd Qu.:1940 3rd Qu.:1945 3rd Qu.: 14 #&gt; Max. :1301 Max. :2400 Max. :2359 Max. :1272 #&gt; NA&#39;s :8255 NA&#39;s :8713 NA&#39;s :9430 #&gt; flight tailnum origin dest #&gt; Min. : 1 Length:336776 Length:336776 Length:336776 #&gt; 1st Qu.: 553 Class :character Class :character Class :character #&gt; Median :1496 Mode :character Mode :character Mode :character #&gt; Mean :1972 #&gt; 3rd Qu.:3465 #&gt; Max. :8500 #&gt; #&gt; air_time distance hour minute #&gt; Min. : 20 Min. : 17 Min. : 1.0 Min. : 0.0 #&gt; 1st Qu.: 82 1st Qu.: 502 1st Qu.: 9.0 1st Qu.: 8.0 #&gt; Median :129 Median : 872 Median :13.0 Median :29.0 #&gt; Mean :151 Mean :1040 Mean :13.2 Mean :26.2 #&gt; 3rd Qu.:192 3rd Qu.:1389 3rd Qu.:17.0 3rd Qu.:44.0 #&gt; Max. :695 Max. :4983 Max. :23.0 Max. :59.0 #&gt; NA&#39;s :9430 #&gt; time_hour #&gt; Min. :2013-01-01 05:00:00 #&gt; 1st Qu.:2013-04-04 13:00:00 #&gt; Median :2013-07-03 10:00:00 #&gt; Mean :2013-07-03 05:22:54 #&gt; 3rd Qu.:2013-10-01 07:00:00 #&gt; Max. :2013-12-31 23:00:00 #&gt; Exercise 5.2.4 Why is NA ^ 0 not missing? Why is NA | TRUE not missing? Why is FALSE &amp; NA not missing? Can you figure out the general rule? (NA * 0 is a tricky counterexample!) NA ^ 0 #&gt; [1] 1 NA ^ 0 == 1 since for all numeric values \\(x ^ 0 = 1\\). NA | TRUE #&gt; [1] TRUE NA | TRUE is TRUE because anything or TRUE is TRUE. If the missing value were TRUE, then TRUE | TRUE == TRUE, and if the missing value was FALSE, then FALSE | TRUE == TRUE. NA &amp; FALSE #&gt; [1] FALSE The value of NA &amp; FALSE is FALSE because anything and FALSE is always FALSE. If the missing value were TRUE, then TRUE &amp; FALSE == FALSE, and if the missing value was FALSE, then FALSE &amp; FALSE == FALSE. NA | FALSE #&gt; [1] NA For NA | FALSE, the value is unknown since TRUE | FALSE == TRUE, but FALSE | FALSE == FALSE. NA &amp; TRUE #&gt; [1] NA For NA &amp; TRUE, the value is unknown since FALSE &amp; TRUE== FALSE, but TRUE &amp; TRUE == TRUE. NA * 0 #&gt; [1] NA Since \\(x * 0 = 0\\) for all finite numbers we might expect NA * 0 == 0, but that’s not the case. The reason that NA * 0 != 0 is that \\(0 \\times \\infty\\) and \\(0 \\times -\\infty\\) are undefined. R represents undefined results as NaN, which is an abbreviation of “not a number”. Inf * 0 #&gt; [1] NaN -Inf * 0 #&gt; [1] NaN 5.3 Arrange rows with arrange() Exercise 5.3.1 How could you use arrange() to sort all missing values to the start? (Hint: use is.na()). The arrange() function puts NA values last. arrange(flights, dep_time) %&gt;% tail() #&gt; # A tibble: 6 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 9 30 NA 1842 NA NA 2019 #&gt; 2 2013 9 30 NA 1455 NA NA 1634 #&gt; 3 2013 9 30 NA 2200 NA NA 2312 #&gt; 4 2013 9 30 NA 1210 NA NA 1330 #&gt; 5 2013 9 30 NA 1159 NA NA 1344 #&gt; 6 2013 9 30 NA 840 NA NA 1020 #&gt; # … with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, #&gt; # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Using desc() does not change that. arrange(flights, desc(dep_time)) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 10 30 2400 2359 1 327 337 #&gt; 2 2013 11 27 2400 2359 1 515 445 #&gt; 3 2013 12 5 2400 2359 1 427 440 #&gt; 4 2013 12 9 2400 2359 1 432 440 #&gt; 5 2013 12 9 2400 2250 70 59 2356 #&gt; 6 2013 12 13 2400 2359 1 432 440 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; To put NA values first, we can add an indicator of whether the column has a missing value. Then we sort by the missing indicator column and the column of interest. For example, to sort the data frame by departure time (dep_time) in ascending order but NA values first, run the following. arrange(flights, desc(is.na(dep_time)), dep_time) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 NA 1630 NA NA 1815 #&gt; 2 2013 1 1 NA 1935 NA NA 2240 #&gt; 3 2013 1 1 NA 1500 NA NA 1825 #&gt; 4 2013 1 1 NA 600 NA NA 901 #&gt; 5 2013 1 2 NA 1540 NA NA 1747 #&gt; 6 2013 1 2 NA 1620 NA NA 1746 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The flights will first be sorted by desc(is.na(dep_time)). Since desc(is.na(dep_time)) is either TRUE when dep_time is missing, or FALSE, when it is not, the rows with missing values of dep_time will come first, since TRUE &gt; FALSE. Exercise 5.3.2 Sort flights to find the most delayed flights. Find the flights that left earliest. Find the most delayed flights by sorting the table by departure delay, dep_delay, in descending order. arrange(flights, desc(dep_delay)) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 9 641 900 1301 1242 1530 #&gt; 2 2013 6 15 1432 1935 1137 1607 2120 #&gt; 3 2013 1 10 1121 1635 1126 1239 1810 #&gt; 4 2013 9 20 1139 1845 1014 1457 2210 #&gt; 5 2013 7 22 845 1600 1005 1044 1815 #&gt; 6 2013 4 10 1100 1900 960 1342 2211 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The most delayed flight was HA 51, JFK to HNL, which was scheduled to leave on January 09, 2013 09:00. Note that the departure time is given as 641, which seems to be less than the scheduled departure time. But the departure was delayed 1,301 minutes, which is 21 hours, 41 minutes. The departure time is the day after the scheduled departure time. Be happy that you weren’t on that flight, and if you happened to have been on that flight and are reading this, I’m sorry for you. Similarly, the earliest departing flight can be found by sorting dep_delay in ascending order. arrange(flights, dep_delay) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 12 7 2040 2123 -43 40 2352 #&gt; 2 2013 2 3 2022 2055 -33 2240 2338 #&gt; 3 2013 11 10 1408 1440 -32 1549 1559 #&gt; 4 2013 1 11 1900 1930 -30 2233 2243 #&gt; 5 2013 1 29 1703 1730 -27 1947 1957 #&gt; 6 2013 8 9 729 755 -26 1002 955 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Flight B6 97 (JFK to DEN) scheduled to depart on December 07, 2013 at 21:23 departed 43 minutes early. Exercise 5.3.3 Sort flights to find the fastest flights. There are actually two ways to interpret this question: one that can be solved by using arrange(), and a more complex interpretation that requires creation of a new variable using mutate(), which we haven’t seen demonstrated before. The colloquial interpretation of “fastest” flight can be understood to mean “the flight with the shortest flight time”. We can use arrange to sort our data by the air_time variable to find the shortest flights: head(arrange(flights, air_time)) #&gt; # A tibble: 6 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 16 1355 1315 40 1442 1411 #&gt; 2 2013 4 13 537 527 10 622 628 #&gt; 3 2013 12 6 922 851 31 1021 954 #&gt; 4 2013 2 3 2153 2129 24 2247 2224 #&gt; 5 2013 2 5 1303 1315 -12 1342 1411 #&gt; 6 2013 2 12 2123 2130 -7 2211 2225 #&gt; # … with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, #&gt; # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Another definition of the “fastest flight” is the flight with the highest average ground speed. The ground speed is not included in the data, but it can be calculated from the distance and air_time of the flight. head(arrange(flights, desc(distance / air_time))) #&gt; # A tibble: 6 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 5 25 1709 1700 9 1923 1937 #&gt; 2 2013 7 2 1558 1513 45 1745 1719 #&gt; 3 2013 5 13 2040 2025 15 2225 2226 #&gt; 4 2013 3 23 1914 1910 4 2045 2043 #&gt; 5 2013 1 12 1559 1600 -1 1849 1917 #&gt; 6 2013 11 17 650 655 -5 1059 1150 #&gt; # … with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, #&gt; # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Exercise 5.3.4 Which flights traveled the longest? Which traveled the shortest? To find the longest flight, sort the flights by the distance column in descending order. arrange(flights, desc(distance)) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 857 900 -3 1516 1530 #&gt; 2 2013 1 2 909 900 9 1525 1530 #&gt; 3 2013 1 3 914 900 14 1504 1530 #&gt; 4 2013 1 4 900 900 0 1516 1530 #&gt; 5 2013 1 5 858 900 -2 1519 1530 #&gt; 6 2013 1 6 1019 900 79 1558 1530 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The longest flight is HA 51, JFK to HNL, which is 4,983 miles. To find the shortest flight, sort the flights by the distance in ascending order, which is the default sort order. arrange(flights, distance) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 7 27 NA 106 NA NA 245 #&gt; 2 2013 1 3 2127 2129 -2 2222 2224 #&gt; 3 2013 1 4 1240 1200 40 1333 1306 #&gt; 4 2013 1 4 1829 1615 134 1937 1721 #&gt; 5 2013 1 4 2128 2129 -1 2218 2224 #&gt; 6 2013 1 5 1155 1200 -5 1241 1306 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The shortest flight is US 1632, EWR to LGA, which is only 17 miles. This is a flight between two of the New York area airports. However, since this flight is missing a departure time so it either did not actually fly or there is a problem with the data. The terms “longest” and “shortest” could also refer to the time of the flight instead of the distance. Now the longest and shortest flights by can be found by sorting by the air_time column. The longest flights by airtime are the following. arrange(flights, desc(air_time)) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 3 17 1337 1335 2 1937 1836 #&gt; 2 2013 2 6 853 900 -7 1542 1540 #&gt; 3 2013 3 15 1001 1000 1 1551 1530 #&gt; 4 2013 3 17 1006 1000 6 1607 1530 #&gt; 5 2013 3 16 1001 1000 1 1544 1530 #&gt; 6 2013 2 5 900 900 0 1555 1540 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The shortest flights by airtime are the following. arrange(flights, air_time) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 16 1355 1315 40 1442 1411 #&gt; 2 2013 4 13 537 527 10 622 628 #&gt; 3 2013 12 6 922 851 31 1021 954 #&gt; 4 2013 2 3 2153 2129 24 2247 2224 #&gt; 5 2013 2 5 1303 1315 -12 1342 1411 #&gt; 6 2013 2 12 2123 2130 -7 2211 2225 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 5.4 Select columns with select() Exercise 5.4.1 Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights. These are a few ways to select columns. Specify columns names as unquoted variable names. select(flights, dep_time, dep_delay, arr_time, arr_delay) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows Specify column names as strings. select(flights, &quot;dep_time&quot;, &quot;dep_delay&quot;, &quot;arr_time&quot;, &quot;arr_delay&quot;) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows Specify the column numbers of the variables. select(flights, 4, 6, 7, 9) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows This works, but is not good practice for two reasons. First, the column location of variables may change, resulting in code that may continue to run without error, but produce the wrong answer. Second code is obfuscated, since it is not clear from the code which variables are being selected. What variable does column 6 correspond to? I just wrote the code, and I’ve already forgotten. Specify the names of the variables with character vector and any_of() or all_of() select(flights, all_of(c(&quot;dep_time&quot;, &quot;dep_delay&quot;, &quot;arr_time&quot;, &quot;arr_delay&quot;))) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows select(flights, any_of(c(&quot;dep_time&quot;, &quot;dep_delay&quot;, &quot;arr_time&quot;, &quot;arr_delay&quot;))) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows This is useful because the names of the variables can be stored in a variable and passed to all_of() or any_of(). variables &lt;- c(&quot;dep_time&quot;, &quot;dep_delay&quot;, &quot;arr_time&quot;, &quot;arr_delay&quot;) select(flights, all_of(variables)) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows These two functions replace the deprecated function one_of(). Selecting the variables by matching the start of their names using starts_with(). select(flights, starts_with(&quot;dep_&quot;), starts_with(&quot;arr_&quot;)) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows Selecting the variables using regular expressions with matches(). Regular expressions provide a flexible way to match string patterns and are discussed in the Strings chapter. select(flights, matches(&quot;^(dep|arr)_(time|delay)$&quot;)) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows Specify the names of the variables with a character vector and use the bang-bang operator (!!). variables &lt;- c(&quot;dep_time&quot;, &quot;dep_delay&quot;, &quot;arr_time&quot;, &quot;arr_delay&quot;) select(flights, !!variables) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows This and the following answers use the features of tidy evaluation not covered in R4DS but covered in the Programming with dplyr vignette. Specify the names of the variables in a character or list vector and use the bang-bang-bang operator. variables &lt;- c(&quot;dep_time&quot;, &quot;dep_delay&quot;, &quot;arr_time&quot;, &quot;arr_delay&quot;) select(flights, !!!variables) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows Specify the unquoted names of the variables in a list using syms() and use the bang-bang-bang operator. variables &lt;- syms(c(&quot;dep_time&quot;, &quot;dep_delay&quot;, &quot;arr_time&quot;, &quot;arr_delay&quot;)) select(flights, !!!variables) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # … with 336,770 more rows Some things that don’t work are: Matching the ends of their names using ends_with() since this will incorrectly include other variables. For example, select(flights, ends_with(&quot;arr_time&quot;), ends_with(&quot;dep_time&quot;)) #&gt; # A tibble: 336,776 x 4 #&gt; arr_time sched_arr_time dep_time sched_dep_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 830 819 517 515 #&gt; 2 850 830 533 529 #&gt; 3 923 850 542 540 #&gt; 4 1004 1022 544 545 #&gt; 5 812 837 554 600 #&gt; 6 740 728 554 558 #&gt; # … with 336,770 more rows Matching the names using contains() since there is not a pattern that can include all these variables without incorrectly including others. select(flights, contains(&quot;_time&quot;), contains(&quot;arr_&quot;)) #&gt; # A tibble: 336,776 x 6 #&gt; dep_time sched_dep_time arr_time sched_arr_time air_time arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 517 515 830 819 227 11 #&gt; 2 533 529 850 830 227 20 #&gt; 3 542 540 923 850 160 33 #&gt; 4 544 545 1004 1022 183 -18 #&gt; 5 554 600 812 837 116 -25 #&gt; 6 554 558 740 728 150 12 #&gt; # … with 336,770 more rows Exercise 5.4.2 What happens if you include the name of a variable multiple times in a select() call? The select() call ignores the duplication. Any duplicated variables are only included once, in the first location they appear. The select() function does not raise an error or warning or print any message if there are duplicated variables. select(flights, year, month, day, year, year) #&gt; # A tibble: 336,776 x 3 #&gt; year month day #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # … with 336,770 more rows This behavior is useful because it means that we can use select() with everything() in order to easily change the order of columns without having to specify the names of all the columns. select(flights, arr_delay, everything()) #&gt; # A tibble: 336,776 x 19 #&gt; arr_delay year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 11 2013 1 1 517 515 2 830 #&gt; 2 20 2013 1 1 533 529 4 850 #&gt; 3 33 2013 1 1 542 540 2 923 #&gt; 4 -18 2013 1 1 544 545 -1 1004 #&gt; 5 -25 2013 1 1 554 600 -6 812 #&gt; 6 12 2013 1 1 554 558 -4 740 #&gt; # … with 336,770 more rows, and 11 more variables: sched_arr_time &lt;int&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Exercise 5.4.3 What does the one_of() function do? Why might it be helpful in conjunction with this vector? The one_of() function selects variables with a character vector rather than unquoted variable name arguments. This function is useful because it is easier to programmatically generate character vectors with variable names than to generate unquoted variable names, which are easier to type. vars &lt;- c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;dep_delay&quot;, &quot;arr_delay&quot;) select(flights, one_of(vars)) #&gt; # A tibble: 336,776 x 5 #&gt; year month day dep_delay arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 #&gt; 2 2013 1 1 4 20 #&gt; 3 2013 1 1 2 33 #&gt; 4 2013 1 1 -1 -18 #&gt; 5 2013 1 1 -6 -25 #&gt; 6 2013 1 1 -4 12 #&gt; # … with 336,770 more rows In the most recent versions of dplyr, one_of has been deprecated in favor of two functions: all_of() and any_of(). These functions behave similarly if all variables are present in the data frame. select(flights, any_of(vars)) #&gt; # A tibble: 336,776 x 5 #&gt; year month day dep_delay arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 #&gt; 2 2013 1 1 4 20 #&gt; 3 2013 1 1 2 33 #&gt; 4 2013 1 1 -1 -18 #&gt; 5 2013 1 1 -6 -25 #&gt; 6 2013 1 1 -4 12 #&gt; # … with 336,770 more rows select(flights, all_of(vars)) #&gt; # A tibble: 336,776 x 5 #&gt; year month day dep_delay arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 #&gt; 2 2013 1 1 4 20 #&gt; 3 2013 1 1 2 33 #&gt; 4 2013 1 1 -1 -18 #&gt; 5 2013 1 1 -6 -25 #&gt; 6 2013 1 1 -4 12 #&gt; # … with 336,770 more rows These functions differ in their strictness. The function all_of() will raise an error if one of the variable names is not present, while any_of() will ignore it. vars2 &lt;- c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;variable_not_in_the_dataframe&quot;) select(flights, all_of(vars2)) #&gt; Error: Can&#39;t subset columns that don&#39;t exist. #&gt; ✖ Column `variable_not_in_the_dataframe` doesn&#39;t exist. select(flights, any_of(vars2)) #&gt; # A tibble: 336,776 x 3 #&gt; year month day #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # … with 336,770 more rows The deprecated function one_of() will raise a warning if an unknown column is encountered. select(flights, one_of(vars2)) #&gt; Warning: Unknown columns: `variable_not_in_the_dataframe` #&gt; # A tibble: 336,776 x 3 #&gt; year month day #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # … with 336,770 more rows In the most recent versions of dplyr, the one_of() function is less necessary due to new behavior in the selection functions. The select() function can now accept the name of a vector containing the variable names you wish to select: select(flights, vars) #&gt; Note: Using an external vector in selections is ambiguous. #&gt; ℹ Use `all_of(vars)` instead of `vars` to silence this message. #&gt; ℹ See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;. #&gt; This message is displayed once per session. #&gt; # A tibble: 336,776 x 5 #&gt; year month day dep_delay arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 #&gt; 2 2013 1 1 4 20 #&gt; 3 2013 1 1 2 33 #&gt; 4 2013 1 1 -1 -18 #&gt; 5 2013 1 1 -6 -25 #&gt; 6 2013 1 1 -4 12 #&gt; # … with 336,770 more rows However there is a problem with the previous code. The name vars could refer to a column named vars in flights or a different variable named vars. What th code does will depend on whether or not vars is a column in flights. If vars was a column in flights, then that code would only select the vars column. For example: flights &lt;- mutate(flights, vars = 1) select(flights, vars) #&gt; # A tibble: 336,776 x 1 #&gt; vars #&gt; &lt;dbl&gt; #&gt; 1 1 #&gt; 2 1 #&gt; 3 1 #&gt; 4 1 #&gt; 5 1 #&gt; 6 1 #&gt; # … with 336,770 more rows However, vars is not a column in flights, as is the case, then select will use the value the value of the , and select those columns. If it has the same name or to ensure that it will not conflict with the names of the columns in the data frame, use the !!! (bang-bang-bang) operator. select(flights, !!!vars) #&gt; # A tibble: 336,776 x 5 #&gt; year month day dep_delay arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 #&gt; 2 2013 1 1 4 20 #&gt; 3 2013 1 1 2 33 #&gt; 4 2013 1 1 -1 -18 #&gt; 5 2013 1 1 -6 -25 #&gt; 6 2013 1 1 -4 12 #&gt; # … with 336,770 more rows This behavior, which is used by many tidyverse functions, is an example of what is called non-standard evaluation (NSE) in R. See the dplyr vignette, Programming with dplyr, for more information on this topic. Exercise 5.4.4 Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default? select(flights, contains(&quot;TIME&quot;)) #&gt; # A tibble: 336,776 x 6 #&gt; dep_time sched_dep_time arr_time sched_arr_time air_time time_hour #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; #&gt; 1 517 515 830 819 227 2013-01-01 05:00:00 #&gt; 2 533 529 850 830 227 2013-01-01 05:00:00 #&gt; 3 542 540 923 850 160 2013-01-01 05:00:00 #&gt; 4 544 545 1004 1022 183 2013-01-01 05:00:00 #&gt; 5 554 600 812 837 116 2013-01-01 06:00:00 #&gt; 6 554 558 740 728 150 2013-01-01 05:00:00 #&gt; # … with 336,770 more rows The default behavior for contains() is to ignore case. This may or may not surprise you. If this behavior does not surprise you, that could be why it is the default. Users searching for variable names probably have a better sense of the letters in the variable than their capitalization. A second, technical, reason is that dplyr works with more than R data frames. It can also work with a variety of databases. Some of these database engines have case insensitive column names, so making functions that match variable names case insensitive by default will make the behavior of select() consistent regardless of whether the table is stored as an R data frame or in a database. To change the behavior add the argument ignore.case = FALSE. select(flights, contains(&quot;TIME&quot;, ignore.case = FALSE)) #&gt; # A tibble: 336,776 x 0 5.5 Add new variables with mutate() Exercise 5.5.1 Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. To get the departure times in the number of minutes, divide dep_time by 100 to get the hours since midnight and multiply by 60 and add the remainder of dep_time divided by 100. For example, 1504 represents 15:04 (or 3:04 PM), which is 904 minutes after midnight. To generalize this approach, we need a way to split out the hour-digits from the minute-digits. Dividing by 100 and discarding the remainder using the integer division operator, %/% gives us the following. 1504 %/% 100 #&gt; [1] 15 Instead of %/% could also use / along with trunc() or floor(), but round() would not work. To get the minutes, instead of discarding the remainder of the division by 100, we only want the remainder. So we use the modulo operator, %%, discussed in the Other Useful Functions section. 1504 %% 100 #&gt; [1] 4 Now, we can combine the hours (multiplied by 60 to convert them to minutes) and minutes to get the number of minutes after midnight. 1504 %/% 100 * 60 + 1504 %% 100 #&gt; [1] 904 There is one remaining issue. Midnight is represented by 2400, which would correspond to 1440 minutes since midnight, but it should correspond to 0. After converting all the times to minutes after midnight, x %% 1440 will convert 1440 to zero while keeping all the other times the same. Now we will put it all together. The following code creates a new data frame flights_times with columns dep_time_mins and sched_dep_time_mins. These columns convert dep_time and sched_dep_time, respectively, to minutes since midnight. flights_times &lt;- mutate(flights, dep_time_mins = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440, sched_dep_time_mins = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440 ) # view only relevant columns select( flights_times, dep_time, dep_time_mins, sched_dep_time, sched_dep_time_mins ) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_time_mins sched_dep_time sched_dep_time_mins #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 317 515 315 #&gt; 2 533 333 529 329 #&gt; 3 542 342 540 340 #&gt; 4 544 344 545 345 #&gt; 5 554 354 600 360 #&gt; 6 554 354 558 358 #&gt; # … with 336,770 more rows Looking ahead to the Functions chapter, this is precisely the sort of situation in which it would make sense to write a function to avoid copying and pasting code. We could define a function time2mins(), which converts a vector of times in from the format used in flights to minutes since midnight. time2mins &lt;- function(x) { (x %/% 100 * 60 + x %% 100) %% 1440 } Using time2mins, the previous code simplifies to the following. flights_times &lt;- mutate(flights, dep_time_mins = time2mins(dep_time), sched_dep_time_mins = time2mins(sched_dep_time) ) # show only the relevant columns select( flights_times, dep_time, dep_time_mins, sched_dep_time, sched_dep_time_mins ) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_time_mins sched_dep_time sched_dep_time_mins #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 317 515 315 #&gt; 2 533 333 529 329 #&gt; 3 542 342 540 340 #&gt; 4 544 344 545 345 #&gt; 5 554 354 600 360 #&gt; 6 554 354 558 358 #&gt; # … with 336,770 more rows Exercise 5.5.2 Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it? I expect that air_time is the difference between the arrival (arr_time) and departure times (dep_time). In other words, air_time = arr_time - dep_time. To check that this relationship, I’ll first need to convert the times to a form more amenable to arithmetic operations using the same calculations as the previous exercise. flights_airtime &lt;- mutate(flights, dep_time = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440, arr_time = (arr_time %/% 100 * 60 + arr_time %% 100) %% 1440, air_time_diff = air_time - arr_time + dep_time ) So, does air_time = arr_time - dep_time? If so, there should be no flights with non-zero values of air_time_diff. nrow(filter(flights_airtime, air_time_diff != 0)) #&gt; [1] 327150 It turns out that there are many flights for which air_time != arr_time - dep_time. Other than data errors, I can think of two reasons why air_time would not equal arr_time - dep_time. The flight passes midnight, so arr_time &lt; dep_time. In these cases, the difference in airtime should be by 24 hours (1,440 minutes). The flight crosses time zones, and the total air time will be off by hours (multiples of 60). All flights in flights departed from New York City and are domestic flights in the US. This means that flights will all be to the same or more westerly time zones. Given the time-zones in the US, the differences due to time-zone should be 60 minutes (Central) 120 minutes (Mountain), 180 minutes (Pacific), 240 minutes (Alaska), or 300 minutes (Hawaii). Both of these explanations have clear patterns that I would expect to see if they were true. In particular, in both cases, since time-zones and crossing midnight only affects the hour part of the time, all values of air_time_diff should be divisible by 60. I’ll visually check this hypothesis by plotting the distribution of air_time_diff. If those two explanations are correct, distribution of air_time_diff should comprise only spikes at multiples of 60. ggplot(flights_airtime, aes(x = air_time_diff)) + geom_histogram(binwidth = 1) #&gt; Warning: Removed 9430 rows containing non-finite values (stat_bin). This is not the case. While, the distribution of air_time_diff has modes at multiples of 60 as hypothesized, it shows that there are many flights in which the difference between air time and local arrival and departure times is not divisible by 60. Let’s also look at flights with Los Angeles as a destination. The discrepancy should be 180 minutes. ggplot(filter(flights_airtime, dest == &quot;LAX&quot;), aes(x = air_time_diff)) + geom_histogram(binwidth = 1) #&gt; Warning: Removed 148 rows containing non-finite values (stat_bin). To fix these time-zone issues, I would want to convert all the times to a date-time to handle overnight flights, and from local time to a common time zone, most likely UTC, to handle flights crossing time-zones. The tzone column of nycflights13::airports gives the time-zone of each airport. See the “Dates and Times” for an introduction on working with date and time data. But that still leaves the other differences unexplained. So what else might be going on? There seem to be too many problems for this to be data entry problems, so I’m probably missing something. So, I’ll reread the documentation to make sure that I understand the definitions of arr_time, dep_time, and air_time. The documentation contains a link to the source of the flights data, https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236. This documentation shows that the flights data does not contain the variables TaxiIn, TaxiOff, WheelsIn, and WheelsOff. It appears that the air_time variable refers to flight time, which is defined as the time between wheels-off (take-off) and wheels-in (landing). But the flight time does not include time spent on the runway taxiing to and from gates. With this new understanding of the data, I now know that the relationship between air_time, arr_time, and dep_time is air_time &lt;= arr_time - dep_time, supposing that the time zones of arr_time and dep_time are in the same time zone. Exercise 5.5.3 Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related? I would expect the departure delay (dep_delay) to be equal to the difference between scheduled departure time (sched_dep_time), and actual departure time (dep_time), dep_time - sched_dep_time = dep_delay. As with the previous question, the first step is to convert all times to the number of minutes since midnight. The column, dep_delay_diff, is the difference between the column, dep_delay, and departure delay calculated directly from the scheduled and actual departure times. flights_deptime &lt;- mutate(flights, dep_time_min = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440, sched_dep_time_min = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440, dep_delay_diff = dep_delay - dep_time_min + sched_dep_time_min ) Does dep_delay_diff equal zero for all rows? filter(flights_deptime, dep_delay_diff != 0) #&gt; # A tibble: 1,236 x 22 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 848 1835 853 1001 1950 #&gt; 2 2013 1 2 42 2359 43 518 442 #&gt; 3 2013 1 2 126 2250 156 233 2359 #&gt; 4 2013 1 3 32 2359 33 504 442 #&gt; 5 2013 1 3 50 2145 185 203 2311 #&gt; 6 2013 1 3 235 2359 156 700 437 #&gt; # … with 1,230 more rows, and 14 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, #&gt; # dep_time_min &lt;dbl&gt;, sched_dep_time_min &lt;dbl&gt;, dep_delay_diff &lt;dbl&gt; No. Unlike the last question, time zones are not an issue since we are only considering departure times.3 However, the discrepancies could be because a flight was scheduled to depart before midnight, but was delayed after midnight. All of these discrepancies are exactly equal to 1440 (24 hours), and the flights with these discrepancies were scheduled to depart later in the day. ggplot( filter(flights_deptime, dep_delay_diff &gt; 0), aes(y = sched_dep_time_min, x = dep_delay_diff) ) + geom_point() Thus the only cases in which the departure delay is not equal to the difference in scheduled departure and actual departure times is due to a quirk in how these columns were stored. Exercise 5.5.4 Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank(). The dplyr package provides multiple functions for ranking, which differ in how they handle tied values: row_number(), min_rank(), dense_rank(). To see how they work, let’s create a data frame with duplicate values in a vector and see how ranking functions handle ties. rankme &lt;- tibble( x = c(10, 5, 1, 5, 5) ) rankme &lt;- mutate(rankme, x_row_number = row_number(x), x_min_rank = min_rank(x), x_dense_rank = dense_rank(x) ) arrange(rankme, x) #&gt; # A tibble: 5 x 4 #&gt; x x_row_number x_min_rank x_dense_rank #&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 1 1 #&gt; 2 5 2 2 2 #&gt; 3 5 3 2 2 #&gt; 4 5 4 2 2 #&gt; 5 10 5 5 3 The function row_number() assigns each element a unique value. The result is equivalent to the index (or row) number of each element after sorting the vector, hence its name. Themin_rank() and dense_rank() assign tied values the same rank, but differ in how they assign values to the next rank. For each set of tied values the min_rank() function assigns a rank equal to the number of values less than that tied value plus one. In contrast, the dense_rank() function assigns a rank equal to the number of distinct values less than that tied value plus one. To see the difference between dense_rank() and min_rank() compare the value of rankme$x_min_rank and rankme$x_dense_rank for x = 10. If I had to choose one for presenting rankings to someone else, I would use min_rank() since its results correspond to the most common usage of rankings in sports or other competitions. In the code below, I use all three functions, but since there are no ties in the top 10 flights, the results don’t differ. flights_delayed &lt;- mutate(flights, dep_delay_min_rank = min_rank(desc(dep_delay)), dep_delay_row_number = row_number(desc(dep_delay)), dep_delay_dense_rank = dense_rank(desc(dep_delay)) ) flights_delayed &lt;- filter(flights_delayed, !(dep_delay_min_rank &gt; 10 | dep_delay_row_number &gt; 10 | dep_delay_dense_rank &gt; 10)) flights_delayed &lt;- arrange(flights_delayed, dep_delay_min_rank) print(select(flights_delayed, month, day, carrier, flight, dep_delay, dep_delay_min_rank, dep_delay_row_number, dep_delay_dense_rank), n = Inf) #&gt; # A tibble: 10 x 8 #&gt; month day carrier flight dep_delay dep_delay_min_r… dep_delay_row_n… #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 9 HA 51 1301 1 1 #&gt; 2 6 15 MQ 3535 1137 2 2 #&gt; 3 1 10 MQ 3695 1126 3 3 #&gt; 4 9 20 AA 177 1014 4 4 #&gt; 5 7 22 MQ 3075 1005 5 5 #&gt; 6 4 10 DL 2391 960 6 6 #&gt; 7 3 17 DL 2119 911 7 7 #&gt; 8 6 27 DL 2007 899 8 8 #&gt; 9 7 22 DL 2047 898 9 9 #&gt; 10 12 5 AA 172 896 10 10 #&gt; # … with 1 more variable: dep_delay_dense_rank &lt;int&gt; In addition to the functions covered here, the rank() function provides several more ways of ranking elements. There are other ways to solve this problem that do not using ranking functions. To select the top 10, sort values with arrange() and select the top values with slice: flights_delayed2 &lt;- arrange(flights, desc(dep_delay)) flights_delayed2 &lt;- slice(flights_delayed2, 1:10) select(flights_delayed2, month, day, carrier, flight, dep_delay) #&gt; # A tibble: 10 x 5 #&gt; month day carrier flight dep_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 9 HA 51 1301 #&gt; 2 6 15 MQ 3535 1137 #&gt; 3 1 10 MQ 3695 1126 #&gt; 4 9 20 AA 177 1014 #&gt; 5 7 22 MQ 3075 1005 #&gt; 6 4 10 DL 2391 960 #&gt; # … with 4 more rows Alternatively, we could use the top_n(). flights_delayed3 &lt;- top_n(flights, 10, dep_delay) flights_delayed3 &lt;- arrange(flights_delayed3, desc(dep_delay)) select(flights_delayed3, month, day, carrier, flight, dep_delay) #&gt; # A tibble: 10 x 5 #&gt; month day carrier flight dep_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 9 HA 51 1301 #&gt; 2 6 15 MQ 3535 1137 #&gt; 3 1 10 MQ 3695 1126 #&gt; 4 9 20 AA 177 1014 #&gt; 5 7 22 MQ 3075 1005 #&gt; 6 4 10 DL 2391 960 #&gt; # … with 4 more rows The previous two approaches will always select 10 rows even if there are tied values. Ranking functions provide more control over how tied values are handled. Those approaches will provide the 10 rows with the largest values of dep_delay, while ranking functions can provide all rows with the 10 largest values of dep_delay. If there are no ties, these approaches are equivalent. If there are ties, then which is more appropriate depends on the use. Exercise 5.5.5 What does 1:3 + 1:10 return? Why? The code given in the question returns the following. 1:3 + 1:10 #&gt; Warning in 1:3 + 1:10: longer object length is not a multiple of shorter object #&gt; length #&gt; [1] 2 4 6 5 7 9 8 10 12 11 This is equivalent to the following. c(1 + 1, 2 + 2, 3 + 3, 1 + 4, 2 + 5, 3 + 6, 1 + 7, 2 + 8, 3 + 9, 1 + 10) #&gt; [1] 2 4 6 5 7 9 8 10 12 11 When adding two vectors, R recycles the shorter vector’s values to create a vector of the same length as the longer vector. The code also raises a warning that the shorter vector is not a multiple of the longer vector. A warning is raised since when this occurs, it is often unintended and may be a bug. Exercise 5.5.6 What trigonometric functions does R provide? All trigonometric functions are all described in a single help page, named Trig. You can open the documentation for these functions with ?Trig or by using ? with any of the following functions, for example:?sin. R provides functions for the three primary trigonometric functions: sine (sin()), cosine (cos()), and tangent (tan()). The input angles to all these functions are in radians. x &lt;- seq(-3, 7, by = 1 / 2) sin(pi * x) #&gt; [1] -3.67e-16 -1.00e+00 2.45e-16 1.00e+00 -1.22e-16 -1.00e+00 0.00e+00 #&gt; [8] 1.00e+00 1.22e-16 -1.00e+00 -2.45e-16 1.00e+00 3.67e-16 -1.00e+00 #&gt; [15] -4.90e-16 1.00e+00 6.12e-16 -1.00e+00 -7.35e-16 1.00e+00 8.57e-16 cos(pi * x) #&gt; [1] -1.00e+00 3.06e-16 1.00e+00 -1.84e-16 -1.00e+00 6.12e-17 1.00e+00 #&gt; [8] 6.12e-17 -1.00e+00 -1.84e-16 1.00e+00 3.06e-16 -1.00e+00 -4.29e-16 #&gt; [15] 1.00e+00 5.51e-16 -1.00e+00 -2.45e-15 1.00e+00 -9.80e-16 -1.00e+00 tan(pi * x) #&gt; [1] 3.67e-16 -3.27e+15 2.45e-16 -5.44e+15 1.22e-16 -1.63e+16 0.00e+00 #&gt; [8] 1.63e+16 -1.22e-16 5.44e+15 -2.45e-16 3.27e+15 -3.67e-16 2.33e+15 #&gt; [15] -4.90e-16 1.81e+15 -6.12e-16 4.08e+14 -7.35e-16 -1.02e+15 -8.57e-16 In the previous code, I used the variable pi. R provides the variable pi which is set to the value of the mathematical constant \\(\\pi\\) .4 pi #&gt; [1] 3.14 Although R provides the pi variable, there is nothing preventing a user from changing its value. For example, I could redefine pi to 3.14 or any other value. pi &lt;- 3.14 pi #&gt; [1] 3.14 pi &lt;- &quot;Apple&quot; pi #&gt; [1] &quot;Apple&quot; For that reason, if you are using the builtin pi variable in computations and are paranoid, you may want to always reference it as base::pi. base::pi #&gt; [1] 3.14 In the previous code block, since the angles were in radians, I wrote them as \\(\\pi\\) times some number. Since it is often easier to write radians multiple of \\(\\pi\\), R provides some convenience functions that do that. The function sinpi(x), is equivalent to sin(pi * x). The functions cospi() and tanpi() are similarly defined for the sin and tan functions, respectively. sinpi(x) #&gt; [1] 0 -1 0 1 0 -1 0 1 0 -1 0 1 0 -1 0 1 0 -1 0 1 0 cospi(x) #&gt; [1] -1 0 1 0 -1 0 1 0 -1 0 1 0 -1 0 1 0 -1 0 1 0 -1 tanpi(x) #&gt; Warning in tanpi(x): NaNs produced #&gt; [1] 0 NaN 0 NaN 0 NaN 0 NaN 0 NaN 0 NaN 0 NaN 0 NaN 0 NaN 0 #&gt; [20] NaN 0 R provides the function arc-cosine (acos()), arc-sine (asin()), and arc-tangent (atan()). x &lt;- seq(-1, 1, by = 1 / 4) acos(x) #&gt; [1] 3.142 2.419 2.094 1.823 1.571 1.318 1.047 0.723 0.000 asin(x) #&gt; [1] -1.571 -0.848 -0.524 -0.253 0.000 0.253 0.524 0.848 1.571 atan(x) #&gt; [1] -0.785 -0.644 -0.464 -0.245 0.000 0.245 0.464 0.644 0.785 Finally, R provides the function atan2(). Calling atan2(y, x) returns the angle between the x-axis and the vector from (0,0) to (x, y). atan2(c(1, 0, -1, 0), c(0, 1, 0, -1)) #&gt; [1] 1.57 0.00 -1.57 3.14 5.6 Grouped summaries with summarise() Exercise 5.6.1 Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios: A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time. A flight is always 10 minutes late. A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time. 99% of the time a flight is on time. 1% of the time it’s 2 hours late. Which is more important: arrival delay or departure delay? What this question gets at is a fundamental question of data analysis: the cost function. As analysts, the reason we are interested in flight delay because it is costly to passengers. But it is worth thinking carefully about how it is costly and use that information in ranking and measuring these scenarios. In many scenarios, arrival delay is more important. In most cases, being arriving late is more costly to the passenger since it could disrupt the next stages of their travel, such as connecting flights or scheduled meetings. If a departure is delayed without affecting the arrival time, this delay will not have those affects plans nor does it affect the total time spent traveling. This delay could be beneficial, if less time is spent in the cramped confines of the airplane itself, or a negative, if that delayed time is still spent in the cramped confines of the airplane on the runway. Variation in arrival time is worse than consistency. If a flight is always 30 minutes late and that delay is known, then it is as if the arrival time is that delayed time. The traveler could easily plan for this. But higher variation in flight times makes it harder to plan. Exercise 5.6.2 Come up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()). not_cancelled &lt;- flights %&gt;% filter(!is.na(dep_delay), !is.na(arr_delay)) The first expression is the following. not_cancelled %&gt;% count(dest) #&gt; # A tibble: 104 x 2 #&gt; dest n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ABQ 254 #&gt; 2 ACK 264 #&gt; 3 ALB 418 #&gt; 4 ANC 8 #&gt; 5 ATL 16837 #&gt; 6 AUS 2411 #&gt; # … with 98 more rows The count() function counts the number of instances within each group of variables. Instead of using the count() function, we can combine the group_by() and summarise() verbs. not_cancelled %&gt;% group_by(dest) %&gt;% summarise(n = length(dest)) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 104 x 2 #&gt; dest n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ABQ 254 #&gt; 2 ACK 264 #&gt; 3 ALB 418 #&gt; 4 ANC 8 #&gt; 5 ATL 16837 #&gt; 6 AUS 2411 #&gt; # … with 98 more rows An alternative method for getting the number of observations in a data frame is the function n(). not_cancelled %&gt;% group_by(dest) %&gt;% summarise(n = n()) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 104 x 2 #&gt; dest n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ABQ 254 #&gt; 2 ACK 264 #&gt; 3 ALB 418 #&gt; 4 ANC 8 #&gt; 5 ATL 16837 #&gt; 6 AUS 2411 #&gt; # … with 98 more rows Another alternative to count() is to use group_by() followed by tally(). In fact, count() is effectively a short-cut for group_by() followed by tally(). not_cancelled %&gt;% group_by(tailnum) %&gt;% tally() #&gt; # A tibble: 4,037 x 2 #&gt; tailnum n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 D942DN 4 #&gt; 2 N0EGMQ 352 #&gt; 3 N10156 145 #&gt; 4 N102UW 48 #&gt; 5 N103US 46 #&gt; 6 N104UW 46 #&gt; # … with 4,031 more rows The second expression also uses the count() function, but adds a wt argument. not_cancelled %&gt;% count(tailnum, wt = distance) #&gt; # A tibble: 4,037 x 2 #&gt; tailnum n #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 D942DN 3418 #&gt; 2 N0EGMQ 239143 #&gt; 3 N10156 109664 #&gt; 4 N102UW 25722 #&gt; 5 N103US 24619 #&gt; 6 N104UW 24616 #&gt; # … with 4,031 more rows As before, we can replicate count() by combining the group_by() and summarise() verbs. But this time instead of using length(), we will use sum() with the weighting variable. not_cancelled %&gt;% group_by(tailnum) %&gt;% summarise(n = sum(distance)) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 4,037 x 2 #&gt; tailnum n #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 D942DN 3418 #&gt; 2 N0EGMQ 239143 #&gt; 3 N10156 109664 #&gt; 4 N102UW 25722 #&gt; 5 N103US 24619 #&gt; 6 N104UW 24616 #&gt; # … with 4,031 more rows Like the previous example, we can also use the combination group_by() and tally(). Any arguments to tally() are summed. not_cancelled %&gt;% group_by(tailnum) %&gt;% tally(distance) #&gt; # A tibble: 4,037 x 2 #&gt; tailnum n #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 D942DN 3418 #&gt; 2 N0EGMQ 239143 #&gt; 3 N10156 109664 #&gt; 4 N102UW 25722 #&gt; 5 N103US 24619 #&gt; 6 N104UW 24616 #&gt; # … with 4,031 more rows Exercise 5.6.3 Our definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay)) is slightly suboptimal. Why? Which is the most important column? If a flight never departs, then it won’t arrive. A flight could also depart and not arrive if it crashes, or if it is redirected and lands in an airport other than its intended destination. So the most important column is arr_delay, which indicates the amount of delay in arrival. filter(flights, !is.na(dep_delay), is.na(arr_delay)) %&gt;% select(dep_time, arr_time, sched_arr_time, dep_delay, arr_delay) #&gt; # A tibble: 1,175 x 5 #&gt; dep_time arr_time sched_arr_time dep_delay arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1525 1934 1805 -5 NA #&gt; 2 1528 2002 1647 29 NA #&gt; 3 1740 2158 2020 -5 NA #&gt; 4 1807 2251 2103 29 NA #&gt; 5 1939 29 2151 59 NA #&gt; 6 1952 2358 2207 22 NA #&gt; # … with 1,169 more rows In this data dep_time can be non-missing and arr_delay missing but arr_time not missing. Some further research found that these rows correspond to diverted flights. The BTS database that is the source for the flights table contains additional information for diverted flights that is not included in the nycflights13 data. The source contains a column DivArrDelay with the description: Difference in minutes between scheduled and actual arrival time for a diverted flight reaching scheduled destination. The ArrDelay column remains NULL for all diverted flights. Exercise 5.6.4 Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay? One pattern in cancelled flights per day is that the number of cancelled flights increases with the total number of flights per day. The proportion of cancelled flights increases with the average delay of flights. To answer these questions, use definition of cancelled used in the chapter Section 5.6.3 and the relationship !(is.na(arr_delay) &amp; is.na(dep_delay)) is equal to !is.na(arr_delay) | !is.na(dep_delay) by De Morgan’s law. The first part of the question asks for any pattern in the number of cancelled flights per day. I’ll look at the relationship between the number of cancelled flights per day and the total number of flights in a day. There should be an increasing relationship for two reasons. First, if all flights are equally likely to be cancelled, then days with more flights should have a higher number of cancellations. Second, it is likely that days with more flights would have a higher probability of cancellations because congestion itself can cause delays and any delay would affect more flights, and large delays can lead to cancellations. cancelled_per_day &lt;- flights %&gt;% mutate(cancelled = (is.na(arr_delay) | is.na(dep_delay))) %&gt;% group_by(year, month, day) %&gt;% summarise( cancelled_num = sum(cancelled), flights_num = n(), ) #&gt; `summarise()` regrouping output by &#39;year&#39;, &#39;month&#39; (override with `.groups` argument) Plotting flights_num against cancelled_num shows that the number of flights cancelled increases with the total number of flights. ggplot(cancelled_per_day) + geom_point(aes(x = flights_num, y = cancelled_num)) The second part of the question asks whether there is a relationship between the proportion of flights cancelled and the average departure delay. I implied this in my answer to the first part of the question, when I noted that increasing delays could result in increased cancellations. The question does not specify which delay, so I will show the relationship for both. cancelled_and_delays &lt;- flights %&gt;% mutate(cancelled = (is.na(arr_delay) | is.na(dep_delay))) %&gt;% group_by(year, month, day) %&gt;% summarise( cancelled_prop = mean(cancelled), avg_dep_delay = mean(dep_delay, na.rm = TRUE), avg_arr_delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% ungroup() #&gt; `summarise()` regrouping output by &#39;year&#39;, &#39;month&#39; (override with `.groups` argument) There is a strong increasing relationship between both average departure delay and and average arrival delay and the proportion of cancelled flights. ggplot(cancelled_and_delays) + geom_point(aes(x = avg_dep_delay, y = cancelled_prop)) ggplot(cancelled_and_delays) + geom_point(aes(x = avg_arr_delay, y = cancelled_prop)) Exercise 5.6.5 Which carrier has the worst delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights %&gt;% group_by(carrier, dest) %&gt;% summarise(n())) flights %&gt;% group_by(carrier) %&gt;% summarise(arr_delay = mean(arr_delay, na.rm = TRUE)) %&gt;% arrange(desc(arr_delay)) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 16 x 2 #&gt; carrier arr_delay #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 F9 21.9 #&gt; 2 FL 20.1 #&gt; 3 EV 15.8 #&gt; 4 YV 15.6 #&gt; 5 OO 11.9 #&gt; 6 MQ 10.8 #&gt; # … with 10 more rows What airline corresponds to the &quot;F9&quot; carrier code? filter(airlines, carrier == &quot;F9&quot;) #&gt; # A tibble: 1 x 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 F9 Frontier Airlines Inc. You can get part of the way to disentangling the effects of airports versus bad carriers by comparing the average delay of each carrier to the average delay of flights within a route (flights from the same origin to the same destination). Comparing delays between carriers and within each route disentangles the effect of carriers and airports. A better analysis would compare the average delay of a carrier’s flights to the average delay of all other carrier’s flights within a route. flights %&gt;% filter(!is.na(arr_delay)) %&gt;% # Total delay by carrier within each origin, dest group_by(origin, dest, carrier) %&gt;% summarise( arr_delay = sum(arr_delay), flights = n() ) %&gt;% # Total delay within each origin dest group_by(origin, dest) %&gt;% mutate( arr_delay_total = sum(arr_delay), flights_total = sum(flights) ) %&gt;% # average delay of each carrier - average delay of other carriers ungroup() %&gt;% mutate( arr_delay_others = (arr_delay_total - arr_delay) / (flights_total - flights), arr_delay_mean = arr_delay / flights, arr_delay_diff = arr_delay_mean - arr_delay_others ) %&gt;% # remove NaN values (when there is only one carrier) filter(is.finite(arr_delay_diff)) %&gt;% # average over all airports it flies to group_by(carrier) %&gt;% summarise(arr_delay_diff = mean(arr_delay_diff)) %&gt;% arrange(desc(arr_delay_diff)) #&gt; `summarise()` regrouping output by &#39;origin&#39;, &#39;dest&#39; (override with `.groups` argument) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 15 x 2 #&gt; carrier arr_delay_diff #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 OO 27.3 #&gt; 2 F9 17.3 #&gt; 3 EV 11.0 #&gt; 4 B6 6.41 #&gt; 5 FL 2.57 #&gt; 6 VX -0.202 #&gt; # … with 9 more rows There are more sophisticated ways to do this analysis, however comparing the delay of flights within each route goes a long ways toward disentangling airport and carrier effects. To see a more complete example of this analysis, see this FiveThirtyEight piece. Exercise 5.6.6 What does the sort argument to count() do? When might you use it? The sort argument to count() sorts the results in order of n. You could use this anytime you would run count() followed by arrange(). For example, the following expression counts the number of flights to a destination and sorts the returned data from highest to lowest. flights %&gt;% count(dest, sort = TRUE) #&gt; # A tibble: 105 x 2 #&gt; dest n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ORD 17283 #&gt; 2 ATL 17215 #&gt; 3 LAX 16174 #&gt; 4 BOS 15508 #&gt; 5 MCO 14082 #&gt; 6 CLT 14064 #&gt; # … with 99 more rows 5.7 Grouped mutates (and filters) Exercise 5.7.1 Refer back to the lists of useful mutate and filtering functions. Describe how each operation changes when you combine it with grouping. Summary functions (mean()), offset functions (lead(), lag()), ranking functions (min_rank(), row_number()), operate within each group when used with group_by() in mutate() or filter(). Arithmetic operators (+, -), logical operators (&lt;, ==), modular arithmetic operators (%%, %/%), logarithmic functions (log) are not affected by group_by. Summary functions like mean(), median(), sum(), std() and others covered in the section Useful Summary Functions calculate their values within each group when used with mutate() or filter() and group_by(). tibble(x = 1:9, group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3)) %&gt;% mutate(x_mean = mean(x)) %&gt;% group_by(group) %&gt;% mutate(x_mean_2 = mean(x)) #&gt; # A tibble: 9 x 4 #&gt; # Groups: group [3] #&gt; x group x_mean x_mean_2 #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 a 5 2 #&gt; 2 2 a 5 2 #&gt; 3 3 a 5 2 #&gt; 4 4 b 5 5 #&gt; 5 5 b 5 5 #&gt; 6 6 b 5 5 #&gt; # … with 3 more rows Arithmetic operators +, -, *, /, ^ are not affected by group_by(). tibble(x = 1:9, group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3)) %&gt;% mutate(y = x + 2) %&gt;% group_by(group) %&gt;% mutate(z = x + 2) #&gt; # A tibble: 9 x 4 #&gt; # Groups: group [3] #&gt; x group y z #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 a 3 3 #&gt; 2 2 a 4 4 #&gt; 3 3 a 5 5 #&gt; 4 4 b 6 6 #&gt; 5 5 b 7 7 #&gt; 6 6 b 8 8 #&gt; # … with 3 more rows The modular arithmetic operators %/% and %% are not affected by group_by() tibble(x = 1:9, group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3)) %&gt;% mutate(y = x %% 2) %&gt;% group_by(group) %&gt;% mutate(z = x %% 2) #&gt; # A tibble: 9 x 4 #&gt; # Groups: group [3] #&gt; x group y z #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 a 1 1 #&gt; 2 2 a 0 0 #&gt; 3 3 a 1 1 #&gt; 4 4 b 0 0 #&gt; 5 5 b 1 1 #&gt; 6 6 b 0 0 #&gt; # … with 3 more rows The logarithmic functions log(), log2(), and log10() are not affected by group_by(). tibble(x = 1:9, group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3)) %&gt;% mutate(y = log(x)) %&gt;% group_by(group) %&gt;% mutate(z = log(x)) #&gt; # A tibble: 9 x 4 #&gt; # Groups: group [3] #&gt; x group y z #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 a 0 0 #&gt; 2 2 a 0.693 0.693 #&gt; 3 3 a 1.10 1.10 #&gt; 4 4 b 1.39 1.39 #&gt; 5 5 b 1.61 1.61 #&gt; 6 6 b 1.79 1.79 #&gt; # … with 3 more rows The offset functions lead() and lag() respect the groupings in group_by(). The functions lag() and lead() will only return values within each group. tibble(x = 1:9, group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3)) %&gt;% group_by(group) %&gt;% mutate(lag_x = lag(x), lead_x = lead(x)) #&gt; # A tibble: 9 x 4 #&gt; # Groups: group [3] #&gt; x group lag_x lead_x #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 a NA 2 #&gt; 2 2 a 1 3 #&gt; 3 3 a 2 NA #&gt; 4 4 b NA 5 #&gt; 5 5 b 4 6 #&gt; 6 6 b 5 NA #&gt; # … with 3 more rows The cumulative and rolling aggregate functions cumsum(), cumprod(), cummin(), cummax(), and cummean() calculate values within each group. tibble(x = 1:9, group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3)) %&gt;% mutate(x_cumsum = cumsum(x)) %&gt;% group_by(group) %&gt;% mutate(x_cumsum_2 = cumsum(x)) #&gt; # A tibble: 9 x 4 #&gt; # Groups: group [3] #&gt; x group x_cumsum x_cumsum_2 #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 a 1 1 #&gt; 2 2 a 3 3 #&gt; 3 3 a 6 6 #&gt; 4 4 b 10 4 #&gt; 5 5 b 15 9 #&gt; 6 6 b 21 15 #&gt; # … with 3 more rows Logical comparisons, &lt;, &lt;=, &gt;, &gt;=, !=, and == are not affected by group_by(). tibble(x = 1:9, y = 9:1, group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3)) %&gt;% mutate(x_lte_y = x &lt;= y) %&gt;% group_by(group) %&gt;% mutate(x_lte_y_2 = x &lt;= y) #&gt; # A tibble: 9 x 5 #&gt; # Groups: group [3] #&gt; x y group x_lte_y x_lte_y_2 #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; #&gt; 1 1 9 a TRUE TRUE #&gt; 2 2 8 a TRUE TRUE #&gt; 3 3 7 a TRUE TRUE #&gt; 4 4 6 b TRUE TRUE #&gt; 5 5 5 b TRUE TRUE #&gt; 6 6 4 b FALSE FALSE #&gt; # … with 3 more rows Ranking functions like min_rank() work within each group when used with group_by(). tibble(x = 1:9, group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3)) %&gt;% mutate(rnk = min_rank(x)) %&gt;% group_by(group) %&gt;% mutate(rnk2 = min_rank(x)) #&gt; # A tibble: 9 x 4 #&gt; # Groups: group [3] #&gt; x group rnk rnk2 #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 a 1 1 #&gt; 2 2 a 2 2 #&gt; 3 3 a 3 3 #&gt; 4 4 b 4 1 #&gt; 5 5 b 5 2 #&gt; 6 6 b 6 3 #&gt; # … with 3 more rows Though not asked in the question, note that arrange() ignores groups when sorting values. tibble(x = runif(9), group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3)) %&gt;% group_by(group) %&gt;% arrange(x) #&gt; # A tibble: 9 x 2 #&gt; # Groups: group [3] #&gt; x group #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 0.00740 b #&gt; 2 0.0808 a #&gt; 3 0.157 b #&gt; 4 0.290 c #&gt; 5 0.466 b #&gt; 6 0.498 c #&gt; # … with 3 more rows However, the order of values from arrange() can interact with groups when used with functions that rely on the ordering of elements, such as lead(), lag(), or cumsum(). tibble(group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3), x = runif(9)) %&gt;% group_by(group) %&gt;% arrange(x) %&gt;% mutate(lag_x = lag(x)) #&gt; # A tibble: 9 x 3 #&gt; # Groups: group [3] #&gt; group x lag_x #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 b 0.0342 NA #&gt; 2 c 0.0637 NA #&gt; 3 a 0.175 NA #&gt; 4 c 0.196 0.0637 #&gt; 5 b 0.320 0.0342 #&gt; 6 b 0.402 0.320 #&gt; # … with 3 more rows Exercise 5.7.2 Which plane (tailnum) has the worst on-time record? The question does not define a way to measure on-time record, so I will consider two metrics: proportion of flights not delayed or cancelled, and mean arrival delay. The first metric is the proportion of not-cancelled and on-time flights. I use the presence of an arrival time as an indicator that a flight was not cancelled. However, there are many planes that have never flown an on-time flight. Additionally, many of the planes that have the lowest proportion of on-time flights have only flown a small number of flights. flights %&gt;% filter(!is.na(tailnum)) %&gt;% mutate(on_time = !is.na(arr_time) &amp; (arr_delay &lt;= 0)) %&gt;% group_by(tailnum) %&gt;% summarise(on_time = mean(on_time), n = n()) %&gt;% filter(min_rank(on_time) == 1) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 110 x 3 #&gt; tailnum on_time n #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 N121DE 0 2 #&gt; 2 N136DL 0 1 #&gt; 3 N143DA 0 1 #&gt; 4 N17627 0 2 #&gt; 5 N240AT 0 5 #&gt; 6 N26906 0 1 #&gt; # … with 104 more rows So, I will remove planes that flew at least 20 flights. The choice of 20 was chosen because it round number near the first quartile of the number of flights by plane.56 quantile(count(flights, tailnum)$n) #&gt; 0% 25% 50% 75% 100% #&gt; 1 23 54 110 2512 The plane with the worst on time record that flew at least 20 flights is: flights %&gt;% filter(!is.na(tailnum), is.na(arr_time) | !is.na(arr_delay)) %&gt;% mutate(on_time = !is.na(arr_time) &amp; (arr_delay &lt;= 0)) %&gt;% group_by(tailnum) %&gt;% summarise(on_time = mean(on_time), n = n()) %&gt;% filter(n &gt;= 20) %&gt;% filter(min_rank(on_time) == 1) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 1 x 3 #&gt; tailnum on_time n #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 N988AT 0.189 37 There are cases where arr_delay is missing but arr_time is not missing. I have not debugged the cause of this bad data, so these rows are dropped for the purposes of this exercise. The second metric is the mean minutes delayed. As with the previous metric, I will only consider planes which flew least 20 flights. A different plane has the worst on-time record when measured as average minutes delayed. flights %&gt;% filter(!is.na(arr_delay)) %&gt;% group_by(tailnum) %&gt;% summarise(arr_delay = mean(arr_delay), n = n()) %&gt;% filter(n &gt;= 20) %&gt;% filter(min_rank(desc(arr_delay)) == 1) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 1 x 3 #&gt; tailnum arr_delay n #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 N203FR 59.1 41 Exercise 5.7.3 What time of day should you fly if you want to avoid delays as much as possible? Let’s group by the hour of the flight. The earlier the flight is scheduled, the lower its expected delay. This is intuitive as delays will affect later flights. Morning flights have fewer (if any) previous flights that can delay them. flights %&gt;% group_by(hour) %&gt;% summarise(arr_delay = mean(arr_delay, na.rm = TRUE)) %&gt;% arrange(arr_delay) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 20 x 2 #&gt; hour arr_delay #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 7 -5.30 #&gt; 2 5 -4.80 #&gt; 3 6 -3.38 #&gt; 4 9 -1.45 #&gt; 5 8 -1.11 #&gt; 6 10 0.954 #&gt; # … with 14 more rows Exercise 5.7.4 For each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination. The key to answering this question is to only include delayed flights when calculating the total delay and proportion of delay. flights %&gt;% filter(arr_delay &gt; 0) %&gt;% group_by(dest) %&gt;% mutate( arr_delay_total = sum(arr_delay), arr_delay_prop = arr_delay / arr_delay_total ) %&gt;% select(dest, month, day, dep_time, carrier, flight, arr_delay, arr_delay_prop) %&gt;% arrange(dest, desc(arr_delay_prop)) #&gt; # A tibble: 133,004 x 8 #&gt; # Groups: dest [103] #&gt; dest month day dep_time carrier flight arr_delay arr_delay_prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ABQ 7 22 2145 B6 1505 153 0.0341 #&gt; 2 ABQ 12 14 2223 B6 65 149 0.0332 #&gt; 3 ABQ 10 15 2146 B6 65 138 0.0308 #&gt; 4 ABQ 7 23 2206 B6 1505 137 0.0305 #&gt; 5 ABQ 12 17 2220 B6 65 136 0.0303 #&gt; 6 ABQ 7 10 2025 B6 1505 126 0.0281 #&gt; # … with 132,998 more rows There is some ambiguity in the meaning of the term flights in the question. The first example defined a flight as a row in the flights table, which is a trip by an aircraft from an airport at a particular date and time. However, flight could also refer to the flight number, which is the code a carrier uses for an airline service of a route. For example, AA1 is the flight number of the 09:00 American Airlines flight between JFK and LAX. The flight number is contained in the flights$flight column, though what is called a “flight” is a combination of the flights$carrier and flights$flight columns. flights %&gt;% filter(arr_delay &gt; 0) %&gt;% group_by(dest, origin, carrier, flight) %&gt;% summarise(arr_delay = sum(arr_delay)) %&gt;% group_by(dest) %&gt;% mutate( arr_delay_prop = arr_delay / sum(arr_delay) ) %&gt;% arrange(dest, desc(arr_delay_prop)) %&gt;% select(carrier, flight, origin, dest, arr_delay_prop) #&gt; `summarise()` regrouping output by &#39;dest&#39;, &#39;origin&#39;, &#39;carrier&#39; (override with `.groups` argument) #&gt; # A tibble: 8,834 x 5 #&gt; # Groups: dest [103] #&gt; carrier flight origin dest arr_delay_prop #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 B6 1505 JFK ABQ 0.567 #&gt; 2 B6 65 JFK ABQ 0.433 #&gt; 3 B6 1191 JFK ACK 0.475 #&gt; 4 B6 1491 JFK ACK 0.414 #&gt; 5 B6 1291 JFK ACK 0.0898 #&gt; 6 B6 1195 JFK ACK 0.0208 #&gt; # … with 8,828 more rows Exercise 5.7.5 Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() explore how the delay of a flight is related to the delay of the immediately preceding flight. This calculates the departure delay of the preceding flight from the same airport. lagged_delays &lt;- flights %&gt;% arrange(origin, month, day, dep_time) %&gt;% group_by(origin) %&gt;% mutate(dep_delay_lag = lag(dep_delay)) %&gt;% filter(!is.na(dep_delay), !is.na(dep_delay_lag)) This plots the relationship between the mean delay of a flight for all values of the previous flight. For delays less than two hours, the relationship between the delay of the preceding flight and the current flight is nearly a line. After that the relationship becomes more variable, as long-delayed flights are interspersed with flights leaving on-time. After about 8-hours, a delayed flight is likely to be followed by a flight leaving on time. lagged_delays %&gt;% group_by(dep_delay_lag) %&gt;% summarise(dep_delay_mean = mean(dep_delay)) %&gt;% ggplot(aes(y = dep_delay_mean, x = dep_delay_lag)) + geom_point() + scale_x_continuous(breaks = seq(0, 1500, by = 120)) + labs(y = &quot;Departure Delay&quot;, x = &quot;Previous Departure Delay&quot;) The overall relationship looks similar in all three origin airports. lagged_delays %&gt;% group_by(origin, dep_delay_lag) %&gt;% summarise(dep_delay_mean = mean(dep_delay)) %&gt;% ggplot(aes(y = dep_delay_mean, x = dep_delay_lag)) + geom_point() + facet_wrap(~ origin, ncol=1) + labs(y = &quot;Departure Delay&quot;, x = &quot;Previous Departure Delay&quot;) #&gt; `summarise()` regrouping output by &#39;origin&#39; (override with `.groups` argument) Exercise 5.7.6 Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air? When calculating this answer we should only compare flights within the same (origin, destination) pair. To find unusual observations, we need to first put them on the same scale. I will standardize values by subtracting the mean from each and then dividing each by the standard deviation. \\[ \\mathsf{standardized}(x) = \\frac{x - \\mathsf{mean}(x)}{\\mathsf{sd}(x)} . \\] A standardized variable is often called a \\(z\\)-score. The units of the standardized variable are standard deviations from the mean. This will put the flight times from different routes on the same scale. The larger the magnitude of the standardized variable for an observation, the more unusual the observation is. Flights with negative values of the standardized variable are faster than the mean flight for that route, while those with positive values are slower than the mean flight for that route. standardized_flights &lt;- flights %&gt;% filter(!is.na(air_time)) %&gt;% group_by(dest, origin) %&gt;% mutate( air_time_mean = mean(air_time), air_time_sd = sd(air_time), n = n() ) %&gt;% ungroup() %&gt;% mutate(air_time_standard = (air_time - air_time_mean) / (air_time_sd + 1)) I add 1 to the denominator and numerator to avoid dividing by zero. Note that the ungroup() here is not necessary. However, I will be using this data frame later. Through experience, I have found that I have fewer bugs when I keep a data frame grouped for only those verbs that need it. If I did not ungroup() this data frame, the arrange() used later would not work as expected. It is better to err on the side of using ungroup() when unnecessary. The distribution of the standardized air flights has long right tail. ggplot(standardized_flights, aes(x = air_time_standard)) + geom_density() #&gt; Warning: Removed 4 rows containing non-finite values (stat_density). Unusually fast flights are those flights with the smallest standardized values. standardized_flights %&gt;% arrange(air_time_standard) %&gt;% select( carrier, flight, origin, dest, month, day, air_time, air_time_mean, air_time_standard ) %&gt;% head(10) %&gt;% print(width = Inf) #&gt; # A tibble: 10 x 9 #&gt; carrier flight origin dest month day air_time air_time_mean #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 DL 1499 LGA ATL 5 25 65 114. #&gt; 2 EV 4667 EWR MSP 7 2 93 151. #&gt; 3 EV 4292 EWR GSP 5 13 55 93.2 #&gt; 4 EV 3805 EWR BNA 3 23 70 115. #&gt; 5 EV 4687 EWR CVG 9 29 62 96.1 #&gt; 6 B6 2002 JFK BUF 11 10 38 57.1 #&gt; air_time_standard #&gt; &lt;dbl&gt; #&gt; 1 -4.56 #&gt; 2 -4.46 #&gt; 3 -4.20 #&gt; 4 -3.73 #&gt; 5 -3.60 #&gt; 6 -3.38 #&gt; # … with 4 more rows I used width = Inf to ensure that all columns will be printed. The fastest flight is DL1499 from LGA to ATL which departed on 2013-05-25 at 17:09. It has an air time of 65 minutes, compared to an average flight time of 114 minutes for its route. This is 4.6 standard deviations below the average flight on its route. It is important to note that this does not necessarily imply that there was a data entry error. We should check these flights to see whether there was some reason for the difference. It may be that we are missing some piece of information that explains these unusual times. A potential issue with the way that we standardized the flights is that the mean and standard deviation used to calculate are sensitive to outliers and outliers is what we are looking for. Instead of standardizing variables with the mean and variance, we could use the median as a measure of central tendency and the interquartile range (IQR) as a measure of spread. The median and IQR are more resistant to outliers than the mean and standard deviation. The following method uses the median and inter-quartile range, which are less sensitive to outliers. standardized_flights2 &lt;- flights %&gt;% filter(!is.na(air_time)) %&gt;% group_by(dest, origin) %&gt;% mutate( air_time_median = median(air_time), air_time_iqr = IQR(air_time), n = n(), air_time_standard = (air_time - air_time_median) / air_time_iqr) The distribution of the standardized air flights using this new definition also has long right tail of slow flights. ggplot(standardized_flights2, aes(x = air_time_standard)) + geom_density() #&gt; Warning: Removed 4 rows containing non-finite values (stat_density). Unusually fast flights are those flights with the smallest standardized values. standardized_flights2 %&gt;% arrange(air_time_standard) %&gt;% select( carrier, flight, origin, dest, month, day, air_time, air_time_median, air_time_standard ) %&gt;% head(10) %&gt;% print(width = Inf) #&gt; # A tibble: 10 x 9 #&gt; # Groups: dest, origin [10] #&gt; carrier flight origin dest month day air_time air_time_median #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 EV 4667 EWR MSP 7 2 93 149 #&gt; 2 DL 1499 LGA ATL 5 25 65 112 #&gt; 3 US 2132 LGA BOS 3 2 21 37 #&gt; 4 B6 30 JFK ROC 3 25 35 51 #&gt; 5 B6 2002 JFK BUF 11 10 38 57 #&gt; 6 EV 4292 EWR GSP 5 13 55 92 #&gt; air_time_standard #&gt; &lt;dbl&gt; #&gt; 1 -3.5 #&gt; 2 -3.36 #&gt; 3 -3.2 #&gt; 4 -3.2 #&gt; 5 -3.17 #&gt; 6 -3.08 #&gt; # … with 4 more rows All of these answers have relied only on using a distribution of comparable observations to find unusual observations. In this case, the comparable observations were flights from the same origin to the same destination. Apart from our knowledge that flights from the same origin to the same destination should have similar air times, we have not used any other domain-specific knowledge. But we know much more about this problem. The most obvious piece of knowledge we have is that we know that flights cannot travel back in time, so there should never be a flight with a negative airtime. But we also know that aircraft have maximum speeds. While different aircraft have different cruising speeds, commercial airliners typically cruise at air speeds around 547–575 mph. Calculating the ground speed of aircraft is complicated by the way in which winds, especially the influence of wind, especially jet streams, on the ground-speed of flights. A strong tailwind can increase ground-speed of the aircraft by 200 mph. Apart from the retired Concorde. For example, in 2018, a transatlantic flight traveled at 770 mph due to a strong jet stream tailwind. This means that any flight traveling at speeds greater than 800 mph is implausible, and it may be worth checking flights traveling at greater than 600 or 700 mph. Ground speed could also be used to identify aircraft flying implausibly slow. Joining flights data with the air craft type in the planes table and getting information about typical or top speeds of those aircraft could provide a more detailed way to identify implausibly fast or slow flights. Additional data on high altitude wind speeds at the time of the flight would further help. Knowing the substance of the data analysis at hand is one of the most important tools of a data scientist. The tools of statistics are a complement, not a substitute, for that knowledge. With that in mind, Let’s plot the distribution of the ground speed of flights. The modal flight in this data has a ground speed of between 400 and 500 mph. The distribution of ground speeds has a large left tail of slower flights below 400 mph constituting the majority. There are very few flights with a ground speed over 500 mph. flights %&gt;% mutate(mph = distance / (air_time / 60)) %&gt;% ggplot(aes(x = mph)) + geom_histogram(binwidth = 10) #&gt; Warning: Removed 9430 rows containing non-finite values (stat_bin). The fastest flight is the same one identified as the largest outlier earlier. Its ground speed was 703 mph. This is fast for a commercial jet, but not impossible. flights %&gt;% mutate(mph = distance / (air_time / 60)) %&gt;% arrange(desc(mph)) %&gt;% select(mph, flight, carrier, flight, month, day, dep_time) %&gt;% head(5) #&gt; # A tibble: 5 x 6 #&gt; mph flight carrier month day dep_time #&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 703. 1499 DL 5 25 1709 #&gt; 2 650. 4667 EV 7 2 1558 #&gt; 3 648 4292 EV 5 13 2040 #&gt; 4 641. 3805 EV 3 23 1914 #&gt; 5 591. 1902 DL 1 12 1559 One explanation for unusually fast flights is that they are “making up time” in the air by flying faster. Commercial aircraft do not fly at their top speed since the airlines are also concerned about fuel consumption. But, if a flight is delayed on the ground, it may fly faster than usual in order to avoid a late arrival. So, I would expect that some of the unusually fast flights were delayed on departure. flights %&gt;% mutate(mph = distance / (air_time / 60)) %&gt;% arrange(desc(mph)) %&gt;% select( origin, dest, mph, year, month, day, dep_time, flight, carrier, dep_delay, arr_delay ) #&gt; # A tibble: 336,776 x 11 #&gt; origin dest mph year month day dep_time flight carrier dep_delay #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 LGA ATL 703. 2013 5 25 1709 1499 DL 9 #&gt; 2 EWR MSP 650. 2013 7 2 1558 4667 EV 45 #&gt; 3 EWR GSP 648 2013 5 13 2040 4292 EV 15 #&gt; 4 EWR BNA 641. 2013 3 23 1914 3805 EV 4 #&gt; 5 LGA PBI 591. 2013 1 12 1559 1902 DL -1 #&gt; 6 JFK SJU 564 2013 11 17 650 315 DL -5 #&gt; # … with 336,770 more rows, and 1 more variable: arr_delay &lt;dbl&gt; head(5) #&gt; [1] 5 Five of the top ten flights had departure delays, and three of those were able to make up that time in the air and arrive ahead of schedule. Overall, there were a few flights that seemed unusually fast, but they all fall into the realm of plausibility and likely are not data entry problems. [Ed. Please correct me if I am missing something] The second part of the question asks us to compare flights to the fastest flight on a route to find the flights most delayed in the air. I will calculate the amount a flight is delayed in air in two ways. The first is the absolute delay, defined as the number of minutes longer than the fastest flight on that route,air_time - min(air_time). The second is the relative delay, which is the percentage increase in air time relative to the time of the fastest flight along that route, (air_time - min(air_time)) / min(air_time) * 100. air_time_delayed &lt;- flights %&gt;% group_by(origin, dest) %&gt;% mutate( air_time_min = min(air_time, na.rm = TRUE), air_time_delay = air_time - air_time_min, air_time_delay_pct = air_time_delay / air_time_min * 100 ) #&gt; Warning in min(air_time, na.rm = TRUE): no non-missing arguments to min; #&gt; returning Inf The most delayed flight in air in minutes was DL841 from JFK to SFO which departed on 2013-07-28 at 17:27. It took 189 minutes longer than the flight with the shortest air time on its route. air_time_delayed %&gt;% arrange(desc(air_time_delay)) %&gt;% select( air_time_delay, carrier, flight, origin, dest, year, month, day, dep_time, air_time, air_time_min ) %&gt;% head() %&gt;% print(width = Inf) #&gt; # A tibble: 6 x 11 #&gt; # Groups: origin, dest [5] #&gt; air_time_delay carrier flight origin dest year month day dep_time air_time #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 189 DL 841 JFK SFO 2013 7 28 1727 490 #&gt; 2 165 DL 426 JFK LAX 2013 11 22 1812 440 #&gt; 3 163 AA 575 JFK EGE 2013 1 28 1806 382 #&gt; 4 147 DL 17 JFK LAX 2013 7 10 1814 422 #&gt; 5 145 UA 745 LGA DEN 2013 9 10 1513 331 #&gt; 6 143 UA 587 EWR LAS 2013 11 22 2142 399 #&gt; air_time_min #&gt; &lt;dbl&gt; #&gt; 1 301 #&gt; 2 275 #&gt; 3 219 #&gt; 4 275 #&gt; 5 186 #&gt; 6 256 The most delayed flight in air as a percentage of the fastest flight along that route was US2136 from LGA to BOS departing on 2013-06-17 at 16:52. It took 410% longer than the flight with the shortest air time on its route. air_time_delayed %&gt;% arrange(desc(air_time_delay)) %&gt;% select( air_time_delay_pct, carrier, flight, origin, dest, year, month, day, dep_time, air_time, air_time_min ) %&gt;% head() %&gt;% print(width = Inf) #&gt; # A tibble: 6 x 11 #&gt; # Groups: origin, dest [5] #&gt; air_time_delay_pct carrier flight origin dest year month day dep_time #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 62.8 DL 841 JFK SFO 2013 7 28 1727 #&gt; 2 60 DL 426 JFK LAX 2013 11 22 1812 #&gt; 3 74.4 AA 575 JFK EGE 2013 1 28 1806 #&gt; 4 53.5 DL 17 JFK LAX 2013 7 10 1814 #&gt; 5 78.0 UA 745 LGA DEN 2013 9 10 1513 #&gt; 6 55.9 UA 587 EWR LAS 2013 11 22 2142 #&gt; air_time air_time_min #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 490 301 #&gt; 2 440 275 #&gt; 3 382 219 #&gt; 4 422 275 #&gt; 5 331 186 #&gt; 6 399 256 Exercise 5.7.7 Find all destinations that are flown by at least two carriers. Use that information to rank the carriers. To restate this question, we are asked to rank airlines by the number of destinations that they fly to, considering only those airports that are flown to by two or more airlines. There are two steps to calculating this ranking. First, find all airports serviced by two or more carriers. Then, rank carriers by the number of those destinations that they service. flights %&gt;% # find all airports with &gt; 1 carrier group_by(dest) %&gt;% mutate(n_carriers = n_distinct(carrier)) %&gt;% filter(n_carriers &gt; 1) %&gt;% # rank carriers by numer of destinations group_by(carrier) %&gt;% summarize(n_dest = n_distinct(dest)) %&gt;% arrange(desc(n_dest)) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 16 x 2 #&gt; carrier n_dest #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 EV 51 #&gt; 2 9E 48 #&gt; 3 UA 42 #&gt; 4 DL 39 #&gt; 5 B6 35 #&gt; 6 AA 19 #&gt; # … with 10 more rows The carrier &quot;EV&quot; flies to the most destinations, considering only airports flown to by two or more carriers. What airline does the &quot;EV&quot; carrier code correspond to? filter(airlines, carrier == &quot;EV&quot;) #&gt; # A tibble: 1 x 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 EV ExpressJet Airlines Inc. Unless you know the airplane industry, it is likely that you don’t recognize ExpressJet; I certainly didn’t. It is a regional airline that partners with major airlines to fly from hubs (larger airports) to smaller airports. This means that many of the shorter flights of major carriers are operated by ExpressJet. This business model explains why ExpressJet services the most destinations. Among the airlines that fly to only one destination from New York are Alaska Airlines and Hawaiian Airlines. filter(airlines, carrier %in% c(&quot;AS&quot;, &quot;F9&quot;, &quot;HA&quot;)) #&gt; # A tibble: 3 x 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 AS Alaska Airlines Inc. #&gt; 2 F9 Frontier Airlines Inc. #&gt; 3 HA Hawaiian Airlines Inc. Exercise 5.7.8 For each plane, count the number of flights before the first delay of greater than 1 hour. The question does not specify arrival or departure delay. I consider dep_delay in this answer, though similar code could be used for arr_delay. flights %&gt;% # sort in increasing order select(tailnum, year, month,day, dep_delay) %&gt;% filter(!is.na(dep_delay)) %&gt;% arrange(tailnum, year, month, day) %&gt;% group_by(tailnum) %&gt;% # cumulative number of flights delayed over one hour mutate(cumulative_hr_delays = cumsum(dep_delay &gt; 60)) %&gt;% # count the number of flights == 0 summarise(total_flights = sum(cumulative_hr_delays &lt; 1)) %&gt;% arrange(total_flights) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 4,037 x 2 #&gt; tailnum total_flights #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 D942DN 0 #&gt; 2 N10575 0 #&gt; 3 N11106 0 #&gt; 4 N11109 0 #&gt; 5 N11187 0 #&gt; 6 N11199 0 #&gt; # … with 4,031 more rows The exception is flights on the days on which daylight savings started (March 10) or ended (November 3). Since in the US, daylight savings goes into effect at 2 a.m., and generally flights are not scheduled to depart between midnight and 2 a.m., the only flights which would be scheduled to depart in Eastern Daylight Savings Time (Eastern Standard Time) time but departed in Eastern Standard Time (Eastern Daylight Savings Time), would have been scheduled before midnight, meaning they were delayed across days. If time zones seem annoying, it is not your imagination. They are. I recommend this video, The Problem with Time &amp; Timezones - Computerphile.↩ Yes, technically, base::pi is an approximation of \\(\\pi\\) to seven digits of precision. Don’t @ me.↩ We could address this issue using a statistical model, but that is outside the scope of this text.↩ The count() function is introduced in Chapter 5.6. It returns the count of rows by group. In this case, the number of rows in flights for each tailnum. The data frame that count() returns has columns for the groups, and a column n, which contains that count.↩ "],
["workflow-scripts.html", "6 Workflow: scripts Exercise 6.1 Exercise 6.2", " 6 Workflow: scripts Exercise 6.1 Go to the RStudio Tips twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it! The current timeline of @rstudiotips is displayed below. Tweets by rstudiotips Exercise 6.2 What other common mistakes will RStudio diagnostics report? Read https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics to find out. You should read that page, but some other diagnostics for R code include the following. Check for missing, unmatched, partially matched, and too many arguments to functions. Warn if a variable is not defined. Warn if a variable is defined but not used. Check that the code style conforms to the tidyverse style guide. "],
["exploratory-data-analysis.html", "7 Exploratory Data Analysis 7.1 Introduction 7.2 Questions 7.3 Variation 7.4 Missing values 7.5 Covariation 7.6 Patterns and models 7.7 ggplot2 calls 7.8 Learning more", " 7 Exploratory Data Analysis 7.1 Introduction This will also use data from the nycflights13 package. The ggbeeswarm, lvplot, and ggstance packages provide some additional functions used in some solutions. library(&quot;tidyverse&quot;) library(&quot;nycflights13&quot;) library(&quot;ggbeeswarm&quot;) library(&quot;lvplot&quot;) library(&quot;ggstance&quot;) 7.2 Questions 7.3 Variation Exercise 7.3.1 Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth. First, I’ll calculate summary statistics for these variables and plot their distributions. summary(select(diamonds, x, y, z)) #&gt; x y z #&gt; Min. : 0.00 Min. : 0.0 Min. : 0.0 #&gt; 1st Qu.: 4.71 1st Qu.: 4.7 1st Qu.: 2.9 #&gt; Median : 5.70 Median : 5.7 Median : 3.5 #&gt; Mean : 5.73 Mean : 5.7 Mean : 3.5 #&gt; 3rd Qu.: 6.54 3rd Qu.: 6.5 3rd Qu.: 4.0 #&gt; Max. :10.74 Max. :58.9 Max. :31.8 ggplot(diamonds) + geom_histogram(mapping = aes(x = x), binwidth = 0.01) ggplot(diamonds) + geom_histogram(mapping = aes(x = y), binwidth = 0.01) ggplot(diamonds) + geom_histogram(mapping = aes(x = z), binwidth = 0.01) There several noticeable features of the distributions: x and y are larger than z, there are outliers, they are all right skewed, and they are multimodal or “spiky”. The typical values of x and y are larger than z, with x and y having inter-quartile ranges of 4.7–6.5, while z has an inter-quartile range of 2.9–4.0. There are two types of outliers in this data. Some diamonds have values of zero and some have abnormally large values of x, y, or z. summary(select(diamonds, x, y, z)) #&gt; x y z #&gt; Min. : 0.00 Min. : 0.0 Min. : 0.0 #&gt; 1st Qu.: 4.71 1st Qu.: 4.7 1st Qu.: 2.9 #&gt; Median : 5.70 Median : 5.7 Median : 3.5 #&gt; Mean : 5.73 Mean : 5.7 Mean : 3.5 #&gt; 3rd Qu.: 6.54 3rd Qu.: 6.5 3rd Qu.: 4.0 #&gt; Max. :10.74 Max. :58.9 Max. :31.8 These appear to be either data entry errors, or an undocumented convention in the dataset for indicating missing values. An alternative hypothesis would be that values of zero are the result of rounding values like 0.002 down, but since there are no diamonds with values of 0.01, that does not seem to be the case. filter(diamonds, x == 0 | y == 0 | z == 0) #&gt; # A tibble: 20 x 10 #&gt; carat cut color clarity depth table price x y z #&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Premium G SI2 59.1 59 3142 6.55 6.48 0 #&gt; 2 1.01 Premium H I1 58.1 59 3167 6.66 6.6 0 #&gt; 3 1.1 Premium G SI2 63 59 3696 6.5 6.47 0 #&gt; 4 1.01 Premium F SI2 59.2 58 3837 6.5 6.47 0 #&gt; 5 1.5 Good G I1 64 61 4731 7.15 7.04 0 #&gt; 6 1.07 Ideal F SI2 61.6 56 4954 0 6.62 0 #&gt; # … with 14 more rows There are also some diamonds with values of y and z that are abnormally large. There are diamonds with y == 58.9 and y == 31.8, and one with z == 31.8. These are probably data errors since the values do not seem in line with the values of the other variables. diamonds %&gt;% arrange(desc(y)) %&gt;% head() #&gt; # A tibble: 6 x 10 #&gt; carat cut color clarity depth table price x y z #&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 Premium H SI2 58.9 57 12210 8.09 58.9 8.06 #&gt; 2 0.51 Ideal E VS1 61.8 55 2075 5.15 31.8 5.12 #&gt; 3 5.01 Fair J I1 65.5 59 18018 10.7 10.5 6.98 #&gt; 4 4.5 Fair J I1 65.8 58 18531 10.2 10.2 6.72 #&gt; 5 4.01 Premium I I1 61 61 15223 10.1 10.1 6.17 #&gt; 6 4.01 Premium J I1 62.5 62 15223 10.0 9.94 6.24 diamonds %&gt;% arrange(desc(z)) %&gt;% head() #&gt; # A tibble: 6 x 10 #&gt; carat cut color clarity depth table price x y z #&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.51 Very Good E VS1 61.8 54.7 1970 5.12 5.15 31.8 #&gt; 2 2 Premium H SI2 58.9 57 12210 8.09 58.9 8.06 #&gt; 3 5.01 Fair J I1 65.5 59 18018 10.7 10.5 6.98 #&gt; 4 4.5 Fair J I1 65.8 58 18531 10.2 10.2 6.72 #&gt; 5 4.13 Fair H I1 64.8 61 17329 10 9.85 6.43 #&gt; 6 3.65 Fair H I1 67.1 53 11668 9.53 9.48 6.38 Initially, I only considered univariate outliers. However, to check the plausibility of those outliers I would informally consider how consistent their values are with the values of the other variables. In this case, scatter plots of each combination of x, y, and z shows these outliers much more clearly. ggplot(diamonds, aes(x = x, y = y)) + geom_point() ggplot(diamonds, aes(x = x, y = z)) + geom_point() ggplot(diamonds, aes(x = y, y = z)) + geom_point() Removing the outliers from x, y, and z makes the distribution easier to see. The right skewness of these distributions is unsurprising; there should be more smaller diamonds than larger ones and these values can never be negative. More interestingly, there are spikes in the distribution at certain values. These spikes often, but not exclusively, occur near integer values. Without knowing more about diamond cutting, I can’t say more about what these spikes represent. If you know, add a comment. I would guess that some diamond sizes are used more often than others, and these spikes correspond to those sizes. Also, I would guess that a diamond cut and carat value of a diamond imply values of x, y, and z. Since there are spikes in the distribution of carat sizes, and only a few different cuts, that could result in these spikes. I’ll leave it to readers to figure out if that’s the case. filter(diamonds, x &gt; 0, x &lt; 10) %&gt;% ggplot() + geom_histogram(mapping = aes(x = x), binwidth = 0.01) + scale_x_continuous(breaks = 1:10) filter(diamonds, y &gt; 0, y &lt; 10) %&gt;% ggplot() + geom_histogram(mapping = aes(x = y), binwidth = 0.01) + scale_x_continuous(breaks = 1:10) filter(diamonds, z &gt; 0, z &lt; 10) %&gt;% ggplot() + geom_histogram(mapping = aes(x = z), binwidth = 0.01) + scale_x_continuous(breaks = 1:10) According to the documentation for diamonds, x is length, y is width, and z is depth. If documentation were unavailable, I would compare the values of the variables to match them to the length, width, and depth. I would expect length to always be less than width, otherwise the length would be called the width. I would also search for the definitions of length, width, and depth with respect to diamond cuts. Depth can be expressed as a percentage of the length/width of the diamond, which means it should be less than both the length and the width. summarise(diamonds, mean(x &gt; y), mean(x &gt; z), mean(y &gt; z)) #&gt; # A tibble: 1 x 3 #&gt; `mean(x &gt; y)` `mean(x &gt; z)` `mean(y &gt; z)` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.434 1.00 1.00 It appears that depth (z) is always smaller than length (x) or width (y), perhaps because a shallower depth helps when setting diamonds in jewelry and due to how it affect the reflection of light. Length is more than width in less than half the observations, the opposite of my expectations. Exercise 7.3.2 Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.) The price data has many spikes, but I can’t tell what each spike corresponds to. The following plots don’t show much difference in the distributions in the last one or two digits. There are no diamonds with a price of $1,500 (between $1,455 and $1,545, including). There’s a bulge in the distribution around $750. ggplot(filter(diamonds, price &lt; 2500), aes(x = price)) + geom_histogram(binwidth = 10, center = 0) ggplot(filter(diamonds), aes(x = price)) + geom_histogram(binwidth = 100, center = 0) The last digits of prices are often not uniformly distributed. They are often round, ending in 0 or 5 (for one-half). Another common pattern is ending in 99, as in $1999. If we plot the distribution of the last one and two digits of prices do we observe patterns like that? diamonds %&gt;% mutate(ending = price %% 10) %&gt;% ggplot(aes(x = ending)) + geom_histogram(binwidth = 1, center = 0) diamonds %&gt;% mutate(ending = price %% 100) %&gt;% ggplot(aes(x = ending)) + geom_histogram(binwidth = 1) diamonds %&gt;% mutate(ending = price %% 1000) %&gt;% filter(ending &gt;= 500, ending &lt;= 800) %&gt;% ggplot(aes(x = ending)) + geom_histogram(binwidth = 1) Exercise 7.3.3 How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference? There are more than 70 times as many 1 carat diamonds as 0.99 carat diamond. diamonds %&gt;% filter(carat &gt;= 0.99, carat &lt;= 1) %&gt;% count(carat) #&gt; # A tibble: 2 x 2 #&gt; carat n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 0.99 23 #&gt; 2 1 1558 I don’t know exactly the process behind how carats are measured, but some way or another some diamonds carat values are being “rounded up” Presumably there is a premium for a 1 carat diamond vs. a 0.99 carat diamond beyond the expected increase in price due to a 0.01 carat increase.7 To check this intuition, we would want to look at the number of diamonds in each carat range to see if there is an unusually low number of 0.99 carat diamonds, and an abnormally large number of 1 carat diamonds. diamonds %&gt;% filter(carat &gt;= 0.9, carat &lt;= 1.1) %&gt;% count(carat) %&gt;% print(n = Inf) #&gt; # A tibble: 21 x 2 #&gt; carat n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 0.9 1485 #&gt; 2 0.91 570 #&gt; 3 0.92 226 #&gt; 4 0.93 142 #&gt; 5 0.94 59 #&gt; 6 0.95 65 #&gt; 7 0.96 103 #&gt; 8 0.97 59 #&gt; 9 0.98 31 #&gt; 10 0.99 23 #&gt; 11 1 1558 #&gt; 12 1.01 2242 #&gt; 13 1.02 883 #&gt; 14 1.03 523 #&gt; 15 1.04 475 #&gt; 16 1.05 361 #&gt; 17 1.06 373 #&gt; 18 1.07 342 #&gt; 19 1.08 246 #&gt; 20 1.09 287 #&gt; 21 1.1 278 Exercise 7.3.4 Compare and contrast coord_cartesian() vs xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows? The coord_cartesian() function zooms in on the area specified by the limits, after having calculated and drawn the geoms. Since the histogram bins have already been calculated, it is unaffected. ggplot(diamonds) + geom_histogram(mapping = aes(x = price)) + coord_cartesian(xlim = c(100, 5000), ylim = c(0, 3000)) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. However, the xlim() and ylim() functions influence actions before the calculation of the stats related to the histogram. Thus, any values outside the x- and y-limits are dropped before calculating bin widths and counts. This can influence how the histogram looks. ggplot(diamonds) + geom_histogram(mapping = aes(x = price)) + xlim(100, 5000) + ylim(0, 3000) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 14714 rows containing non-finite values (stat_bin). #&gt; Warning: Removed 6 rows containing missing values (geom_bar). 7.4 Missing values Exercise 7.4.1 What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference? Missing values are removed when the number of observations in each bin are calculated. See the warning message: Removed 9 rows containing non-finite values (stat_bin) diamonds2 &lt;- diamonds %&gt;% mutate(y = ifelse(y &lt; 3 | y &gt; 20, NA, y)) ggplot(diamonds2, aes(x = y)) + geom_histogram() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 9 rows containing non-finite values (stat_bin). In the geom_bar() function, NA is treated as another category. The x aesthetic in geom_bar() requires a discrete (categorical) variable, and missing values act like another category. diamonds %&gt;% mutate(cut = if_else(runif(n()) &lt; 0.1, NA_character_, as.character(cut))) %&gt;% ggplot() + geom_bar(mapping = aes(x = cut)) In a histogram, the x aesthetic variable needs to be numeric, and stat_bin() groups the observations by ranges into bins. Since the numeric value of the NA observations is unknown, they cannot be placed in a particular bin, and are dropped. Exercise 7.4.2 What does na.rm = TRUE do in mean() and sum()? This option removes NA values from the vector prior to calculating the mean and sum. mean(c(0, 1, 2, NA), na.rm = TRUE) #&gt; [1] 1 sum(c(0, 1, 2, NA), na.rm = TRUE) #&gt; [1] 3 7.5 Covariation 7.5.1 A categorical and continuous variable Exercise 7.5.1.1 Use what you’ve learned to improve the visualization of the departure times of cancelled vs. non-cancelled flights. Instead of a freqplot use a box-plot nycflights13::flights %&gt;% mutate( cancelled = is.na(dep_time), sched_hour = sched_dep_time %/% 100, sched_min = sched_dep_time %% 100, sched_dep_time = sched_hour + sched_min / 60 ) %&gt;% ggplot() + geom_boxplot(mapping = aes(y = sched_dep_time, x = cancelled)) Exercise 7.5.1.2 What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive? What are the general relationships of each variable with the price of the diamonds? I will consider the variables: carat, clarity, color, and cut. I ignore the dimensions of the diamond since carat measures size, and thus incorporates most of the information contained in these variables. Since both price and carat are continuous variables, I use a scatter plot to visualize their relationship. ggplot(diamonds, aes(x = carat, y = price)) + geom_point() However, since there is a large number of points in the data, I will use a boxplot by binning carat, as suggested in the chapter: ggplot(data = diamonds, mapping = aes(x = carat, y = price)) + geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)), orientation = &quot;x&quot;) Note that the choice of the binning width is important, as if it were too large it would obscure any relationship, and if it were too small, the values in the bins could be too variable to reveal underlying trends. Version 3.3.0 of ggplot2 introduced changes to boxplots that may affect the orientation. This geom treats each axis differently and, thus, can thus have two orientations. Often the orientation is easy to deduce from a combination of the given mappings and the types of positional scales in use. Thus, ggplot2 will by default try to guess which orientation the layer should have. Under rare circumstances, the orientation is ambiguous and guessing may fail If you are getting something different with your code check the version of ggplot2. Use orientation = &quot;x&quot; (vertical boxplots) or orientation = &quot;y&quot; (horizontal boxplots) to explicitly specify how the geom should treat these axes. The variables color and clarity are ordered categorical variables. The chapter suggests visualizing a categorical and continuous variable using frequency polygons or boxplots. In this case, I will use a box plot since it will better show a relationship between the variables. There is a weak negative relationship between color and price. The scale of diamond color goes from D (best) to J (worst). Currently, the levels of diamonds$color are in the wrong order. Before plotting, I will reverse the order of the color levels so they will be in increasing order of quality on the x-axis. The color column is an example of a factor variable, which is covered in the “Factors” chapter of R4DS. diamonds %&gt;% mutate(color = fct_rev(color)) %&gt;% ggplot(aes(x = color, y = price)) + geom_boxplot() There is also weak negative relationship between clarity and price. The scale of clarity goes from I1 (worst) to IF (best). ggplot(data = diamonds) + geom_boxplot(mapping = aes(x = clarity, y = price)) For both clarity and color, there is a much larger amount of variation within each category than between categories. Carat is clearly the single best predictor of diamond prices. Now that we have established that carat appears to be the best predictor of price, what is the relationship between it and cut? Since this is an example of a continuous (carat) and categorical (cut) variable, it can be visualized with a box plot. ggplot(diamonds, aes(x = cut, y = carat)) + geom_boxplot() There is a lot of variability in the distribution of carat sizes within each cut category. There is a slight negative relationship between carat and cut. Noticeably, the largest carat diamonds have a cut of “Fair” (the lowest). This negative relationship can be due to the way in which diamonds are selected for sale. A larger diamond can be profitably sold with a lower quality cut, while a smaller diamond requires a better cut. Exercise 7.5.1.3 Install the ggstance package, and create a horizontal box plot. How does this compare to using coord_flip()? Earlier, we created this horizontal box plot of the distribution hwy by class, using geom_boxplot() and coord_flip(): ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) + coord_flip() In this case the output looks the same, but x and y aesthetics are flipped. library(&quot;ggstance&quot;) ggplot(data = mpg) + geom_boxploth(mapping = aes(y = reorder(class, hwy, FUN = median), x = hwy)) Current versions of ggplot2 (since version 3.3.0) do not require coord_flip(). All geoms can choose the direction. The direction is be inferred from the aesthetic mapping. In this case, switching x and y produces a horizontal boxplot. ggplot(data = mpg) + geom_boxplot(mapping = aes(y = reorder(class, hwy, FUN = median), x = hwy)) The orientation argument is used to explicitly specify the axis orientation of the plot. ggplot(data = mpg) + geom_boxplot(mapping = aes(y = reorder(class, hwy, FUN = median), x = hwy), orientation = &quot;y&quot;) Exercise 7.5.1.4 One problem with box plots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs cut. What do you learn? How do you interpret the plots? Like box-plots, the boxes of the letter-value plot correspond to quantiles. However, they incorporate far more quantiles than box-plots. They are useful for larger datasets because, larger datasets can give precise estimates of quantiles beyond the quartiles, and in expectation, larger datasets should have more outliers (in absolute numbers). ggplot(diamonds, aes(x = cut, y = price)) + geom_lv() The letter-value plot is described in Hofmann, Wickham, and Kafadar (2017). Exercise 7.5.1.5 Compare and contrast geom_violin() with a faceted geom_histogram(), or a colored geom_freqpoly(). What are the pros and cons of each method? I produce plots for these three methods below. The geom_freqpoly() is better for look-up: meaning that given a price, it is easy to tell which cut has the highest density. However, the overlapping lines makes it difficult to distinguish how the overall distributions relate to each other. The geom_violin() and faceted geom_histogram() have similar strengths and weaknesses. It is easy to visually distinguish differences in the overall shape of the distributions (skewness, central values, variance, etc). However, since we can’t easily compare the vertical values of the distribution, it is difficult to look up which category has the highest density for a given price. All of these methods depend on tuning parameters to determine the level of smoothness of the distribution. ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + geom_freqpoly(mapping = aes(color = cut), binwidth = 500) ggplot(data = diamonds, mapping = aes(x = price)) + geom_histogram() + facet_wrap(~cut, ncol = 1, scales = &quot;free_y&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(data = diamonds, mapping = aes(x = cut, y = price)) + geom_violin() + coord_flip() The violin plot was first described in Hintze and Nelson (1998). Exercise 7.5.1.6 If you have a small dataset, it’s sometimes useful to use geom_jitter() to see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does. There are two methods: geom_quasirandom() produces plots that are a mix of jitter and violin plots. There are several different methods that determine exactly how the random location of the points is generated. geom_beeswarm() produces a plot similar to a violin plot, but by offsetting the points. I’ll use the mpg box plot example since these methods display individual points, they are better suited for smaller datasets. ggplot(data = mpg) + geom_quasirandom(mapping = aes( x = reorder(class, hwy, FUN = median), y = hwy )) ggplot(data = mpg) + geom_quasirandom( mapping = aes( x = reorder(class, hwy, FUN = median), y = hwy ), method = &quot;tukey&quot; ) ggplot(data = mpg) + geom_quasirandom( mapping = aes( x = reorder(class, hwy, FUN = median), y = hwy ), method = &quot;tukeyDense&quot; ) ggplot(data = mpg) + geom_quasirandom( mapping = aes( x = reorder(class, hwy, FUN = median), y = hwy ), method = &quot;frowney&quot; ) ggplot(data = mpg) + geom_quasirandom( mapping = aes( x = reorder(class, hwy, FUN = median), y = hwy ), method = &quot;smiley&quot; ) ggplot(data = mpg) + geom_beeswarm(mapping = aes( x = reorder(class, hwy, FUN = median), y = hwy )) 7.5.2 Two categorical variables Exercise 7.5.2.1 How could you rescale the count dataset above to more clearly show the distribution of cut within color, or color within cut? To clearly show the distribution of cut within color, calculate a new variable prop which is the proportion of each cut within a color. This is done using a grouped mutate. diamonds %&gt;% count(color, cut) %&gt;% group_by(color) %&gt;% mutate(prop = n / sum(n)) %&gt;% ggplot(mapping = aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = prop)) Similarly, to scale by the distribution of color within cut, diamonds %&gt;% count(color, cut) %&gt;% group_by(cut) %&gt;% mutate(prop = n / sum(n)) %&gt;% ggplot(mapping = aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = prop)) I add limit = c(0, 1) to put the color scale between (0, 1). These are the logical boundaries of proportions. This makes it possible to compare each cell to its actual value, and would improve comparisons across multiple plots. However, it ends up limiting the colors and makes it harder to compare within the dataset. However, using the default limits of the minimum and maximum values makes it easier to compare within the dataset the emphasizing relative differences, but harder to compare across datasets. Exercise 7.5.2.2 Use geom_tile() together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it? flights %&gt;% group_by(month, dest) %&gt;% summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = factor(month), y = dest, fill = dep_delay)) + geom_tile() + labs(x = &quot;Month&quot;, y = &quot;Destination&quot;, fill = &quot;Departure Delay&quot;) #&gt; `summarise()` regrouping output by &#39;month&#39; (override with `.groups` argument) There are several things that could be done to improve it, sort destinations by a meaningful quantity (distance, number of flights, average delay) remove missing values How to treat missing values is difficult. In this case, missing values correspond to airports which don’t have regular flights (at least one flight each month) from NYC. These are likely smaller airports (with higher variance in their average due to fewer observations). When we group all pairs of (month, dest) again by dest, we should have a total count of 12 (one for each month) per group (dest). This makes it easy to filter. flights %&gt;% group_by(month, dest) %&gt;% # This gives us (month, dest) pairs summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% group_by(dest) %&gt;% # group all (month, dest) pairs by dest .. filter(n() == 12) %&gt;% # and only select those that have one entry per month ungroup() %&gt;% mutate(dest = reorder(dest, dep_delay)) %&gt;% ggplot(aes(x = factor(month), y = dest, fill = dep_delay)) + geom_tile() + labs(x = &quot;Month&quot;, y = &quot;Destination&quot;, fill = &quot;Departure Delay&quot;) #&gt; `summarise()` regrouping output by &#39;month&#39; (override with `.groups` argument) Exercise 7.5.2.3 Why is it slightly better to use aes(x = color, y = cut) rather than aes(x = cut, y = color) in the example above? It’s usually better to use the categorical variable with a larger number of categories or the longer labels on the y axis. If at all possible, labels should be horizontal because that is easier to read. However, switching the order doesn’t result in overlapping labels. diamonds %&gt;% count(color, cut) %&gt;% ggplot(mapping = aes(y = color, x = cut)) + geom_tile(mapping = aes(fill = n)) Another justification, for switching the order is that the larger numbers are at the top when x = color and y = cut, and that lowers the cognitive burden of interpreting the plot. 7.5.3 Two continuous variables Exercise 7.5.3.1 Instead of summarizing the conditional distribution with a box plot, you could use a frequency polygon. What do you need to consider when using cut_width() vs cut_number()? How does that impact a visualization of the 2d distribution of carat and price? Both cut_width() and cut_number() split a variable into groups. When using cut_width(), we need to choose the width, and the number of bins will be calculated automatically. When using cut_number(), we need to specify the number of bins, and the widths will be calculated automatically. In either case, we want to choose the bin widths and number to be large enough to aggregate observations to remove noise, but not so large as to remove all the signal. If categorical colors are used, no more than eight colors should be used in order to keep them distinct. Using cut_number, I will split carats into quantiles (five groups). ggplot( data = diamonds, mapping = aes(color = cut_number(carat, 5), x = price) ) + geom_freqpoly() + labs(x = &quot;Price&quot;, y = &quot;Count&quot;, color = &quot;Carat&quot;) Alternatively, I could use cut_width to specify widths at which to cut. I will choose 1-carat widths. Since there are very few diamonds larger than 2-carats, this is not as informative. However, using a width of 0.5 carats creates too many groups, and splitting at non-whole numbers is unappealing. ggplot( data = diamonds, mapping = aes(color = cut_width(carat, 1, boundary = 0), x = price) ) + geom_freqpoly() + labs(x = &quot;Price&quot;, y = &quot;Count&quot;, color = &quot;Carat&quot;) Exercise 7.5.3.2 Visualize the distribution of carat, partitioned by price. Plotted with a box plot with 10 bins with an equal number of observations, and the width determined by the number of observations. ggplot(diamonds, aes(x = cut_number(price, 10), y = carat)) + geom_boxplot() + coord_flip() + xlab(&quot;Price&quot;) Plotted with a box plot with 10 equal-width bins of $2,000. The argument boundary = 0 ensures that first bin is $0–$2,000. ggplot(diamonds, aes(x = cut_width(price, 2000, boundary = 0), y = carat)) + geom_boxplot(varwidth = TRUE) + coord_flip() + xlab(&quot;Price&quot;) Exercise 7.5.3.3 How does the price distribution of very large diamonds compare to small diamonds. Is it as you expect, or does it surprise you? The distribution of very large diamonds is more variable. I am not surprised, since I knew little about diamond prices. After the fact, it does not seem surprising (as many thing do). I would guess that this is due to the way in which diamonds are selected for retail sales. Suppose that someone selling a diamond only finds it profitable to sell it if some combination size, cut, clarity, and color are above a certain threshold. The smallest diamonds are only profitable to sell if they are exceptional in all the other factors (cut, clarity, and color), so the small diamonds sold have similar characteristics. However, larger diamonds may be profitable regardless of the values of the other factors. Thus we will observe large diamonds with a wider variety of cut, clarity, and color and thus more variability in prices. Exercise 7.5.3.4 Combine two of the techniques you’ve learned to visualize the combined distribution of cut, carat, and price. There are many options to try, so your solutions may vary from mine. Here are a few options that I tried. ggplot(diamonds, aes(x = carat, y = price)) + geom_hex() + facet_wrap(~cut, ncol = 1) ggplot(diamonds, aes(x = cut_number(carat, 5), y = price, colour = cut)) + geom_boxplot() ggplot(diamonds, aes(colour = cut_number(carat, 5), y = price, x = cut)) + geom_boxplot() Exercise 7.5.3.5 Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately. ggplot(data = diamonds) + geom_point(mapping = aes(x = x, y = y)) + coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) Why is a scatterplot a better display than a binned plot for this case? In this case, there is a strong relationship between \\(x\\) and \\(y\\). The outliers in this case are not extreme in either \\(x\\) or \\(y\\). A binned plot would not reveal these outliers, and may lead us to conclude that the largest value of \\(x\\) was an outlier even though it appears to fit the bivariate pattern well. The later chapter Model Basics discusses fitting models to bivariate data and plotting residuals, which would reveal this outliers. 7.6 Patterns and models No exercises 7.7 ggplot2 calls No exercises 7.8 Learning more No exercises References "],
["workflow-projects.html", "8 Workflow: projects", " 8 Workflow: projects No exercises "],
["wrangle-intro.html", "9 Introduction", " 9 Introduction No exercises "],
["tibbles.html", "10 Tibbles Exercise 10.1 Exercise 10.2 Exercise 10.3 Exercise 10.4 Exercise 10.5 Exercise 10.6", " 10 Tibbles library(&quot;tidyverse&quot;) Exercise 10.1 How can you tell if an object is a tibble? (Hint: try printing mtcars, which is a regular data frame). When we print mtcars, it prints all the columns. mtcars #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160.0 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.88 17.0 0 1 4 4 #&gt; Datsun 710 22.8 4 108.0 93 3.85 2.32 18.6 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258.0 110 3.08 3.21 19.4 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360.0 175 3.15 3.44 17.0 0 0 3 2 #&gt; Valiant 18.1 6 225.0 105 2.76 3.46 20.2 1 0 3 1 #&gt; Duster 360 14.3 8 360.0 245 3.21 3.57 15.8 0 0 3 4 #&gt; Merc 240D 24.4 4 146.7 62 3.69 3.19 20.0 1 0 4 2 #&gt; Merc 230 22.8 4 140.8 95 3.92 3.15 22.9 1 0 4 2 #&gt; Merc 280 19.2 6 167.6 123 3.92 3.44 18.3 1 0 4 4 #&gt; Merc 280C 17.8 6 167.6 123 3.92 3.44 18.9 1 0 4 4 #&gt; Merc 450SE 16.4 8 275.8 180 3.07 4.07 17.4 0 0 3 3 #&gt; Merc 450SL 17.3 8 275.8 180 3.07 3.73 17.6 0 0 3 3 #&gt; Merc 450SLC 15.2 8 275.8 180 3.07 3.78 18.0 0 0 3 3 #&gt; Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.25 18.0 0 0 3 4 #&gt; Lincoln Continental 10.4 8 460.0 215 3.00 5.42 17.8 0 0 3 4 #&gt; Chrysler Imperial 14.7 8 440.0 230 3.23 5.34 17.4 0 0 3 4 #&gt; Fiat 128 32.4 4 78.7 66 4.08 2.20 19.5 1 1 4 1 #&gt; Honda Civic 30.4 4 75.7 52 4.93 1.61 18.5 1 1 4 2 #&gt; Toyota Corolla 33.9 4 71.1 65 4.22 1.83 19.9 1 1 4 1 #&gt; Toyota Corona 21.5 4 120.1 97 3.70 2.46 20.0 1 0 3 1 #&gt; Dodge Challenger 15.5 8 318.0 150 2.76 3.52 16.9 0 0 3 2 #&gt; AMC Javelin 15.2 8 304.0 150 3.15 3.44 17.3 0 0 3 2 #&gt; Camaro Z28 13.3 8 350.0 245 3.73 3.84 15.4 0 0 3 4 #&gt; Pontiac Firebird 19.2 8 400.0 175 3.08 3.85 17.1 0 0 3 2 #&gt; Fiat X1-9 27.3 4 79.0 66 4.08 1.94 18.9 1 1 4 1 #&gt; Porsche 914-2 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; Lotus Europa 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 #&gt; Ford Pantera L 15.8 8 351.0 264 4.22 3.17 14.5 0 1 5 4 #&gt; Ferrari Dino 19.7 6 145.0 175 3.62 2.77 15.5 0 1 5 6 #&gt; Maserati Bora 15.0 8 301.0 335 3.54 3.57 14.6 0 1 5 8 #&gt; Volvo 142E 21.4 4 121.0 109 4.11 2.78 18.6 1 1 4 2 But when we first convert mtcars to a tibble using as_tibble(), it prints only the first ten observations. There are also some other differences in formatting of the printed data frame. It prints the number of rows and columns and the date type of each column. as_tibble(mtcars) #&gt; # A tibble: 32 x 11 #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 #&gt; 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 #&gt; 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 #&gt; 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 #&gt; 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 #&gt; 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 #&gt; # … with 26 more rows You can use the function is_tibble() to check whether a data frame is a tibble or not. The mtcars data frame is not a tibble. is_tibble(mtcars) #&gt; [1] FALSE But the diamonds and flights data are tibbles. is_tibble(ggplot2::diamonds) #&gt; [1] TRUE is_tibble(nycflights13::flights) #&gt; [1] TRUE is_tibble(as_tibble(mtcars)) #&gt; [1] TRUE More generally, you can use the class() function to find out the class of an object. Tibbles has the classes c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;), while old data frames will only have the class &quot;data.frame&quot;. class(mtcars) #&gt; [1] &quot;data.frame&quot; class(ggplot2::diamonds) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; class(nycflights13::flights) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; If you are interested in reading more on R’s classes, read the chapters on object oriented programming in Advanced R. Exercise 10.2 Compare and contrast the following operations on a data.frame and equivalent tibble. What is different? Why might the default data frame behaviors cause you frustration? df &lt;- data.frame(abc = 1, xyz = &quot;a&quot;) df$x #&gt; [1] &quot;a&quot; df[, &quot;xyz&quot;] #&gt; [1] &quot;a&quot; df[, c(&quot;abc&quot;, &quot;xyz&quot;)] #&gt; abc xyz #&gt; 1 1 a tbl &lt;- as_tibble(df) tbl$x #&gt; Warning: Unknown or uninitialised column: `x`. #&gt; NULL tbl[, &quot;xyz&quot;] #&gt; # A tibble: 1 x 1 #&gt; xyz #&gt; &lt;chr&gt; #&gt; 1 a tbl[, c(&quot;abc&quot;, &quot;xyz&quot;)] #&gt; # A tibble: 1 x 2 #&gt; abc xyz #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 a The $ operator will match any column name that starts with the name following it. Since there is a column named xyz, the expression df$x will be expanded to df$xyz. This behavior of the $ operator saves a few keystrokes, but it can result in accidentally using a different column than you thought you were using. With data.frames, with [ the type of object that is returned differs on the number of columns. If it is one column, it won’t return a data.frame, but instead will return a vector. With more than one column, then it will return a data.frame. This is fine if you know what you are passing in, but suppose you did df[ , vars] where vars was a variable. Then what that code does depends on length(vars) and you’d have to write code to account for those situations or risk bugs. Exercise 10.3 If you have the name of a variable stored in an object, e.g. var &lt;- &quot;mpg&quot;, how can you extract the reference variable from a tibble? You can use the double bracket, like df[[var]]. You cannot use the dollar sign, because df$var would look for a column named var. Exercise 10.4 Practice referring to non-syntactic names in the following data frame by: Extracting the variable called 1. Plotting a scatterplot of 1 vs 2. Creating a new column called 3 which is 2 divided by 1. Renaming the columns to one, two and three. For this example, I’ll create a dataset called annoying with columns named 1 and 2. annoying &lt;- tibble( `1` = 1:10, `2` = `1` * 2 + rnorm(length(`1`)) ) To extract the variable named 1: annoying[[&quot;1&quot;]] #&gt; [1] 1 2 3 4 5 6 7 8 9 10 or annoying$`1` #&gt; [1] 1 2 3 4 5 6 7 8 9 10 To create a scatter plot of 1 vs. 2: ggplot(annoying, aes(x = `1`, y = `2`)) + geom_point() To add a new column 3 which is 2 divided by 1: mutate(annoying, `3` = `2` / `1`) #&gt; # A tibble: 10 x 3 #&gt; `1` `2` `3` #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 0.600 0.600 #&gt; 2 2 4.26 2.13 #&gt; 3 3 3.56 1.19 #&gt; 4 4 7.99 2.00 #&gt; 5 5 10.6 2.12 #&gt; 6 6 13.1 2.19 #&gt; # … with 4 more rows or annoying[[&quot;3&quot;]] &lt;- annoying$`2` / annoying$`1` or annoying[[&quot;3&quot;]] &lt;- annoying[[&quot;2&quot;]] / annoying[[&quot;1&quot;]] To rename the columns to one, two, and three, run: annoying &lt;- rename(annoying, one = `1`, two = `2`, three = `3`) glimpse(annoying) #&gt; Rows: 10 #&gt; Columns: 3 #&gt; $ one &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 #&gt; $ two &lt;dbl&gt; 0.60, 4.26, 3.56, 7.99, 10.62, 13.15, 12.18, 15.75, 17.76, 19.72 #&gt; $ three &lt;dbl&gt; 0.60, 2.13, 1.19, 2.00, 2.12, 2.19, 1.74, 1.97, 1.97, 1.97 Exercise 10.5 What does tibble::enframe() do? When might you use it? The function tibble::enframe() converts named vectors to a data frame with names and values enframe(c(a = 1, b = 2, c = 3)) #&gt; # A tibble: 3 x 2 #&gt; name value #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a 1 #&gt; 2 b 2 #&gt; 3 c 3 Exercise 10.6 What option controls how many additional column names are printed at the footer of a tibble? The help page for the print() method of tibble objects is discussed in ?print.tbl. The n_extra argument determines the number of extra columns to print information for. "],
["data-import.html", "11 Data import 11.1 Introduction 11.2 Getting started 11.3 Parsing a vector 11.4 Parsing a file 11.5 Writing to a file 11.6 Other types of data", " 11 Data import 11.1 Introduction library(&quot;tidyverse&quot;) 11.2 Getting started Exercise 11.2.1 What function would you use to read a file where fields were separated with “|”? Use the read_delim() function with the argument delim=&quot;|&quot;. read_delim(file, delim = &quot;|&quot;) Exercise 11.2.2 Apart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common? They have the following arguments in common: intersect(names(formals(read_csv)), names(formals(read_tsv))) #&gt; [1] &quot;file&quot; &quot;col_names&quot; &quot;col_types&quot; &quot;locale&quot; #&gt; [5] &quot;na&quot; &quot;quoted_na&quot; &quot;quote&quot; &quot;comment&quot; #&gt; [9] &quot;trim_ws&quot; &quot;skip&quot; &quot;n_max&quot; &quot;guess_max&quot; #&gt; [13] &quot;progress&quot; &quot;skip_empty_rows&quot; col_names and col_types are used to specify the column names and how to parse the columns locale is important for determining things like the encoding and whether “.” or “,” is used as a decimal mark. na and quoted_na control which strings are treated as missing values when parsing vectors trim_ws trims whitespace before and after cells before parsing n_max sets how many rows to read guess_max sets how many rows to use when guessing the column type progress determines whether a progress bar is shown. In fact, the two functions have the exact same arguments: identical(names(formals(read_csv)), names(formals(read_tsv))) #&gt; [1] TRUE Exercise 11.2.3 What are the most important arguments to read_fwf()? The most important argument to read_fwf() which reads “fixed-width formats”, is col_positions which tells the function where data columns begin and end. Exercise 11.2.4 Sometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character, like &quot; or '. By convention, read_csv() assumes that the quoting character will be &quot;, and if you want to change it you’ll need to use read_delim() instead. What arguments do you need to specify to read the following text into a data frame? &quot;x,y\\n1,&#39;a,b&#39;&quot; For read_delim(), we will will need to specify a delimiter, in this case &quot;,&quot;, and a quote argument. x &lt;- &quot;x,y\\n1,&#39;a,b&#39;&quot; read_delim(x, &quot;,&quot;, quote = &quot;&#39;&quot;) #&gt; # A tibble: 1 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 a,b However, this question is out of date. read_csv() now supports a quote argument, so the following code works. read_csv(x, quote = &quot;&#39;&quot;) #&gt; # A tibble: 1 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 a,b Exercise 11.2.5 Identify what is wrong with each of the following inline CSV files. What happens when you run the code? read_csv(&quot;a,b\\n1,2,3\\n4,5,6&quot;) #&gt; Warning: 2 parsing failures. #&gt; row col expected actual file #&gt; 1 -- 2 columns 3 columns literal data #&gt; 2 -- 2 columns 3 columns literal data #&gt; # A tibble: 2 x 2 #&gt; a b #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 #&gt; 2 4 5 Only two columns are specified in the header “a” and “b”, but the rows have three columns, so the last column is dropped. read_csv(&quot;a,b,c\\n1,2\\n1,2,3,4&quot;) #&gt; Warning: 2 parsing failures. #&gt; row col expected actual file #&gt; 1 -- 3 columns 2 columns literal data #&gt; 2 -- 3 columns 4 columns literal data #&gt; # A tibble: 2 x 3 #&gt; a b c #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 NA #&gt; 2 1 2 3 The numbers of columns in the data do not match the number of columns in the header (three). In row one, there are only two values, so column c is set to missing. In row two, there is an extra value, and that value is dropped. read_csv(&quot;a,b\\n\\&quot;1&quot;) #&gt; Warning: 2 parsing failures. #&gt; row col expected actual file #&gt; 1 a closing quote at end of file literal data #&gt; 1 -- 2 columns 1 columns literal data #&gt; # A tibble: 1 x 2 #&gt; a b #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 &lt;NA&gt; It’s not clear what the intent was here. The opening quote &quot;1 is dropped because it is not closed, and a is treated as an integer. read_csv(&quot;a,b\\n1,2\\na,b&quot;) #&gt; # A tibble: 2 x 2 #&gt; a b #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 2 #&gt; 2 a b Both “a” and “b” are treated as character vectors since they contain non-numeric strings. This may have been intentional, or the author may have intended the values of the columns to be “1,2” and “a,b”. read_csv(&quot;a;b\\n1;3&quot;) #&gt; # A tibble: 1 x 1 #&gt; `a;b` #&gt; &lt;chr&gt; #&gt; 1 1;3 The values are separated by “;” rather than “,”. Use read_csv2() instead: read_csv2(&quot;a;b\\n1;3&quot;) #&gt; Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. #&gt; # A tibble: 1 x 2 #&gt; a b #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 3 11.3 Parsing a vector Exercise 11.3.1 What are the most important arguments to locale()? The locale object has arguments to set the following: date and time formats: date_names, date_format, and time_format time zone: tz numbers: decimal_mark, grouping_mark encoding: encoding Exercise 11.3.2 What happens if you try and set decimal_mark and grouping_mark to the same character? What happens to the default value of grouping_mark when you set decimal_mark to &quot;,&quot;? What happens to the default value of decimal_mark when you set the grouping_mark to &quot;.&quot;? If the decimal and grouping marks are set to the same character, locale throws an error: locale(decimal_mark = &quot;.&quot;, grouping_mark = &quot;.&quot;) #&gt; Error: `decimal_mark` and `grouping_mark` must be different If the decimal_mark is set to the comma &quot;,&quot;, then the grouping mark is set to the period &quot;.&quot;: locale(decimal_mark = &quot;,&quot;) #&gt; &lt;locale&gt; #&gt; Numbers: 123.456,78 #&gt; Formats: %AD / %AT #&gt; Timezone: UTC #&gt; Encoding: UTF-8 #&gt; &lt;date_names&gt; #&gt; Days: Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday #&gt; (Thu), Friday (Fri), Saturday (Sat) #&gt; Months: January (Jan), February (Feb), March (Mar), April (Apr), May (May), #&gt; June (Jun), July (Jul), August (Aug), September (Sep), October #&gt; (Oct), November (Nov), December (Dec) #&gt; AM/PM: AM/PM If the grouping mark is set to a period, then the decimal mark is set to a comma locale(grouping_mark = &quot;.&quot;) #&gt; &lt;locale&gt; #&gt; Numbers: 123.456,78 #&gt; Formats: %AD / %AT #&gt; Timezone: UTC #&gt; Encoding: UTF-8 #&gt; &lt;date_names&gt; #&gt; Days: Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday #&gt; (Thu), Friday (Fri), Saturday (Sat) #&gt; Months: January (Jan), February (Feb), March (Mar), April (Apr), May (May), #&gt; June (Jun), July (Jul), August (Aug), September (Sep), October #&gt; (Oct), November (Nov), December (Dec) #&gt; AM/PM: AM/PM Exercise 11.3.3 I didn’t discuss the date_format and time_format options to locale(). What do they do? Construct an example that shows when they might be useful. They provide default date and time formats. The readr vignette discusses using these to parse dates: since dates can include languages specific weekday and month names, and different conventions for specifying AM/PM locale() #&gt; &lt;locale&gt; #&gt; Numbers: 123,456.78 #&gt; Formats: %AD / %AT #&gt; Timezone: UTC #&gt; Encoding: UTF-8 #&gt; &lt;date_names&gt; #&gt; Days: Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday #&gt; (Thu), Friday (Fri), Saturday (Sat) #&gt; Months: January (Jan), February (Feb), March (Mar), April (Apr), May (May), #&gt; June (Jun), July (Jul), August (Aug), September (Sep), October #&gt; (Oct), November (Nov), December (Dec) #&gt; AM/PM: AM/PM Examples from the readr vignette of parsing French dates parse_date(&quot;1 janvier 2015&quot;, &quot;%d %B %Y&quot;, locale = locale(&quot;fr&quot;)) #&gt; [1] &quot;2015-01-01&quot; parse_date(&quot;14 oct. 1979&quot;, &quot;%d %b %Y&quot;, locale = locale(&quot;fr&quot;)) #&gt; [1] &quot;1979-10-14&quot; Both the date format and time format are used for guessing column types. Thus if you were often parsing data that had non-standard formats for the date and time, you could specify custom values for date_format and time_format. locale_custom &lt;- locale(date_format = &quot;Day %d Mon %M Year %y&quot;, time_format = &quot;Sec %S Min %M Hour %H&quot;) date_custom &lt;- c(&quot;Day 01 Mon 02 Year 03&quot;, &quot;Day 03 Mon 01 Year 01&quot;) parse_date(date_custom) #&gt; Warning: 2 parsing failures. #&gt; row col expected actual #&gt; 1 -- date like Day 01 Mon 02 Year 03 #&gt; 2 -- date like Day 03 Mon 01 Year 01 #&gt; [1] NA NA parse_date(date_custom, locale = locale_custom) #&gt; [1] &quot;2003-01-01&quot; &quot;2001-01-03&quot; time_custom &lt;- c(&quot;Sec 01 Min 02 Hour 03&quot;, &quot;Sec 03 Min 02 Hour 01&quot;) parse_time(time_custom) #&gt; Warning: 2 parsing failures. #&gt; row col expected actual #&gt; 1 -- time like Sec 01 Min 02 Hour 03 #&gt; 2 -- time like Sec 03 Min 02 Hour 01 #&gt; NA #&gt; NA parse_time(time_custom, locale = locale_custom) #&gt; 03:02:01 #&gt; 01:02:03 Exercise 11.3.4 If you live outside the US, create a new locale object that encapsulates the settings for the types of file you read most commonly. Read the help page for locale() using ?locale to learn about the different variables that can be set. As an example, consider Australia. Most of the defaults values are valid, except that the date format is “(d)d/mm/yyyy”, meaning that January 2, 2006 is written as 02/01/2006. However, default locale will parse that date as February 1, 2006. parse_date(&quot;02/01/2006&quot;) #&gt; Warning: 1 parsing failure. #&gt; row col expected actual #&gt; 1 -- date like 02/01/2006 #&gt; [1] NA To correctly parse Australian dates, define a new locale object. au_locale &lt;- locale(date_format = &quot;%d/%m/%Y&quot;) Using parse_date() with the au_locale as its locale will correctly parse our example date. parse_date(&quot;02/01/2006&quot;, locale = au_locale) #&gt; [1] &quot;2006-01-02&quot; Exercise 11.3.5 What’s the difference between read_csv() and read_csv2()? The delimiter. The function read_csv() uses a comma, while read_csv2() uses a semi-colon (;). Using a semi-colon is useful when commas are used as the decimal point (as in Europe). Exercise 11.3.6 What are the most common encodings used in Europe? What are the most common encodings used in Asia? Do some googling to find out. UTF-8 is standard now, and ASCII has been around forever. For the European languages, there are separate encodings for Romance languages and Eastern European languages using Latin script, Cyrillic, Greek, Hebrew, Turkish: usually with separate ISO and Windows encoding standards. There is also Mac OS Roman. For Asian languages Arabic and Vietnamese have ISO and Windows standards. The other major Asian scripts have their own: Japanese: JIS X 0208, Shift JIS, ISO-2022-JP Chinese: GB 2312, GBK, GB 18030 Korean: KS X 1001, EUC-KR, ISO-2022-KR The list in the documentation for stringi::stri_enc_detect() is a good list of encodings since it supports the most common encodings. Western European Latin script languages: ISO-8859-1, Windows-1250 (also CP-1250 for code-point) Eastern European Latin script languages: ISO-8859-2, Windows-1252 Greek: ISO-8859-7 Turkish: ISO-8859-9, Windows-1254 Hebrew: ISO-8859-8, IBM424, Windows 1255 Russian: Windows 1251 Japanese: Shift JIS, ISO-2022-JP, EUC-JP Korean: ISO-2022-KR, EUC-KR Chinese: GB18030, ISO-2022-CN (Simplified), Big5 (Traditional) Arabic: ISO-8859-6, IBM420, Windows 1256 For more information on character encodings see the following sources. The Wikipedia page Character encoding, has a good list of encodings. Unicode CLDR project What is the most common encoding of each language (Stack Overflow) “What Every Programmer Absolutely, Positively Needs To Know About Encodings And Character Sets To Work With Text”, http://kunststube.net/encoding/. Programs that identify the encoding of text include: readr::guess_encoding() stringi::str_enc_detect() iconv chardet (Python) Exercise 11.3.7 Generate the correct format string to parse each of the following dates and times: d1 &lt;- &quot;January 1, 2010&quot; d2 &lt;- &quot;2015-Mar-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; d4 &lt;- c(&quot;August 19 (2015)&quot;, &quot;July 1 (2015)&quot;) d5 &lt;- &quot;12/30/14&quot; # Dec 30, 2014 t1 &lt;- &quot;1705&quot; t2 &lt;- &quot;11:15:10.12 PM&quot; The correct formats are: parse_date(d1, &quot;%B %d, %Y&quot;) #&gt; [1] &quot;2010-01-01&quot; parse_date(d2, &quot;%Y-%b-%d&quot;) #&gt; [1] &quot;2015-03-07&quot; parse_date(d3, &quot;%d-%b-%Y&quot;) #&gt; [1] &quot;2017-06-06&quot; parse_date(d4, &quot;%B %d (%Y)&quot;) #&gt; [1] &quot;2015-08-19&quot; &quot;2015-07-01&quot; parse_date(d5, &quot;%m/%d/%y&quot;) #&gt; [1] &quot;2014-12-30&quot; parse_time(t1, &quot;%H%M&quot;) #&gt; 17:05:00 The time t2 uses real seconds, parse_time(t2, &quot;%H:%M:%OS %p&quot;) #&gt; 23:15:10.12 11.4 Parsing a file No exercises 11.5 Writing to a file No exercises 11.6 Other types of data No exercises "],
["tidy-data.html", "12 Tidy data 12.1 Introduction 12.2 Tidy data 12.3 Pivoting 12.4 Separating and uniting 12.5 Missing values 12.6 Case Study 12.7 Non-tidy data", " 12 Tidy data 12.1 Introduction library(&quot;tidyverse&quot;) 12.2 Tidy data Exercise 12.2.1 Using prose, describe how the variables and observations are organized in each of the sample tables. In table table1, each row represents a (country, year) combination. The columns cases and population contain the values for those variables. table1 #&gt; # A tibble: 6 x 4 #&gt; country year cases population #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 745 19987071 #&gt; 2 Afghanistan 2000 2666 20595360 #&gt; 3 Brazil 1999 37737 172006362 #&gt; 4 Brazil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 In table2, each row represents a (country, year, variable) combination. The column count contains the values of variables cases and population in separate rows. table2 #&gt; # A tibble: 12 x 4 #&gt; country year type count #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 cases 745 #&gt; 2 Afghanistan 1999 population 19987071 #&gt; 3 Afghanistan 2000 cases 2666 #&gt; 4 Afghanistan 2000 population 20595360 #&gt; 5 Brazil 1999 cases 37737 #&gt; 6 Brazil 1999 population 172006362 #&gt; # … with 6 more rows In table3, each row represents a (country, year) combination. The column rate provides the values of both cases and population in a string formatted like cases / population. table3 #&gt; # A tibble: 6 x 3 #&gt; country year rate #&gt; * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Afghanistan 1999 745/19987071 #&gt; 2 Afghanistan 2000 2666/20595360 #&gt; 3 Brazil 1999 37737/172006362 #&gt; 4 Brazil 2000 80488/174504898 #&gt; 5 China 1999 212258/1272915272 #&gt; 6 China 2000 213766/1280428583 Table 4 is split into two tables, one table for each variable. The table table4a contains the values of cases and table4b contains the values of population. Within each table, each row represents a country, each column represents a year, and the cells are the value of the table’s variable for that country and year. table4a #&gt; # A tibble: 3 x 3 #&gt; country `1999` `2000` #&gt; * &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghanistan 745 2666 #&gt; 2 Brazil 37737 80488 #&gt; 3 China 212258 213766 table4b #&gt; # A tibble: 3 x 3 #&gt; country `1999` `2000` #&gt; * &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghanistan 19987071 20595360 #&gt; 2 Brazil 172006362 174504898 #&gt; 3 China 1272915272 1280428583 Exercise 12.2.2 Compute the rate for table2, and table4a + table4b. You will need to perform four operations: Extract the number of TB cases per country per year. Extract the matching population per country per year. Divide cases by population, and multiply by 10000. Store back in the appropriate place. Which representation is easiest to work with? Which is hardest? Why? To calculate cases per person, we need to divide cases by population for each country and year. This is easiest if the cases and population variables are two columns in a data frame in which rows represent (country, year) combinations. Table 2: First, create separate tables for cases and population and ensure that they are sorted in the same order. t2_cases &lt;- filter(table2, type == &quot;cases&quot;) %&gt;% rename(cases = count) %&gt;% arrange(country, year) t2_population &lt;- filter(table2, type == &quot;population&quot;) %&gt;% rename(population = count) %&gt;% arrange(country, year) Then create a new data frame with the population and cases columns, and calculate the cases per capita in a new column. t2_cases_per_cap &lt;- tibble( year = t2_cases$year, country = t2_cases$country, cases = t2_cases$cases, population = t2_population$population ) %&gt;% mutate(cases_per_cap = (cases / population) * 10000) %&gt;% select(country, year, cases_per_cap) To store this new variable in the appropriate location, we will add new rows to table2. t2_cases_per_cap &lt;- t2_cases_per_cap %&gt;% mutate(type = &quot;cases_per_cap&quot;) %&gt;% rename(count = cases_per_cap) bind_rows(table2, t2_cases_per_cap) %&gt;% arrange(country, year, type, count) #&gt; # A tibble: 18 x 4 #&gt; country year type count #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Afghanistan 1999 cases 745 #&gt; 2 Afghanistan 1999 cases_per_cap 0.373 #&gt; 3 Afghanistan 1999 population 19987071 #&gt; 4 Afghanistan 2000 cases 2666 #&gt; 5 Afghanistan 2000 cases_per_cap 1.29 #&gt; 6 Afghanistan 2000 population 20595360 #&gt; # … with 12 more rows Note that after adding the cases_per_cap rows, the type of count is coerced to numeric (double) because cases_per_cap is not an integer. For table4a and table4b, create a new table for cases per capita, which we’ll name table4c, with country rows and year columns. table4c &lt;- tibble( country = table4a$country, `1999` = table4a[[&quot;1999&quot;]] / table4b[[&quot;1999&quot;]] * 10000, `2000` = table4a[[&quot;2000&quot;]] / table4b[[&quot;2000&quot;]] * 10000 ) table4c #&gt; # A tibble: 3 x 3 #&gt; country `1999` `2000` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan 0.373 1.29 #&gt; 2 Brazil 2.19 4.61 #&gt; 3 China 1.67 1.67 Neither table is particularly easy to work with. Since table2 has separate rows for cases and population we needed to generate a table with columns for cases and population where we could calculate cases per capita. table4a and table4b split the cases and population variables into different tables which made it easy to divide cases by population. However, we had to repeat this calculation for each row. The ideal format of a data frame to answer this question is one with columns country, year, cases, and population. Then problem could be answered with a single mutate() call. Exercise 12.2.3 Recreate the plot showing change in cases over time using table2 instead of table1. What do you need to do first? Before creating the plot with change in cases over time, we need to filter table to only include rows representing cases of TB. table2 %&gt;% filter(type == &quot;cases&quot;) %&gt;% ggplot(aes(year, count)) + geom_line(aes(group = country), colour = &quot;grey50&quot;) + geom_point(aes(colour = country)) + scale_x_continuous(breaks = unique(table2$year)) + ylab(&quot;cases&quot;) 12.3 Pivoting This code is reproduced from the chapter because it is needed by the exercises. tidy4a &lt;- table4a %&gt;% pivot_longer(c(`1999`, `2000`), names_to = &quot;year&quot;, values_to = &quot;cases&quot;) tidy4b &lt;- table4b %&gt;% pivot_longer(c(`1999`, `2000`), names_to = &quot;year&quot;, values_to = &quot;population&quot;) Exercise 12.3.1 Why are pivot_longer() and pivot_wider() not perfectly symmetrical? Carefully consider the following example: Carefully consider the following example: stocks &lt;- tibble( year = c(2015, 2015, 2016, 2016), half = c( 1, 2, 1, 2), return = c(1.88, 0.59, 0.92, 0.17) ) stocks %&gt;% pivot_wider(names_from = year, values_from = return) %&gt;% pivot_longer(`2015`:`2016`, names_to = &quot;year&quot;, values_to = &quot;return&quot;) #&gt; # A tibble: 4 x 3 #&gt; half year return #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 2015 1.88 #&gt; 2 1 2016 0.92 #&gt; 3 2 2015 0.59 #&gt; 4 2 2016 0.17 (Hint: look at the variable types and think about column names.) pivot_longer() has a names_ptype argument, e.g. names_ptype = list(year = double()). What does it do? The functions pivot_longer() and pivot_wider() are not perfectly symmetrical because column type information is lost when a data frame is converted from wide to long. The function pivot_longer() stacks multiple columns which may have had multiple data types into a single column with a single data type. This transformation throws away the individual data types of the original columns. The function pivot_wider() creates column names from values in column. These column names will always be treated as character values by pivot_longer() so if the original variable used to create the column names did not have a character data type, then the round-trip will not reproduce the same dataset. In the provided example, columns have the following data types: glimpse(stocks) #&gt; Rows: 4 #&gt; Columns: 3 #&gt; $ year &lt;dbl&gt; 2015, 2015, 2016, 2016 #&gt; $ half &lt;dbl&gt; 1, 2, 1, 2 #&gt; $ return &lt;dbl&gt; 1.88, 0.59, 0.92, 0.17 The pivot_wider() expression pivots the table to create a data frame with years as column names, and the values in return as the column values. stocks %&gt;% pivot_wider(names_from = year, values_from = return) #&gt; # A tibble: 2 x 3 #&gt; half `2015` `2016` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1.88 0.92 #&gt; 2 2 0.59 0.17 The pivot_longer() expression unpivots the table, returning it to a tidy data frame with columns for half, year, and return. stocks %&gt;% pivot_wider(names_from = year, values_from = return)%&gt;% pivot_longer(`2015`:`2016`, names_to = &quot;year&quot;, values_to = &quot;return&quot;) #&gt; # A tibble: 4 x 3 #&gt; half year return #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 2015 1.88 #&gt; 2 1 2016 0.92 #&gt; 3 2 2015 0.59 #&gt; 4 2 2016 0.17 There is one difference, in the new data frame, year has a data type of character rather than numeric. The names_to column created from column names by pivot_longer() will be character by default, which is usually a safe assumption, since syntactically valid-column names can only be character values. The original data types of column which pivot_wider() used to create the column names was not stored, so pivot_longer() has no idea that the column names in this case should be numeric values. In the current version of tidyr, the names_ptype argument does not convert the year column to a numeric vector, and it will raise an error. stocks %&gt;% pivot_wider(names_from = year, values_from = return)%&gt;% pivot_longer(`2015`:`2016`, names_to = &quot;year&quot;, values_to = &quot;return&quot;, names_ptype = list(year = double())) #&gt; Error: Can&#39;t convert &lt;character&gt; to &lt;double&gt;. Instead, use the names_transform argument to pivot_longer(), which provides a function to coerce the column to a different data type. stocks %&gt;% pivot_wider(names_from = year, values_from = return)%&gt;% pivot_longer(`2015`:`2016`, names_to = &quot;year&quot;, values_to = &quot;return&quot;, names_transform = list(year = as.numeric)) #&gt; # A tibble: 4 x 3 #&gt; half year return #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2015 1.88 #&gt; 2 1 2016 0.92 #&gt; 3 2 2015 0.59 #&gt; 4 2 2016 0.17 Exercise 12.3.2 Why does this code fail? table4a %&gt;% pivot_longer(c(1999, 2000), names_to = &quot;year&quot;, values_to = &quot;cases&quot;) #&gt; Error: Can&#39;t subset columns that don&#39;t exist. #&gt; ✖ Locations 1999 and 2000 don&#39;t exist. #&gt; ℹ There are only 3 columns. The code fails because the column names 1999 and 2000 are not non-syntactic variable names.[^non-syntactic] When selecting variables from a data frame, tidyverse functions will interpret numbers, like 1999 and 2000, as column numbers. In this case, pivot_longer() tries to select the 1999th and 2000th column of the data frame. To select the columns 1999 and 2000, the names must be surrounded in backticks (```) or provided as strings. table4a %&gt;% pivot_longer(c(`1999`, `2000`), names_to = &quot;year&quot;, values_to = &quot;cases&quot;) #&gt; # A tibble: 6 x 3 #&gt; country year cases #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 745 #&gt; 2 Afghanistan 2000 2666 #&gt; 3 Brazil 1999 37737 #&gt; 4 Brazil 2000 80488 #&gt; 5 China 1999 212258 #&gt; 6 China 2000 213766 table4a %&gt;% pivot_longer(c(&quot;1999&quot;, &quot;2000&quot;), names_to = &quot;year&quot;, values_to = &quot;cases&quot;) #&gt; # A tibble: 6 x 3 #&gt; country year cases #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 745 #&gt; 2 Afghanistan 2000 2666 #&gt; 3 Brazil 1999 37737 #&gt; 4 Brazil 2000 80488 #&gt; 5 China 1999 212258 #&gt; 6 China 2000 213766 Exercise 12.3.3 What would happen if you widen this table? Why? How could you add a new column to uniquely identify each value? people &lt;- tribble( ~name, ~key, ~value, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Phillip Woods&quot;, &quot;age&quot;, 50, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) glimpse(people) #&gt; Rows: 5 #&gt; Columns: 3 #&gt; $ name &lt;chr&gt; &quot;Phillip Woods&quot;, &quot;Phillip Woods&quot;, &quot;Phillip Woods&quot;, &quot;Jessica Cor… #&gt; $ key &lt;chr&gt; &quot;age&quot;, &quot;height&quot;, &quot;age&quot;, &quot;age&quot;, &quot;height&quot; #&gt; $ value &lt;dbl&gt; 45, 186, 50, 37, 156 Widening this data frame using pivot_wider() produces columns that are lists of numeric vectors because the name and key columns do not uniquely identify rows. In particular, there are two rows with values for the age of “Phillip Woods”. pivot_wider(people, names_from=&quot;name&quot;, values_from = &quot;value&quot;) #&gt; Warning: Values are not uniquely identified; output will contain list-cols. #&gt; * Use `values_fn = list` to suppress this warning. #&gt; * Use `values_fn = length` to identify where the duplicates arise #&gt; * Use `values_fn = {summary_fun}` to summarise duplicates #&gt; # A tibble: 2 x 3 #&gt; key `Phillip Woods` `Jessica Cordero` #&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 age &lt;dbl [2]&gt; &lt;dbl [1]&gt; #&gt; 2 height &lt;dbl [1]&gt; &lt;dbl [1]&gt; We could solve the problem by adding a row with a distinct observation count for each combination of name and key. people2 &lt;- people %&gt;% group_by(name, key) %&gt;% mutate(obs = row_number()) people2 #&gt; # A tibble: 5 x 4 #&gt; # Groups: name, key [4] #&gt; name key value obs #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Phillip Woods age 45 1 #&gt; 2 Phillip Woods height 186 1 #&gt; 3 Phillip Woods age 50 2 #&gt; 4 Jessica Cordero age 37 1 #&gt; 5 Jessica Cordero height 156 1 We can make people2 wider because the combination of name and obs will uniquely identify the rows in the wide data frame. pivot_wider(people2, names_from=&quot;name&quot;, values_from = &quot;value&quot;) #&gt; # A tibble: 3 x 4 #&gt; # Groups: key [2] #&gt; key obs `Phillip Woods` `Jessica Cordero` #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 age 1 45 37 #&gt; 2 height 1 186 156 #&gt; 3 age 2 50 NA Another way to solve this problem is by keeping only distinct rows of the name and key values, and dropping duplicate rows. people %&gt;% distinct(name, key, .keep_all = TRUE) %&gt;% pivot_wider(names_from=&quot;name&quot;, values_from = &quot;value&quot;) #&gt; # A tibble: 2 x 3 #&gt; key `Phillip Woods` `Jessica Cordero` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 age 45 37 #&gt; 2 height 186 156 However, before doing this understand why there are duplicates in the data. The duplicate values may not be just a nuisance, but may indicate deeper problems with the data. Exercise 12.3.4 Tidy the simple tibble below. Do you need to make it wider or longer? What are the variables? preg &lt;- tribble( ~pregnant, ~male, ~female, &quot;yes&quot;, NA, 10, &quot;no&quot;, 20, 12 ) To tidy the preg table use pivot_longer() to create a long table. The variables in this data are: sex (“female”, “male”) pregnant (“yes”, “no”) count, which is a non-negative integer representing the number of observations. The observations in this data are unique combinations of sex and pregnancy status. preg_tidy &lt;- preg %&gt;% pivot_longer(c(male, female), names_to = &quot;sex&quot;, values_to = &quot;count&quot;) preg_tidy #&gt; # A tibble: 4 x 3 #&gt; pregnant sex count #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 yes male NA #&gt; 2 yes female 10 #&gt; 3 no male 20 #&gt; 4 no female 12 Remove the (male, pregnant) row with a missing value to simplify the tidied data frame. preg_tidy2 &lt;- preg %&gt;% pivot_longer(c(male, female), names_to = &quot;sex&quot;, values_to = &quot;count&quot;, values_drop_na = TRUE) preg_tidy2 #&gt; # A tibble: 3 x 3 #&gt; pregnant sex count #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 yes female 10 #&gt; 2 no male 20 #&gt; 3 no female 12 This an example of turning an explicit missing value into an implicit missing value, which is discussed in the upcoming section, Missing Values section. The missing (male, pregnant) row represents an implicit missing value because the value of count can be inferred from its absence. In the tidy data, we can represent rows with missing values of count either explicitly with an NA (as in preg_tidy) or implicitly by the absence of a row (as in preg_tidy2). But in the wide data, the missing values can only be represented explicitly. Though we have already done enough to make the data tidy, there are some other transformations that can clean the data further. If a variable takes two values, like pregnant and sex, it is often preferable to store them as logical vectors. preg_tidy3 &lt;- preg_tidy2 %&gt;% mutate( female = sex == &quot;female&quot;, pregnant = pregnant == &quot;yes&quot; ) %&gt;% select(female, pregnant, count) preg_tidy3 #&gt; # A tibble: 3 x 3 #&gt; female pregnant count #&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; #&gt; 1 TRUE TRUE 10 #&gt; 2 FALSE FALSE 20 #&gt; 3 TRUE FALSE 12 In the previous data frame, I named the logical variable representing the sex female, not sex. This makes the meaning of the variable self-documenting. If the variable were named sex with values TRUE and FALSE, without reading the documentation, we wouldn’t know whether TRUE means male or female. Apart from some minor memory savings, representing these variables as logical vectors results in more clear and concise code. Compare the filter() calls to select non-pregnant females from preg_tidy2 and preg_tidy. filter(preg_tidy2, sex == &quot;female&quot;, pregnant == &quot;no&quot;) #&gt; # A tibble: 1 x 3 #&gt; pregnant sex count #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 no female 12 filter(preg_tidy3, female, !pregnant) #&gt; # A tibble: 1 x 3 #&gt; female pregnant count #&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; #&gt; 1 TRUE FALSE 12 12.4 Separating and uniting Exercise 12.4.1 What do the extra and fill arguments do in separate()? Experiment with the various options for the following two toy datasets. tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) #&gt; Warning: Expected 3 pieces. Additional pieces discarded in 1 rows [2]. #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e f #&gt; 3 h i j tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) #&gt; Warning: Expected 3 pieces. Missing pieces filled with `NA` in 1 rows [2]. #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e &lt;NA&gt; #&gt; 3 f g i The extra argument tells separate() what to do if there are too many pieces, and the fill argument tells it what to do if there aren’t enough. By default, separate() drops extra values with a warning. tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) #&gt; Warning: Expected 3 pieces. Additional pieces discarded in 1 rows [2]. #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e f #&gt; 3 h i j Adding the argument, extra = &quot;drop&quot;, produces the same result as above but without the warning. tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), extra = &quot;drop&quot;) #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e f #&gt; 3 h i j Setting extra = &quot;merge&quot;, then the extra values are not split, so &quot;f,g&quot; appears in column three. tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), extra = &quot;merge&quot;) #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e f,g #&gt; 3 h i j In this example, one of the values, &quot;d,e&quot;, has too few elements. The default for fill is similar to those in separate(); it fills columns with missing values but emits a warning. In this example, the 2nd row of column three is NA. tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) #&gt; Warning: Expected 3 pieces. Missing pieces filled with `NA` in 1 rows [2]. #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e &lt;NA&gt; #&gt; 3 f g i Alternative options for the fill are &quot;right&quot;, to fill with missing values from the right, but without a warning tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), fill = &quot;right&quot;) #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e &lt;NA&gt; #&gt; 3 f g i The option fill = &quot;left&quot; also fills with missing values without emitting a warning, but this time from the left side. Now, the 2nd row of column one will be missing, and the other values in that row are shifted right. tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), fill = &quot;left&quot;) #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 &lt;NA&gt; d e #&gt; 3 f g i Exercise 12.4.2 Both unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE? The remove argument discards input columns in the result data frame. You would set it to FALSE if you want to create a new variable, but keep the old one. Exercise 12.4.3 Compare and contrast separate() and extract(), Why are there three variations of separation (by position, by separator, and with groups), but only one unite? The function separate(), splits a column into multiple columns by separator, if the sep argument is a character vector, or by character positions, if sep is numeric. # example with separators tibble(x = c(&quot;X_1&quot;, &quot;X_2&quot;, &quot;AA_1&quot;, &quot;AA_2&quot;)) %&gt;% separate(x, c(&quot;variable&quot;, &quot;into&quot;), sep = &quot;_&quot;) #&gt; # A tibble: 4 x 2 #&gt; variable into #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 X 1 #&gt; 2 X 2 #&gt; 3 AA 1 #&gt; 4 AA 2 # example with position tibble(x = c(&quot;X1&quot;, &quot;X2&quot;, &quot;Y1&quot;, &quot;Y2&quot;)) %&gt;% separate(x, c(&quot;variable&quot;, &quot;into&quot;), sep = c(1)) #&gt; # A tibble: 4 x 2 #&gt; variable into #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 X 1 #&gt; 2 X 2 #&gt; 3 Y 1 #&gt; 4 Y 2 The function extract() uses a regular expression to specify groups in character vector and split that single character vector into multiple columns. This is more flexible than separate() because it does not require a common separator or specific column positions. # example with separators tibble(x = c(&quot;X_1&quot;, &quot;X_2&quot;, &quot;AA_1&quot;, &quot;AA_2&quot;)) %&gt;% extract(x, c(&quot;variable&quot;, &quot;id&quot;), regex = &quot;([A-Z])_([0-9])&quot;) #&gt; # A tibble: 4 x 2 #&gt; variable id #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 X 1 #&gt; 2 X 2 #&gt; 3 A 1 #&gt; 4 A 2 # example with position tibble(x = c(&quot;X1&quot;, &quot;X2&quot;, &quot;Y1&quot;, &quot;Y2&quot;)) %&gt;% extract(x, c(&quot;variable&quot;, &quot;id&quot;), regex = &quot;([A-Z])([0-9])&quot;) #&gt; # A tibble: 4 x 2 #&gt; variable id #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 X 1 #&gt; 2 X 2 #&gt; 3 Y 1 #&gt; 4 Y 2 # example that separate could not parse tibble(x = c(&quot;X1&quot;, &quot;X20&quot;, &quot;AA11&quot;, &quot;AA2&quot;)) %&gt;% extract(x, c(&quot;variable&quot;, &quot;id&quot;), regex = &quot;([A-Z]+)([0-9]+)&quot;) #&gt; # A tibble: 4 x 2 #&gt; variable id #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 X 1 #&gt; 2 X 20 #&gt; 3 AA 11 #&gt; 4 AA 2 Both separate() and extract() convert a single column to many columns. However, unite() converts many columns to one, with a choice of a separator to include between column values. tibble(variable = c(&quot;X&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;Y&quot;), id = c(1, 2, 1, 2)) %&gt;% unite(x, variable, id, sep = &quot;_&quot;) #&gt; # A tibble: 4 x 1 #&gt; x #&gt; &lt;chr&gt; #&gt; 1 X_1 #&gt; 2 X_2 #&gt; 3 Y_1 #&gt; 4 Y_2 In other words, with extract() and separate() only one column can be chosen, but there are many choices how to split that single column into different columns. With unite(), there are many choices as to which columns to include, but only one choice as to how to combine their contents into a single vector. 12.5 Missing values Exercise 12.5.1 Compare and contrast the fill arguments to pivot_wider() and complete(). The values_fill argument in pivot_wider() and the fill argument to complete() both set vales to replace NA. Both arguments accept named lists to set values for each column. Additionally, the values_fill argument of pivot_wider() accepts a single value. In complete(), the fill argument also sets a value to replace NAs but it is named list, allowing for different values for different variables. Also, both cases replace both implicit and explicit missing values. For example, this will fill in the missing values of the long data frame with 0 complete(): stocks &lt;- tibble( year = c(2015, 2015, 2015, 2015, 2016, 2016, 2016), qtr = c( 1, 2, 3, 4, 2, 3, 4), return = c(1.88, 0.59, 0.35, NA, 0.92, 0.17, 2.66) ) stocks %&gt;% pivot_wider(names_from = year, values_from = return, values_fill = 0) #&gt; # A tibble: 4 x 3 #&gt; qtr `2015` `2016` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1.88 0 #&gt; 2 2 0.59 0.92 #&gt; 3 3 0.35 0.17 #&gt; 4 4 NA 2.66 stocks &lt;- tibble( year = c(2015, 2015, 2015, 2015, 2016, 2016, 2016), qtr = c( 1, 2, 3, 4, 2, 3, 4), return = c(1.88, 0.59, 0.35, NA, 0.92, 0.17, 2.66) ) stocks %&gt;% pivot_wider(names_from = year, values_from = return, values_fill = 0) #&gt; # A tibble: 4 x 3 #&gt; qtr `2015` `2016` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1.88 0 #&gt; 2 2 0.59 0.92 #&gt; 3 3 0.35 0.17 #&gt; 4 4 NA 2.66 For example, this will fill in the missing values of the long data frame with 0 complete(): stocks %&gt;% complete(year, qtr, fill=list(return=0)) #&gt; # A tibble: 8 x 3 #&gt; year qtr return #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2015 1 1.88 #&gt; 2 2015 2 0.59 #&gt; 3 2015 3 0.35 #&gt; 4 2015 4 0 #&gt; 5 2016 1 0 #&gt; 6 2016 2 0.92 #&gt; # … with 2 more rows Exercise 12.5.2 What does the direction argument to fill() do? With fill, the direction determines whether NA values should be replaced by the previous non-missing value (&quot;down&quot;) or the next non-missing value (&quot;up&quot;). 12.6 Case Study This code is repeated from the chapter because it is needed by the exercises. who1 &lt;- who %&gt;% pivot_longer( cols = new_sp_m014:newrel_f65, names_to = &quot;key&quot;, values_to = &quot;cases&quot;, values_drop_na = TRUE ) who1 #&gt; # A tibble: 76,046 x 6 #&gt; country iso2 iso3 year key cases #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 new_sp_m014 0 #&gt; 2 Afghanistan AF AFG 1997 new_sp_m1524 10 #&gt; 3 Afghanistan AF AFG 1997 new_sp_m2534 6 #&gt; 4 Afghanistan AF AFG 1997 new_sp_m3544 3 #&gt; 5 Afghanistan AF AFG 1997 new_sp_m4554 5 #&gt; 6 Afghanistan AF AFG 1997 new_sp_m5564 2 #&gt; # … with 76,040 more rows who2 &lt;- who1 %&gt;% mutate(names_from = stringr::str_replace(key, &quot;newrel&quot;, &quot;new_rel&quot;)) who2 #&gt; # A tibble: 76,046 x 7 #&gt; country iso2 iso3 year key cases names_from #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Afghanistan AF AFG 1997 new_sp_m014 0 new_sp_m014 #&gt; 2 Afghanistan AF AFG 1997 new_sp_m1524 10 new_sp_m1524 #&gt; 3 Afghanistan AF AFG 1997 new_sp_m2534 6 new_sp_m2534 #&gt; 4 Afghanistan AF AFG 1997 new_sp_m3544 3 new_sp_m3544 #&gt; 5 Afghanistan AF AFG 1997 new_sp_m4554 5 new_sp_m4554 #&gt; 6 Afghanistan AF AFG 1997 new_sp_m5564 2 new_sp_m5564 #&gt; # … with 76,040 more rows who3 &lt;- who2 %&gt;% separate(key, c(&quot;new&quot;, &quot;type&quot;, &quot;sexage&quot;), sep = &quot;_&quot;) #&gt; Warning: Expected 3 pieces. Missing pieces filled with `NA` in 2580 rows [243, #&gt; 244, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 903, #&gt; 904, 905, 906, ...]. who3 #&gt; # A tibble: 76,046 x 9 #&gt; country iso2 iso3 year new type sexage cases names_from #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Afghanistan AF AFG 1997 new sp m014 0 new_sp_m014 #&gt; 2 Afghanistan AF AFG 1997 new sp m1524 10 new_sp_m1524 #&gt; 3 Afghanistan AF AFG 1997 new sp m2534 6 new_sp_m2534 #&gt; 4 Afghanistan AF AFG 1997 new sp m3544 3 new_sp_m3544 #&gt; 5 Afghanistan AF AFG 1997 new sp m4554 5 new_sp_m4554 #&gt; 6 Afghanistan AF AFG 1997 new sp m5564 2 new_sp_m5564 #&gt; # … with 76,040 more rows who3 %&gt;% count(new) #&gt; # A tibble: 2 x 2 #&gt; new n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 new 73466 #&gt; 2 newrel 2580 who4 &lt;- who3 %&gt;% select(-new, -iso2, -iso3) who5 &lt;- who4 %&gt;% separate(sexage, c(&quot;sex&quot;, &quot;age&quot;), sep = 1) who5 #&gt; # A tibble: 76,046 x 7 #&gt; country year type sex age cases names_from #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Afghanistan 1997 sp m 014 0 new_sp_m014 #&gt; 2 Afghanistan 1997 sp m 1524 10 new_sp_m1524 #&gt; 3 Afghanistan 1997 sp m 2534 6 new_sp_m2534 #&gt; 4 Afghanistan 1997 sp m 3544 3 new_sp_m3544 #&gt; 5 Afghanistan 1997 sp m 4554 5 new_sp_m4554 #&gt; 6 Afghanistan 1997 sp m 5564 2 new_sp_m5564 #&gt; # … with 76,040 more rows Exercise 12.6.1 In this case study, I set na.rm = TRUE just to make it easier to check that we had the correct values. Is this reasonable? Think about how missing values are represented in this dataset. Are there implicit missing values? What’s the difference between an NA and zero? The reasonableness of using na.rm = TRUE depends on how missing values are represented in this dataset. The main concern is whether a missing value means that there were no cases of TB or whether it means that the WHO does not have data on the number of TB cases. Here are some things we should look for to help distinguish between these cases. If there are no 0 values in the data, then missing values may be used to indicate no cases. If there are both explicit and implicit missing values, then it suggests that missing values are being used differently. In that case, it is likely that explicit missing values would mean no cases, and implicit missing values would mean no data on the number of cases. First, I’ll check for the presence of zeros in the data. who1 %&gt;% filter(cases == 0) %&gt;% nrow() #&gt; [1] 11080 There are zeros in the data, so it appears that cases of zero TB are explicitly indicated, and the value ofNA is used to indicate missing data. Second, I should check whether all values for a (country, year) are missing or whether it is possible for only some columns to be missing. pivot_longer(who, c(new_sp_m014:newrel_f65), names_to = &quot;key&quot;, values_to = &quot;cases&quot;) %&gt;% group_by(country, year) %&gt;% mutate(prop_missing = sum(is.na(cases)) / n()) %&gt;% filter(prop_missing &gt; 0, prop_missing &lt; 1) #&gt; # A tibble: 195,104 x 7 #&gt; # Groups: country, year [3,484] #&gt; country iso2 iso3 year key cases prop_missing #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan AF AFG 1997 new_sp_m014 0 0.75 #&gt; 2 Afghanistan AF AFG 1997 new_sp_m1524 10 0.75 #&gt; 3 Afghanistan AF AFG 1997 new_sp_m2534 6 0.75 #&gt; 4 Afghanistan AF AFG 1997 new_sp_m3544 3 0.75 #&gt; 5 Afghanistan AF AFG 1997 new_sp_m4554 5 0.75 #&gt; 6 Afghanistan AF AFG 1997 new_sp_m5564 2 0.75 #&gt; # … with 195,098 more rows From the results above, it looks like it is possible for a (country, year) row to contain some, but not all, missing values in its columns. Finally, I will check for implicit missing values. Implicit missing values are (year, country) combinations that do not appear in the data. nrow(who) #&gt; [1] 7240 who %&gt;% complete(country, year) %&gt;% nrow() #&gt; [1] 7446 Since the number of complete cases of (country, year) is greater than the number of rows in who, there are some implicit values. But that doesn’t tell us what those implicit missing values are. To do this, I will use the anti_join() function introduced in the later Relational Data chapter. anti_join(complete(who, country, year), who, by = c(&quot;country&quot;, &quot;year&quot;)) %&gt;% select(country, year) %&gt;% group_by(country) %&gt;% # so I can make better sense of the years summarise(min_year = min(year), max_year = max(year)) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 9 x 3 #&gt; country min_year max_year #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Bonaire, Saint Eustatius and Saba 1980 2009 #&gt; 2 Curacao 1980 2009 #&gt; 3 Montenegro 1980 2004 #&gt; 4 Netherlands Antilles 2010 2013 #&gt; 5 Serbia 1980 2004 #&gt; 6 Serbia &amp; Montenegro 2005 2013 #&gt; # … with 3 more rows All of these refer to (country, year) combinations for years prior to the existence of the country. For example, Timor-Leste achieved independence in 2002, so years prior to that are not included in the data. To summarize: 0 is used to represent no cases of TB. Explicit missing values (NAs) are used to represent missing data for (country, year) combinations in which the country existed in that year. Implicit missing values are used to represent missing data because a country did not exist in that year. Exercise 12.6.2 What happens if you neglect the mutate() step? (mutate(key = str_replace(key, &quot;newrel&quot;, &quot;new_rel&quot;)) The separate() function emits the warning “too few values”. If we check the rows for keys beginning with &quot;newrel_&quot;, we see that sexage is missing, and type = m014. who3a &lt;- who1 %&gt;% separate(key, c(&quot;new&quot;, &quot;type&quot;, &quot;sexage&quot;), sep = &quot;_&quot;) #&gt; Warning: Expected 3 pieces. Missing pieces filled with `NA` in 2580 rows [243, #&gt; 244, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 903, #&gt; 904, 905, 906, ...]. filter(who3a, new == &quot;newrel&quot;) %&gt;% head() #&gt; # A tibble: 6 x 8 #&gt; country iso2 iso3 year new type sexage cases #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 2013 newrel m014 &lt;NA&gt; 1705 #&gt; 2 Afghanistan AF AFG 2013 newrel f014 &lt;NA&gt; 1749 #&gt; 3 Albania AL ALB 2013 newrel m014 &lt;NA&gt; 14 #&gt; 4 Albania AL ALB 2013 newrel m1524 &lt;NA&gt; 60 #&gt; 5 Albania AL ALB 2013 newrel m2534 &lt;NA&gt; 61 #&gt; 6 Albania AL ALB 2013 newrel m3544 &lt;NA&gt; 32 Exercise 12.6.3 I claimed that iso2 and iso3 were redundant with country. Confirm this claim. If iso2 and iso3 are redundant with country, then, within each country, there should only be one distinct combination of iso2 and iso3 values, which is the case. select(who3, country, iso2, iso3) %&gt;% distinct() %&gt;% group_by(country) %&gt;% filter(n() &gt; 1) #&gt; # A tibble: 0 x 3 #&gt; # Groups: country [0] #&gt; # … with 3 variables: country &lt;chr&gt;, iso2 &lt;chr&gt;, iso3 &lt;chr&gt; This makes sense, since iso2 and iso3 contain the 2- and 3-letter country abbreviations for the country. The iso2 variable contains each country’s ISO 3166 alpha-2, and the iso3 variable contains each country’s ISO 3166 alpha-3 abbreviation. You may recognize the ISO 3166-2 abbreviations, since they are almost identical to internet country-code top level domains, such as .uk (United Kingdom), .ly (Libya), .tv (Tuvalu), and .io (British Indian Ocean Territory). Exercise 12.6.4 For each country, year, and sex compute the total number of cases of TB. Make an informative visualization of the data. who5 %&gt;% group_by(country, year, sex) %&gt;% filter(year &gt; 1995) %&gt;% summarise(cases = sum(cases)) %&gt;% unite(country_sex, country, sex, remove = FALSE) %&gt;% ggplot(aes(x = year, y = cases, group = country_sex, colour = sex)) + geom_line() #&gt; `summarise()` regrouping output by &#39;country&#39;, &#39;year&#39; (override with `.groups` argument) A small multiples plot faceting by country is difficult given the number of countries. Focusing on those countries with the largest changes or absolute magnitudes after providing the context above is another option. 12.7 Non-tidy data No exercises [ex-12.2.2]: It would be better to join these tables using the methods covered in the Relational Data. We could use inner_join(t2_cases, t2_population, by = c(&quot;country&quot;, &quot;year&quot;)). [non-syntactic]: See the Creating Tibbles section. "],
["relational-data.html", "13 Relational data 13.1 Introduction 13.2 nycflights13 13.3 Keys 13.4 Mutating joins 13.5 Filtering joins 13.6 Join problems 13.7 Set operations", " 13 Relational data 13.1 Introduction The datamodelr package is used to draw database schema. library(&quot;tidyverse&quot;) library(&quot;nycflights13&quot;) library(&quot;viridis&quot;) library(&quot;datamodelr&quot;) 13.2 nycflights13 Exercise 13.2.1 Imagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables would you need to combine? Drawing the routes requires the latitude and longitude of the origin and the destination airports of each flight. This requires the flights and airports tables. The flights table has the origin (origin) and destination (dest) airport of each flight. The airports table has the longitude (lon) and latitude (lat) of each airport. To get the latitude and longitude for the origin and destination of each flight, requires two joins for flights to airports, once for the latitude and longitude of the origin airport, and once for the latitude and longitude of the destination airport. I use an inner join in order to drop any flights with missing airports since they will not have a longitude or latitude. flights_latlon &lt;- flights %&gt;% inner_join(select(airports, origin = faa, origin_lat = lat, origin_lon = lon), by = &quot;origin&quot; ) %&gt;% inner_join(select(airports, dest = faa, dest_lat = lat, dest_lon = lon), by = &quot;dest&quot; ) This plots the approximate flight paths of the first 100 flights in the flights dataset. flights_latlon %&gt;% slice(1:100) %&gt;% ggplot(aes( x = origin_lon, xend = dest_lon, y = origin_lat, yend = dest_lat )) + borders(&quot;state&quot;) + geom_segment(arrow = arrow(length = unit(0.1, &quot;cm&quot;))) + coord_quickmap() + labs(y = &quot;Latitude&quot;, x = &quot;Longitude&quot;) Exercise 13.2.2 I forgot to draw the relationship between weather and airports. What is the relationship and how should it appear in the diagram? The column airports$faa is a foreign key of weather$origin. The following drawing updates the one in Section 13.2 to include this relation. The line representing the new relation between weather and airports is colored black. The lines representing the old relations are gray and thinner. Exercise 13.2.3 Weather only contains information for the origin (NYC) airports. If it contained weather records for all airports in the USA, what additional relation would it define with flights? If the weather was included for all airports in the US, then it would provide the weather for the destination of each flight. The weather data frame columns (year, month, day, hour, origin) are a foreign key for the flights data frame columns (year, month, day, hour, dest). This would provide information about the weather at the destination airport at the time of the flight take off, unless the arrival date-time were calculated. So why was this not a relationship prior to adding additional rows to the weather table? In a foreign key relationship, the collection of columns in the child table must refer to a unique collection of columns in the parent table. When the weather table only contained New York airports, there were many values of (year, month, day, hour, dest) in flights that did not appear in the weather table. Therefore, it was not a foreign key. It was only after all combinations of year, month, day, hour, and airports that are defined in flights were added to the weather table that there existed this relation between these tables. Exercise 13.2.4 We know that some days of the year are “special”, and fewer people than usual fly on them. How might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables? I would add a table of special dates, similar to the following table. special_days &lt;- tribble( ~year, ~month, ~day, ~holiday, 2013, 01, 01, &quot;New Years Day&quot;, 2013, 07, 04, &quot;Independence Day&quot;, 2013, 11, 29, &quot;Thanksgiving Day&quot;, 2013, 12, 25, &quot;Christmas Day&quot; ) The primary key of the table would be the (year, month, day) columns. The (year, month, day) columns could be used to join special_days with other tables. 13.3 Keys Exercise 13.3.1 Add a surrogate key to flights. I add the column flight_id as a surrogate key. I sort the data prior to making the key, even though it is not strictly necessary, so the order of the rows has some meaning. flights %&gt;% arrange(year, month, day, sched_dep_time, carrier, flight) %&gt;% mutate(flight_id = row_number()) %&gt;% glimpse() #&gt; Rows: 336,776 #&gt; Columns: 20 #&gt; $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, … #&gt; $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … #&gt; $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … #&gt; $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 559, 558, 559, 558, 558, 557,… #&gt; $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 558, 559, 600, 600, 600, 600, 600,… #&gt; $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -4, 0, -2, -1, -2, -2, -3, NA, 1, 0, -5, … #&gt; $ arr_time &lt;int&gt; 830, 850, 923, 1004, 740, 702, 753, 941, 849, 853, 838… #&gt; $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 728, 706, 745, 910, 851, 856, 846… #&gt; $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, 12, -4, 8, 31, -2, -3, -8, NA, -6, -7… #&gt; $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;B6&quot;, … #&gt; $ flight &lt;int&gt; 1545, 1714, 1141, 725, 1696, 1806, 301, 707, 49, 71, 7… #&gt; $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N39463&quot;, &quot;N70… #&gt; $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;EWR&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;LGA&quot;… #&gt; $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ORD&quot;, &quot;BOS&quot;, &quot;ORD&quot;, &quot;DFW&quot;… #&gt; $ air_time &lt;dbl&gt; 227, 227, 160, 183, 150, 44, 138, 257, 149, 158, 140, … #&gt; $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 719, 187, 733, 1389, 1028, 100… #&gt; $ hour &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, … #&gt; $ minute &lt;dbl&gt; 15, 29, 40, 45, 58, 59, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 … #&gt; $ flight_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,… Exercise 13.3.2 Identify the keys in the following datasets Lahman::Batting babynames::babynames nasaweather::atmos fueleconomy::vehicles ggplot2::diamonds (You might need to install some packages and read some documentation.) The answer to each part follows. The primary key for Lahman::Batting is (playerID, yearID, stint). The columns (playerID, yearID) are not a primary key because players can play on different teams within the same year. Lahman::Batting %&gt;% count(playerID, yearID, stint) %&gt;% filter(n &gt; 1) %&gt;% nrow() #&gt; [1] 0 The primary key for babynames::babynames is (year, sex, name). The columns (year, name) are not a primary key since there are separate counts for each name for each sex, and the same names can be used by more than one sex. babynames::babynames %&gt;% count(year, sex, name) %&gt;% filter(n &gt; 1) %&gt;% nrow() #&gt; Using `n` as weighting variable #&gt; ℹ Quiet this message with `wt = n` or count rows with `wt = 1` #&gt; [1] 1924665 The primary key for nasaweather::atmos is (lat, long, year, month). The primary key represents the location and time that the measurement was taken. nasaweather::atmos %&gt;% count(lat, long, year, month) %&gt;% filter(n &gt; 1) %&gt;% nrow() #&gt; [1] 0 The column id, the unique EPA identifier of the vehicle, is the primary key for fueleconomy::vehicles. fueleconomy::vehicles %&gt;% count(id) %&gt;% filter(n &gt; 1) %&gt;% nrow() #&gt; [1] 0 There is no primary key for ggplot2::diamonds since there is no combination of variables that uniquely identifies each observation. This is implied by the fact that the number of distinct rows in the dataset is less than the total number of rows, meaning that there are some duplicate rows. ggplot2::diamonds %&gt;% distinct() %&gt;% nrow() #&gt; [1] 53794 nrow(ggplot2::diamonds) #&gt; [1] 53940 If we need a unique identifier for our analysis, we could add a surrogate key. diamonds &lt;- mutate(ggplot2::diamonds, id = row_number()) Exercise 13.3.3 Draw a diagram illustrating the connections between the Batting, Master, and Salaries tables in the Lahman package. Draw another diagram that shows the relationship between Master, Managers, AwardsManagers. How would you characterize the relationship between the Batting, Pitching, and Fielding tables? For the Batting, Master, and Salaries tables: Master Primary key: playerID Batting Primary key: playerID, yearID, stint Foreign keys: playerID = Master$playerID (many-to-1) Salaries Primary key: yearID, teamID, playerID Foreign keys: playerID = Master$playerID (many-to-1) The columns teamID and lgID are not foreign keys even though they appear in multiple tables (with the same meaning) because they are not primary keys for any of the tables considered in this exercise. The teamID variable references Teams$teamID, and lgID does not have its own table. R for Data Science uses database schema diagrams to illustrate relations between the tables. Most flowchart or diagramming software can be used used to create database schema diagrams, as well as some specialized database software. The diagrams in R for Data Science were created with OmniGraffle, and their sources can be found in its GitHub repository. The following diagram was created with OmniGraffle in the same style as those in R for Data Science . It shows the relations between the Master, Batting and Salaries tables. Another option to draw database schema diagrams is the R package datamodelr, which can programmatically create database schema diagrams. The following code uses datamodelr to draw a diagram of the relations between the Batting, Master, and Salaries tables. dm1 &lt;- dm_from_data_frames(list( Batting = Lahman::Batting, Master = Lahman::Master, Salaries = Lahman::Salaries )) %&gt;% dm_set_key(&quot;Batting&quot;, c(&quot;playerID&quot;, &quot;yearID&quot;, &quot;stint&quot;)) %&gt;% dm_set_key(&quot;Master&quot;, &quot;playerID&quot;) %&gt;% dm_set_key(&quot;Salaries&quot;, c(&quot;yearID&quot;, &quot;teamID&quot;, &quot;playerID&quot;)) %&gt;% dm_add_references( Batting$playerID == Master$playerID, Salaries$playerID == Master$playerID ) dm_create_graph(dm1, rankdir = &quot;LR&quot;, columnArrows = TRUE) %&gt;% dm_render_graph() For the Master, Manager, and AwardsManagers tables: Master Primary key: playerID Managers Primary key: yearID, teamID, inseason Foreign keys: playerID references Master$playerID (many-to-1) AwardsManagers: Primary key: playerID, awardID, yearID Foreign keys: playerID references Master$playerID (many-to-1) For AwardsManagers, the columns (awardID, yearID, lgID) are not a primary key because there can be, and have been ties, as indicated by the tie variable. The relations between the Master, Managers, and AwardsManagers tables are shown in the following two diagrams: the first created manually with OmniGraffle, and the second programmatically in R with the datamodelr package. dm2 &lt;- dm_from_data_frames(list( Master = Lahman::Master, Managers = Lahman::Managers, AwardsManagers = Lahman::AwardsManagers )) %&gt;% dm_set_key(&quot;Master&quot;, &quot;playerID&quot;) %&gt;% dm_set_key(&quot;Managers&quot;, c(&quot;yearID&quot;, &quot;teamID&quot;, &quot;inseason&quot;)) %&gt;% dm_set_key(&quot;AwardsManagers&quot;, c(&quot;playerID&quot;, &quot;awardID&quot;, &quot;yearID&quot;)) %&gt;% dm_add_references( Managers$playerID == Master$playerID, AwardsManagers$playerID == Master$playerID ) dm_create_graph(dm2, rankdir = &quot;LR&quot;, columnArrows = TRUE) %&gt;% dm_render_graph() The primary keys of Batting, Pitching, and Fielding are the following: Batting: (playerID, yearID, stint) Pitching: (playerID, yearID, stint) Fielding: (playerID, yearID, stint, POS). While Batting and Pitching has one row per player, year, stint, the Fielding table has additional rows for each position (POS) a player played within a stint. Since Batting, Pitching, and Fielding all share the playerID, yearID, and stint we would expect some foreign key relations between these tables. The columns (playerID, yearID, stint) in Pitching are a foreign key which references the same columns in Batting. We can check this by checking that all observed combinations of values of these columns appearing in Pitching also appear in Batting. To do this I use an anti-join, which is discussed in the section Filtering Joins. nrow(anti_join(Lahman::Pitching, Lahman::Batting, by = c(&quot;playerID&quot;, &quot;yearID&quot;, &quot;stint&quot;) )) #&gt; [1] 0 Similarly, the columns (playerID, yearID, stint) in Fielding are a foreign key which references the same columns in Batting. nrow(anti_join(Lahman::Fielding, Lahman::Batting, by = c(&quot;playerID&quot;, &quot;yearID&quot;, &quot;stint&quot;) )) #&gt; [1] 0 The following diagram shows the relations between the Batting, Pitching, and Fielding tables. 13.4 Mutating joins flights2 &lt;- flights %&gt;% select(year:day, hour, origin, dest, tailnum, carrier) Exercise 13.4.1 Compute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States: airports %&gt;% semi_join(flights, c(&quot;faa&quot; = &quot;dest&quot;)) %&gt;% ggplot(aes(lon, lat)) + borders(&quot;state&quot;) + geom_point() + coord_quickmap() (Don’t worry if you don’t understand what semi_join() does — you’ll learn about it next.) You might want to use the size or color of the points to display the average delay for each airport. avg_dest_delays &lt;- flights %&gt;% group_by(dest) %&gt;% # arrival delay NA&#39;s are cancelled flights summarise(delay = mean(arr_delay, na.rm = TRUE)) %&gt;% inner_join(airports, by = c(dest = &quot;faa&quot;)) #&gt; `summarise()` ungrouping output (override with `.groups` argument) avg_dest_delays %&gt;% ggplot(aes(lon, lat, colour = delay)) + borders(&quot;state&quot;) + geom_point() + coord_quickmap() Exercise 13.4.2 Add the location of the origin and destination (i.e. the lat and lon) to flights. You can perform one join after another. If duplicate variables are found, by default, dplyr will distinguish the two by adding .x, and .y to the ends of the variable names to solve naming conflicts. airport_locations &lt;- airports %&gt;% select(faa, lat, lon) flights %&gt;% select(year:day, hour, origin, dest) %&gt;% left_join( airport_locations, by = c(&quot;origin&quot; = &quot;faa&quot;) ) %&gt;% left_join( airport_locations, by = c(&quot;dest&quot; = &quot;faa&quot;) ) #&gt; # A tibble: 336,776 x 10 #&gt; year month day hour origin dest lat.x lon.x lat.y lon.y #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 5 EWR IAH 40.7 -74.2 30.0 -95.3 #&gt; 2 2013 1 1 5 LGA IAH 40.8 -73.9 30.0 -95.3 #&gt; 3 2013 1 1 5 JFK MIA 40.6 -73.8 25.8 -80.3 #&gt; 4 2013 1 1 5 JFK BQN 40.6 -73.8 NA NA #&gt; 5 2013 1 1 6 LGA ATL 40.8 -73.9 33.6 -84.4 #&gt; 6 2013 1 1 5 EWR ORD 40.7 -74.2 42.0 -87.9 #&gt; # … with 336,770 more rows The suffix argument overrides this default behavior. Since is always good practice to have clear variable names, I will use the suffixes &quot;_dest&quot; and &quot;_origin&quot; to specify whether the column refers to the destination or origin airport. airport_locations &lt;- airports %&gt;% select(faa, lat, lon) flights %&gt;% select(year:day, hour, origin, dest) %&gt;% left_join( airport_locations, by = c(&quot;origin&quot; = &quot;faa&quot;) ) %&gt;% left_join( airport_locations, by = c(&quot;dest&quot; = &quot;faa&quot;), suffix = c(&quot;_origin&quot;, &quot;_dest&quot;) # existing lat and lon variables in tibble gain the _origin suffix # new lat and lon variables are given _dest suffix ) #&gt; # A tibble: 336,776 x 10 #&gt; year month day hour origin dest lat_origin lon_origin lat_dest lon_dest #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 5 EWR IAH 40.7 -74.2 30.0 -95.3 #&gt; 2 2013 1 1 5 LGA IAH 40.8 -73.9 30.0 -95.3 #&gt; 3 2013 1 1 5 JFK MIA 40.6 -73.8 25.8 -80.3 #&gt; 4 2013 1 1 5 JFK BQN 40.6 -73.8 NA NA #&gt; 5 2013 1 1 6 LGA ATL 40.8 -73.9 33.6 -84.4 #&gt; 6 2013 1 1 5 EWR ORD 40.7 -74.2 42.0 -87.9 #&gt; # … with 336,770 more rows Exercise 13.4.3 Is there a relationship between the age of a plane and its delays? The question does not specify whether the relationship is with departure delay or arrival delay. I will look at both. To compare the age of the plane to flights delay, I merge flights with the planes, which contains a variable plane_year, with the year in which the plane was built. To look at the relationship between plane age and departure delay, I will calculate the average arrival and departure delay for each age of a flight. Since there are few planes older than 25 years, so I truncate age at 25 years. plane_cohorts &lt;- inner_join(flights, select(planes, tailnum, plane_year = year), by = &quot;tailnum&quot; ) %&gt;% mutate(age = year - plane_year) %&gt;% filter(!is.na(age)) %&gt;% mutate(age = if_else(age &gt; 25, 25L, age)) %&gt;% group_by(age) %&gt;% summarise( dep_delay_mean = mean(dep_delay, na.rm = TRUE), dep_delay_sd = sd(dep_delay, na.rm = TRUE), arr_delay_mean = mean(arr_delay, na.rm = TRUE), arr_delay_sd = sd(arr_delay, na.rm = TRUE), n_arr_delay = sum(!is.na(arr_delay)), n_dep_delay = sum(!is.na(dep_delay)) ) #&gt; `summarise()` ungrouping output (override with `.groups` argument) I will look for a relationship between departure delay and age by plotting age against the average departure delay. The average departure delay is increasing for planes with ages up until 10 years. After that the departure delay decreases or levels off. The decrease in departure delay could be because older planes with many mechanical issues are removed from service or because air lines schedule these planes with enough time so that mechanical issues do not delay them. ggplot(plane_cohorts, aes(x = age, y = dep_delay_mean)) + geom_point() + scale_x_continuous(&quot;Age of plane (years)&quot;, breaks = seq(0, 30, by = 10)) + scale_y_continuous(&quot;Mean Departure Delay (minutes)&quot;) There is a similar relationship in arrival delays. Delays increase with the age of the plane until ten years, then it declines and flattens out. ggplot(plane_cohorts, aes(x = age, y = arr_delay_mean)) + geom_point() + scale_x_continuous(&quot;Age of Plane (years)&quot;, breaks = seq(0, 30, by = 10)) + scale_y_continuous(&quot;Mean Arrival Delay (minutes)&quot;) Exercise 13.4.4 What weather conditions make it more likely to see a delay? Almost any amount of precipitation is associated with a delay. However, there is not a strong a trend above 0.02 in. of precipitation. flight_weather &lt;- flights %&gt;% inner_join(weather, by = c( &quot;origin&quot; = &quot;origin&quot;, &quot;year&quot; = &quot;year&quot;, &quot;month&quot; = &quot;month&quot;, &quot;day&quot; = &quot;day&quot;, &quot;hour&quot; = &quot;hour&quot; )) flight_weather %&gt;% group_by(precip) %&gt;% summarise(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = precip, y = delay)) + geom_line() + geom_point() #&gt; `summarise()` ungrouping output (override with `.groups` argument) There seems to be a stronger relationship between visibility and delay. Delays are higher when visibility is less than 2 miles. flight_weather %&gt;% ungroup() %&gt;% mutate(visib_cat = cut_interval(visib, n = 10)) %&gt;% group_by(visib_cat) %&gt;% summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = visib_cat, y = dep_delay)) + geom_point() #&gt; `summarise()` ungrouping output (override with `.groups` argument) Exercise 13.4.5 What happened on June 13, 2013? Display the spatial pattern of delays, and then use Google to cross-reference with the weather. There was a large series of storms (derechos) in the southeastern US (see June 12-13, 2013 derecho series). The following plot show that the largest delays were in Tennessee (Nashville), the Southeast, and the Midwest, which were the locations of the derechos. flights %&gt;% filter(year == 2013, month == 6, day == 13) %&gt;% group_by(dest) %&gt;% summarise(delay = mean(arr_delay, na.rm = TRUE)) %&gt;% inner_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) %&gt;% ggplot(aes(y = lat, x = lon, size = delay, colour = delay)) + borders(&quot;state&quot;) + geom_point() + coord_quickmap() + scale_colour_viridis() #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; Warning: Removed 3 rows containing missing values (geom_point). 13.5 Filtering joins Exercise 13.5.1 What does it mean for a flight to have a missing tailnum? What do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.) Flights that have a missing tailnum all have missing values of arr_time, meaning that the flight was canceled. flights %&gt;% filter(is.na(tailnum), !is.na(arr_time)) %&gt;% nrow() #&gt; [1] 0 Many of the tail numbers that don’t have a matching value in planes are registered to American Airlines (AA) or Envoy Airlines (MQ). The documentation for planes states American Airways (AA) and Envoy Air (MQ) report fleet numbers rather than tail numbers so can’t be matched. flights %&gt;% anti_join(planes, by = &quot;tailnum&quot;) %&gt;% count(carrier, sort = TRUE) %&gt;% mutate(p = n / sum(n)) #&gt; # A tibble: 10 x 3 #&gt; carrier n p #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 MQ 25397 0.483 #&gt; 2 AA 22558 0.429 #&gt; 3 UA 1693 0.0322 #&gt; 4 9E 1044 0.0198 #&gt; 5 B6 830 0.0158 #&gt; 6 US 699 0.0133 #&gt; # … with 4 more rows However, not all tail numbers appearing inflights from these carriers are missing from the planes table. I don’t know how to reconcile this discrepancy. flights %&gt;% distinct(carrier, tailnum) %&gt;% left_join(planes, by = &quot;tailnum&quot;) %&gt;% group_by(carrier) %&gt;% summarise(total_planes = n(), not_in_planes = sum(is.na(model))) %&gt;% mutate(missing_pct = not_in_planes / total_planes) %&gt;% arrange(desc(missing_pct)) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 16 x 4 #&gt; carrier total_planes not_in_planes missing_pct #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 MQ 238 234 0.983 #&gt; 2 AA 601 430 0.715 #&gt; 3 F9 26 3 0.115 #&gt; 4 FL 129 12 0.0930 #&gt; 5 UA 621 23 0.0370 #&gt; 6 US 290 9 0.0310 #&gt; # … with 10 more rows Exercise 13.5.2 Filter flights to only show flights with planes that have flown at least 100 flights. First, I find all planes that have flown at least 100 flights. I need to filter flights that are missing a tail number otherwise all flights missing a tail number will be treated as a single plane. planes_gte100 &lt;- flights %&gt;% filter(!is.na(tailnum)) %&gt;% group_by(tailnum) %&gt;% count() %&gt;% filter(n &gt;= 100) Now, I will semi join the data frame of planes that have flown at least 100 flights to the data frame of flights to select the flights by those planes. flights %&gt;% semi_join(planes_gte100, by = &quot;tailnum&quot;) #&gt; # A tibble: 228,390 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 544 545 -1 1004 1022 #&gt; 4 2013 1 1 554 558 -4 740 728 #&gt; 5 2013 1 1 555 600 -5 913 854 #&gt; 6 2013 1 1 557 600 -3 709 723 #&gt; # … with 228,384 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; This can also be answered with a grouped mutate. flights %&gt;% filter(!is.na(tailnum)) %&gt;% group_by(tailnum) %&gt;% mutate(n = n()) %&gt;% filter(n &gt;= 100) #&gt; # A tibble: 228,390 x 20 #&gt; # Groups: tailnum [1,217] #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 544 545 -1 1004 1022 #&gt; 4 2013 1 1 554 558 -4 740 728 #&gt; 5 2013 1 1 555 600 -5 913 854 #&gt; 6 2013 1 1 557 600 -3 709 723 #&gt; # … with 228,384 more rows, and 12 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, #&gt; # n &lt;int&gt; Exercise 13.5.3 Combine fueleconomy::vehicles and fueleconomy::common to find only the records for the most common models. fueleconomy::vehicles %&gt;% semi_join(fueleconomy::common, by = c(&quot;make&quot;, &quot;model&quot;)) #&gt; # A tibble: 14,531 x 12 #&gt; id make model year class trans drive cyl displ fuel hwy cty #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1833 Acura Integ… 1986 Subcom… Automa… Front-… 4 1.6 Regu… 28 22 #&gt; 2 1834 Acura Integ… 1986 Subcom… Manual… Front-… 4 1.6 Regu… 28 23 #&gt; 3 3037 Acura Integ… 1987 Subcom… Automa… Front-… 4 1.6 Regu… 28 22 #&gt; 4 3038 Acura Integ… 1987 Subcom… Manual… Front-… 4 1.6 Regu… 28 23 #&gt; 5 4183 Acura Integ… 1988 Subcom… Automa… Front-… 4 1.6 Regu… 27 22 #&gt; 6 4184 Acura Integ… 1988 Subcom… Manual… Front-… 4 1.6 Regu… 28 23 #&gt; # … with 14,525 more rows Why does the above code join on make and model and not just model? It is possible for two car brands (make) to produce a car with the same name (model). In both the vehicles and common data we can find some examples. For example, “Truck 4WD” is produced by many different brands. fueleconomy::vehicles %&gt;% distinct(model, make) %&gt;% group_by(model) %&gt;% filter(n() &gt; 1) %&gt;% arrange(model) #&gt; # A tibble: 126 x 2 #&gt; # Groups: model [60] #&gt; make model #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Audi 200 #&gt; 2 Chrysler 200 #&gt; 3 Mcevoy Motors 240 DL/240 GL Wagon #&gt; 4 Volvo 240 DL/240 GL Wagon #&gt; 5 Lambda Control Systems 300E #&gt; 6 Mercedes-Benz 300E #&gt; # … with 120 more rows fueleconomy::common %&gt;% distinct(model, make) %&gt;% group_by(model) %&gt;% filter(n() &gt; 1) %&gt;% arrange(model) #&gt; # A tibble: 8 x 2 #&gt; # Groups: model [3] #&gt; make model #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Dodge Colt #&gt; 2 Plymouth Colt #&gt; 3 Mitsubishi Truck 2WD #&gt; 4 Nissan Truck 2WD #&gt; 5 Toyota Truck 2WD #&gt; 6 Mitsubishi Truck 4WD #&gt; # … with 2 more rows If we were to merge these data on the model column alone, there would be incorrect matches. Exercise 13.5.4 Find the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns? I will start by clarifying how I will be measuring the concepts in the question. There are three concepts that need to be defined more precisely. What is meant by “delay”? I will use departure delay. Since the weather data only contains data for the New York City airports, and departure delays will be more sensitive to New York City weather conditions than arrival delays. What is meant by “worst”? I define worst delay as the average departure delay per flight for flights scheduled to depart in that hour. For hour, I will use the scheduled departure time rather than the actual departure time. If planes are delayed due to weather conditions, the weather conditions during the scheduled time are more important than the actual departure time, at which point, the weather could have improved. What is meant by “48 hours over the course of the year”? This could mean two days, a span of 48 contiguous hours, or 48 hours that are not necessarily contiguous hours. I will find 48 not-necessarily contiguous hours. That definition makes better use of the methods introduced in this section and chapter. What is the unit of analysis? Although the question mentions only hours, I will use airport hours. The weather dataset has an observation for each airport for each hour. Since all the departure airports are in the vicinity of New York City, their weather should be similar, it will not be the same. First, I need to find the 48 hours with the worst delays. I group flights by hour of scheduled departure time and calculate the average delay. Then I select the 48 observations (hours) with the highest average delay. worst_hours &lt;- flights %&gt;% mutate(hour = sched_dep_time %/% 100) %&gt;% group_by(origin, year, month, day, hour) %&gt;% summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() %&gt;% arrange(desc(dep_delay)) %&gt;% slice(1:48) #&gt; `summarise()` regrouping output by &#39;origin&#39;, &#39;year&#39;, &#39;month&#39;, &#39;day&#39; (override with `.groups` argument) Then I can use semi_join() to get the weather for these hours. weather_most_delayed &lt;- semi_join(weather, worst_hours, by = c(&quot;origin&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;)) For weather, I’ll focus on precipitation, wind speed, and temperature. I will display these in both a table and a plot. Many of these observations have a higher than average wind speed (10 mph) or some precipitation. However, I would have expected the weather for the hours with the worst delays to be much worse. select(weather_most_delayed, temp, wind_speed, precip) %&gt;% print(n = 48) #&gt; # A tibble: 48 x 3 #&gt; temp wind_speed precip #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 27.0 13.8 0 #&gt; 2 28.0 19.6 0 #&gt; 3 28.9 28.8 0 #&gt; 4 33.8 9.21 0.06 #&gt; 5 34.0 8.06 0.05 #&gt; 6 80.1 8.06 0 #&gt; 7 86 13.8 0 #&gt; 8 73.4 6.90 0.08 #&gt; 9 84.0 5.75 0 #&gt; 10 78.8 18.4 0.23 #&gt; 11 53.6 0 0 #&gt; 12 60.8 31.1 0.11 #&gt; 13 55.4 17.3 0.14 #&gt; 14 53.1 9.21 0.01 #&gt; 15 55.9 11.5 0.1 #&gt; 16 55.4 8.06 0.15 #&gt; 17 57.0 29.9 0 #&gt; 18 33.8 20.7 0.02 #&gt; 19 34.0 19.6 0.01 #&gt; 20 36.0 21.9 0.01 #&gt; 21 37.9 16.1 0 #&gt; 22 32 13.8 0.12 #&gt; 23 60.1 33.4 0.14 #&gt; 24 60.8 11.5 0.02 #&gt; 25 62.1 17.3 0 #&gt; 26 66.9 10.4 0 #&gt; 27 66.9 13.8 0 #&gt; 28 79.0 10.4 0 #&gt; 29 77 16.1 0.07 #&gt; 30 75.9 13.8 0 #&gt; 31 82.4 8.06 0 #&gt; 32 86 9.21 0 #&gt; 33 80.1 9.21 0 #&gt; 34 80.6 11.5 0 #&gt; 35 78.1 6.90 0 #&gt; 36 75.2 10.4 0.01 #&gt; 37 73.9 5.75 0.03 #&gt; 38 73.9 8.06 0 #&gt; 39 75.0 4.60 0 #&gt; 40 75.0 4.60 0.01 #&gt; 41 80.1 0 0.01 #&gt; 42 80.1 0 0 #&gt; 43 77 10.4 0 #&gt; 44 82.0 10.4 0 #&gt; 45 72.0 13.8 0.3 #&gt; 46 72.0 4.60 0.03 #&gt; 47 51.1 4.60 0 #&gt; 48 54.0 6.90 0 ggplot(weather_most_delayed, aes(x = precip, y = wind_speed, color = temp)) + geom_point() It’s hard to say much more than that without using the tools from Exploratory Data Analysis section to look for covariation between weather and flight delays using all flights. Implicitly in my informal analysis of trends in weather using only the 48 hours with the worst delays, I was comparing the weather in these hours to some belief I had about what constitutes “normal” or “good” weather. It would be better to actually use data to make that comparison. Exercise 13.5.5 What does anti_join(flights, airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) tell you? What does anti_join(airports, flights, by = c(&quot;faa&quot; = &quot;dest&quot;)) tell you? The expression anti_join(flights, airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) returns the flights that went to an airport that is not in the FAA list of destinations. Since the FAA list only contains domestic airports, these are likely foreign flights. However, running that expression that there are only four airports in this list. anti_join(flights, airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) %&gt;% distinct(dest) #&gt; # A tibble: 4 x 1 #&gt; dest #&gt; &lt;chr&gt; #&gt; 1 BQN #&gt; 2 SJU #&gt; 3 STT #&gt; 4 PSE In this set of four airports three are in Puerto Rico (BQN, SJU, and PSE) and one is in the US Virgin Islands ( STT). The reason for this discrepancy is that the flights and airports tables are derived from different sources. The flights data comes from the US Department of Transportation Bureau of Transportation Statistics, while the airport metadata comes from openflights.org. The BTS includes Puerto Rico and U.S. Virgin Islands as “domestic” (part of the US), while the openflights.org give use different values of country for airports in the US states (&quot;United States&quot;) Puerto Rico (&quot;Puerto Rico&quot;) and US Virgin Islands (&quot;Virgin Islands&quot;). The expression anti_join(airports, flights, by = c(&quot;faa&quot; = &quot;dest&quot;)) returns the US airports that were not the destination of any flight in the data. Since the data contains all flights from New York City airports, this is also the list of US airports that did not have a nonstop flight from New York City in 2013. anti_join(airports, flights, by = c(&quot;faa&quot; = &quot;dest&quot;)) #&gt; # A tibble: 1,357 x 8 #&gt; faa name lat lon alt tz dst tzone #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 04G Lansdowne Airport 41.1 -80.6 1044 -5 A America/New_Y… #&gt; 2 06A Moton Field Municipal Airp… 32.5 -85.7 264 -6 A America/Chica… #&gt; 3 06C Schaumburg Regional 42.0 -88.1 801 -6 A America/Chica… #&gt; 4 06N Randall Airport 41.4 -74.4 523 -5 A America/New_Y… #&gt; 5 09J Jekyll Island Airport 31.1 -81.4 11 -5 A America/New_Y… #&gt; 6 0A9 Elizabethton Municipal Air… 36.4 -82.2 1593 -5 A America/New_Y… #&gt; # … with 1,351 more rows Exercise 13.5.6 You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned above. At each point in time, each plane is flown by a single airline. However, a plane can be sold and fly for multiple airlines. Logically, it is possible that a plane can fly for multiple airlines over the course of its lifetime. But, it is not necessarily the case that a plane will fly for more than one airline in this data, especially since it comprises only a year of data. So let’s check to see if there are any planes in the data flew for multiple airlines. First, find all distinct airline, plane combinations. planes_carriers &lt;- flights %&gt;% filter(!is.na(tailnum)) %&gt;% distinct(tailnum, carrier) The number of planes that have flown for more than one airline are those tailnum that appear more than once in the planes_carriers data. planes_carriers %&gt;% count(tailnum) %&gt;% filter(n &gt; 1) %&gt;% nrow() #&gt; [1] 17 The names of airlines are easier to understand than the two-letter carrier codes. The airlines data frame contains the names of the airlines. carrier_transfer_tbl &lt;- planes_carriers %&gt;% # keep only planes which have flown for more than one airline group_by(tailnum) %&gt;% filter(n() &gt; 1) %&gt;% # join with airlines to get airline names left_join(airlines, by = &quot;carrier&quot;) %&gt;% arrange(tailnum, carrier) carrier_transfer_tbl #&gt; # A tibble: 34 x 3 #&gt; # Groups: tailnum [17] #&gt; carrier tailnum name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 9E N146PQ Endeavor Air Inc. #&gt; 2 EV N146PQ ExpressJet Airlines Inc. #&gt; 3 9E N153PQ Endeavor Air Inc. #&gt; 4 EV N153PQ ExpressJet Airlines Inc. #&gt; 5 9E N176PQ Endeavor Air Inc. #&gt; 6 EV N176PQ ExpressJet Airlines Inc. #&gt; # … with 28 more rows 13.6 Join problems No exercises 13.7 Set operations No exercises "],
["strings.html", "14 Strings 14.1 Introduction 14.2 String basics 14.3 Matching patterns with regular expressions 14.4 Tools 14.5 Other types of pattern 14.6 Other uses of regular expressions 14.7 stringi", " 14 Strings 14.1 Introduction library(&quot;tidyverse&quot;) 14.2 String basics Exercise 14.2.1 In code that doesn’t use stringr, you’ll often see paste() and paste0(). What’s the difference between the two functions? What stringr function are they equivalent to? How do the functions differ in their handling of NA? The function paste() separates strings by spaces by default, while paste0() does not separate strings with spaces by default. paste(&quot;foo&quot;, &quot;bar&quot;) #&gt; [1] &quot;foo bar&quot; paste0(&quot;foo&quot;, &quot;bar&quot;) #&gt; [1] &quot;foobar&quot; Since str_c() does not separate strings with spaces by default it is closer in behavior to paste0(). str_c(&quot;foo&quot;, &quot;bar&quot;) #&gt; [1] &quot;foobar&quot; However, str_c() and the paste function handle NA differently. The function str_c() propagates NA, if any argument is a missing value, it returns a missing value. This is in line with how the numeric R functions, e.g. sum(), mean(), handle missing values. However, the paste functions, convert NA to the string &quot;NA&quot; and then treat it as any other character vector. str_c(&quot;foo&quot;, NA) #&gt; [1] NA paste(&quot;foo&quot;, NA) #&gt; [1] &quot;foo NA&quot; paste0(&quot;foo&quot;, NA) #&gt; [1] &quot;fooNA&quot; Exercise 14.2.2 In your own words, describe the difference between the sep and collapse arguments to str_c(). The sep argument is the string inserted between arguments to str_c(), while collapse is the string used to separate any elements of the character vector into a character vector of length one. Exercise 14.2.3 Use str_length() and str_sub() to extract the middle character from a string. What will you do if the string has an even number of characters? The following function extracts the middle character. If the string has an even number of characters the choice is arbitrary. We choose to select \\(\\lceil n / 2 \\rceil\\), because that case works even if the string is only of length one. A more general method would allow the user to select either the floor or ceiling for the middle character of an even string. x &lt;- c(&quot;a&quot;, &quot;abc&quot;, &quot;abcd&quot;, &quot;abcde&quot;, &quot;abcdef&quot;) L &lt;- str_length(x) m &lt;- ceiling(L / 2) str_sub(x, m, m) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;c&quot; Exercise 14.2.4 What does str_wrap() do? When might you want to use it? The function str_wrap() wraps text so that it fits within a certain width. This is useful for wrapping long strings of text to be typeset. Exercise 14.2.5 What does str_trim() do? What’s the opposite of str_trim()? The function str_trim() trims the whitespace from a string. str_trim(&quot; abc &quot;) #&gt; [1] &quot;abc&quot; str_trim(&quot; abc &quot;, side = &quot;left&quot;) #&gt; [1] &quot;abc &quot; str_trim(&quot; abc &quot;, side = &quot;right&quot;) #&gt; [1] &quot; abc&quot; The opposite of str_trim() is str_pad() which adds characters to each side. str_pad(&quot;abc&quot;, 5, side = &quot;both&quot;) #&gt; [1] &quot; abc &quot; str_pad(&quot;abc&quot;, 4, side = &quot;right&quot;) #&gt; [1] &quot;abc &quot; str_pad(&quot;abc&quot;, 4, side = &quot;left&quot;) #&gt; [1] &quot; abc&quot; Exercise 14.2.6 Write a function that turns (e.g.) a vector c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) into the string &quot;a, b, and c&quot;. Think carefully about what it should do if given a vector of length 0, 1, or 2. See the Chapter [Functions] for more details on writing R functions. This function needs to handle four cases. n == 0: an empty string, e.g. &quot;&quot;. n == 1: the original vector, e.g. &quot;a&quot;. n == 2: return the two elements separated by “and”, e.g. &quot;a and b&quot;. n &gt; 2: return the first n - 1 elements separated by commas, and the last element separated by a comma and “and”, e.g. &quot;a, b, and c&quot;. str_commasep &lt;- function(x, delim = &quot;,&quot;) { n &lt;- length(x) if (n == 0) { &quot;&quot; } else if (n == 1) { x } else if (n == 2) { # no comma before and when n == 2 str_c(x[[1]], &quot;and&quot;, x[[2]], sep = &quot; &quot;) } else { # commas after all n - 1 elements not_last &lt;- str_c(x[seq_len(n - 1)], delim) # prepend &quot;and&quot; to the last element last &lt;- str_c(&quot;and&quot;, x[[n]], sep = &quot; &quot;) # combine parts with spaces str_c(c(not_last, last), collapse = &quot; &quot;) } } str_commasep(&quot;&quot;) #&gt; [1] &quot;&quot; str_commasep(&quot;a&quot;) #&gt; [1] &quot;a&quot; str_commasep(c(&quot;a&quot;, &quot;b&quot;)) #&gt; [1] &quot;a and b&quot; str_commasep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) #&gt; [1] &quot;a, b, and c&quot; str_commasep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) #&gt; [1] &quot;a, b, c, and d&quot; 14.3 Matching patterns with regular expressions 14.3.1 Basic matches Exercise 14.3.1.1 Explain why each of these strings don’t match a \\: &quot;\\&quot;, &quot;\\\\&quot;, &quot;\\\\\\&quot;. &quot;\\&quot;: This will escape the next character in the R string. &quot;\\\\&quot;: This will resolve to \\ in the regular expression, which will escape the next character in the regular expression. &quot;\\\\\\&quot;: The first two backslashes will resolve to a literal backslash in the regular expression, the third will escape the next character. So in the regular expression, this will escape some escaped character. Exercise 14.3.1.2 How would you match the sequence &quot;'\\ ? str_view(&quot;\\&quot;&#39;\\\\&quot;, &quot;\\&quot;&#39;\\\\\\\\&quot;, match = TRUE) Exercise 14.3.1.3 What patterns will the regular expression \\..\\..\\.. match? How would you represent it as a string? It will match any patterns that are a dot followed by any character, repeated three times. str_view(c(&quot;.a.b.c&quot;, &quot;.a.b&quot;, &quot;.....&quot;), c(&quot;\\\\..\\\\..\\\\..&quot;), match = TRUE) 14.3.2 Anchors Exercise 14.3.2.1 How would you match the literal string &quot;$^$&quot;? To check that the pattern works, I’ll include both the string &quot;$^$&quot;, and an example where that pattern occurs in the middle of the string which should not be matched. str_view(c(&quot;$^$&quot;, &quot;ab$^$sfas&quot;), &quot;^\\\\$\\\\^\\\\$$&quot;, match = TRUE) Exercise 14.3.2.2 Given the corpus of common words in stringr::words, create regular expressions that find all words that: Start with “y”. End with “x” Are exactly three letters long. (Don’t cheat by using str_length()!) Have seven letters or more. Since this list is long, you might want to use the match argument to str_view() to show only the matching or non-matching words. The answer to each part follows. The words that start with “y” are: str_view(stringr::words, &quot;^y&quot;, match = TRUE) End with “x” str_view(stringr::words, &quot;x$&quot;, match = TRUE) Are exactly three letters long are str_view(stringr::words, &quot;^...$&quot;, match = TRUE) The words that have seven letters or more: str_view(stringr::words, &quot;.......&quot;, match = TRUE) Since the pattern ....... is not anchored with either . or $ this will match any word with at last seven letters. The pattern, ^.......$, matches words with exactly seven characters. 14.3.3 Character classes and alternatives Exercise 14.3.3.1 Create regular expressions to find all words that: Start with a vowel. That only contain consonants. (Hint: thinking about matching “not”-vowels.) End with ed, but not with eed. End with ing or ise. The answer to each part follows. Words starting with vowels str_subset(stringr::words, &quot;^[aeiou]&quot;) #&gt; [1] &quot;a&quot; &quot;able&quot; &quot;about&quot; &quot;absolute&quot; &quot;accept&quot; #&gt; [6] &quot;account&quot; &quot;achieve&quot; &quot;across&quot; &quot;act&quot; &quot;active&quot; #&gt; [11] &quot;actual&quot; &quot;add&quot; &quot;address&quot; &quot;admit&quot; &quot;advertise&quot; #&gt; [16] &quot;affect&quot; &quot;afford&quot; &quot;after&quot; &quot;afternoon&quot; &quot;again&quot; #&gt; [21] &quot;against&quot; &quot;age&quot; &quot;agent&quot; &quot;ago&quot; &quot;agree&quot; #&gt; [26] &quot;air&quot; &quot;all&quot; &quot;allow&quot; &quot;almost&quot; &quot;along&quot; #&gt; [31] &quot;already&quot; &quot;alright&quot; &quot;also&quot; &quot;although&quot; &quot;always&quot; #&gt; [36] &quot;america&quot; &quot;amount&quot; &quot;and&quot; &quot;another&quot; &quot;answer&quot; #&gt; [41] &quot;any&quot; &quot;apart&quot; &quot;apparent&quot; &quot;appear&quot; &quot;apply&quot; #&gt; [46] &quot;appoint&quot; &quot;approach&quot; &quot;appropriate&quot; &quot;area&quot; &quot;argue&quot; #&gt; [51] &quot;arm&quot; &quot;around&quot; &quot;arrange&quot; &quot;art&quot; &quot;as&quot; #&gt; [56] &quot;ask&quot; &quot;associate&quot; &quot;assume&quot; &quot;at&quot; &quot;attend&quot; #&gt; [61] &quot;authority&quot; &quot;available&quot; &quot;aware&quot; &quot;away&quot; &quot;awful&quot; #&gt; [66] &quot;each&quot; &quot;early&quot; &quot;east&quot; &quot;easy&quot; &quot;eat&quot; #&gt; [71] &quot;economy&quot; &quot;educate&quot; &quot;effect&quot; &quot;egg&quot; &quot;eight&quot; #&gt; [76] &quot;either&quot; &quot;elect&quot; &quot;electric&quot; &quot;eleven&quot; &quot;else&quot; #&gt; [81] &quot;employ&quot; &quot;encourage&quot; &quot;end&quot; &quot;engine&quot; &quot;english&quot; #&gt; [86] &quot;enjoy&quot; &quot;enough&quot; &quot;enter&quot; &quot;environment&quot; &quot;equal&quot; #&gt; [91] &quot;especial&quot; &quot;europe&quot; &quot;even&quot; &quot;evening&quot; &quot;ever&quot; #&gt; [96] &quot;every&quot; &quot;evidence&quot; &quot;exact&quot; &quot;example&quot; &quot;except&quot; #&gt; [101] &quot;excuse&quot; &quot;exercise&quot; &quot;exist&quot; &quot;expect&quot; &quot;expense&quot; #&gt; [106] &quot;experience&quot; &quot;explain&quot; &quot;express&quot; &quot;extra&quot; &quot;eye&quot; #&gt; [111] &quot;idea&quot; &quot;identify&quot; &quot;if&quot; &quot;imagine&quot; &quot;important&quot; #&gt; [116] &quot;improve&quot; &quot;in&quot; &quot;include&quot; &quot;income&quot; &quot;increase&quot; #&gt; [121] &quot;indeed&quot; &quot;individual&quot; &quot;industry&quot; &quot;inform&quot; &quot;inside&quot; #&gt; [126] &quot;instead&quot; &quot;insure&quot; &quot;interest&quot; &quot;into&quot; &quot;introduce&quot; #&gt; [131] &quot;invest&quot; &quot;involve&quot; &quot;issue&quot; &quot;it&quot; &quot;item&quot; #&gt; [136] &quot;obvious&quot; &quot;occasion&quot; &quot;odd&quot; &quot;of&quot; &quot;off&quot; #&gt; [141] &quot;offer&quot; &quot;office&quot; &quot;often&quot; &quot;okay&quot; &quot;old&quot; #&gt; [146] &quot;on&quot; &quot;once&quot; &quot;one&quot; &quot;only&quot; &quot;open&quot; #&gt; [151] &quot;operate&quot; &quot;opportunity&quot; &quot;oppose&quot; &quot;or&quot; &quot;order&quot; #&gt; [156] &quot;organize&quot; &quot;original&quot; &quot;other&quot; &quot;otherwise&quot; &quot;ought&quot; #&gt; [161] &quot;out&quot; &quot;over&quot; &quot;own&quot; &quot;under&quot; &quot;understand&quot; #&gt; [166] &quot;union&quot; &quot;unit&quot; &quot;unite&quot; &quot;university&quot; &quot;unless&quot; #&gt; [171] &quot;until&quot; &quot;up&quot; &quot;upon&quot; &quot;use&quot; &quot;usual&quot; Words that contain only consonants: Use the negate argument of str_subset. str_subset(stringr::words, &quot;[aeiou]&quot;, negate=TRUE) #&gt; [1] &quot;by&quot; &quot;dry&quot; &quot;fly&quot; &quot;mrs&quot; &quot;try&quot; &quot;why&quot; Alternatively, using str_view() the consonant-only words are: str_view(stringr::words, &quot;[aeiou]&quot;, match=FALSE) Words that end with “-ed” but not ending in “-eed”. str_subset(stringr::words, &quot;[^e]ed$&quot;) #&gt; [1] &quot;bed&quot; &quot;hundred&quot; &quot;red&quot; The pattern above will not match the word &quot;ed&quot;. If we wanted to include that, we could include it as a special case. str_subset(c(&quot;ed&quot;, stringr::words), &quot;(^|[^e])ed$&quot;) #&gt; [1] &quot;ed&quot; &quot;bed&quot; &quot;hundred&quot; &quot;red&quot; Words ending in ing or ise: str_subset(stringr::words, &quot;i(ng|se)$&quot;) #&gt; [1] &quot;advertise&quot; &quot;bring&quot; &quot;during&quot; &quot;evening&quot; &quot;exercise&quot; &quot;king&quot; #&gt; [7] &quot;meaning&quot; &quot;morning&quot; &quot;otherwise&quot; &quot;practise&quot; &quot;raise&quot; &quot;realise&quot; #&gt; [13] &quot;ring&quot; &quot;rise&quot; &quot;sing&quot; &quot;surprise&quot; &quot;thing&quot; Exercise 14.3.3.2 Empirically verify the rule “i” before e except after “c”. length(str_subset(stringr::words, &quot;(cei|[^c]ie)&quot;)) #&gt; [1] 14 length(str_subset(stringr::words, &quot;(cie|[^c]ei)&quot;)) #&gt; [1] 3 Exercise 14.3.3.3 Is “q” always followed by a “u”? In the stringr::words dataset, yes. str_view(stringr::words, &quot;q[^u]&quot;, match = TRUE) In the English language— no. However, the examples are few, and mostly loanwords, such as “burqa” and “cinq”. Also, “qwerty”. That I had to add all of those examples to the list of words that spellchecking should ignore is indicative of their rarity. Exercise 14.3.3.4 Write a regular expression that matches a word if it’s probably written in British English, not American English. In the general case, this is hard, and could require a dictionary. But, there are a few heuristics to consider that would account for some common cases: British English tends to use the following: “ou” instead of “o” use of “ae” and “oe” instead of “a” and “o” ends in ise instead of ize ends in yse The regex ou|ise$|ae|oe|yse$ would match these. There are other spelling differences between American and British English but they are not patterns amenable to regular expressions. It would require a dictionary with differences in spellings for different words. Exercise 14.3.3.5 Create a regular expression that will match telephone numbers as commonly written in your country. &lt;div class=&quot;alert alert-primary hints-alert&gt; This answer can be improved and expanded. The answer to this will vary by country. For the United States, phone numbers have a format like 123-456-7890 or (123)456-7890). These regular expressions will parse the first form x &lt;- c(&quot;123-456-7890&quot;, &quot;(123)456-7890&quot;, &quot;(123) 456-7890&quot;, &quot;1235-2351&quot;) str_view(x, &quot;\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d\\\\d&quot;) str_view(x, &quot;[0-9][0-9][0-9]-[0-9][0-9][0-9]-[0-9][0-9][0-9][0-9]&quot;) The regular expressions will parse the second form: str_view(x, &quot;\\\\(\\\\d\\\\d\\\\d\\\\)\\\\s*\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d\\\\d&quot;) str_view(x, &quot;\\\\([0-9][0-9][0-9]\\\\)[ ]*[0-9][0-9][0-9]-[0-9][0-9][0-9][0-9]&quot;) This regular expression can be simplified with the {m,n} regular expression modifier introduced in the next section, str_view(x, &quot;\\\\d{3}-\\\\d{3}-\\\\d{4}&quot;) str_view(x, &quot;\\\\(\\\\d{3}\\\\)\\\\s*\\\\d{3}-\\\\d{4}&quot;) Note that this pattern doesn’t account for phone numbers that are invalid due to an invalid area code. Nor does this pattern account for special numbers like 911. It also doesn’t parse a leading country code or an extensions. See the Wikipedia page for the North American Numbering Plan for more information on the complexities of US phone numbers, and this Stack Overflow question for a discussion of using a regex for phone number validation. The R package dialr implements robust phone number parsing. Generally, for patterns like phone numbers or URLs it is better to use a dedicated package. It is easy to match the pattern for the most common cases and useful for learning regular expressions, but in real applications there often edge cases that are handled by dedicated packages. 14.3.4 Repetition Exercise 14.3.4.1 Describe the equivalents of ?, +, * in {m,n} form. Pattern {m,n} Meaning ? {0,1} Match at most 1 + {1,} Match 1 or more * {0,} Match 0 or more For example, let’s repeat the examples in the chapter, replacing ? with {0,1}, + with {1,}, and * with {*,}. x &lt;- &quot;1888 is the longest year in Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &quot;CC?&quot;) str_view(x, &quot;CC{0,1}&quot;) str_view(x, &quot;CC+&quot;) str_view(x, &quot;CC{1,}&quot;) str_view_all(x, &quot;C[LX]+&quot;) str_view_all(x, &quot;C[LX]{1,}&quot;) The chapter does not contain an example of *. This pattern looks for a “C” optionally followed by any number of “L” or “X” characters. str_view_all(x, &quot;C[LX]*&quot;) str_view_all(x, &quot;C[LX]{0,}&quot;) Exercise 14.3.4.2 Describe in words what these regular expressions match: (read carefully to see if I’m using a regular expression or a string that defines a regular expression.) ^.*$ &quot;\\\\{.+\\\\}&quot; \\d{4}-\\d{2}-\\d{2} &quot;\\\\\\\\{4}&quot; The answer to each part follows. ^.*$ will match any string. For example: ^.*$: c(&quot;dog&quot;, &quot;$1.23&quot;, &quot;lorem ipsum&quot;). &quot;\\\\{.+\\\\}&quot; will match any string with curly braces surrounding at least one character. For example: &quot;\\\\{.+\\\\}&quot;: c(&quot;{a}&quot;, &quot;{abc}&quot;). \\d{4}-\\d{2}-\\d{2} will match four digits followed by a hyphen, followed by two digits followed by a hyphen, followed by another two digits. This is a regular expression that can match dates formatted like “YYYY-MM-DD” (“%Y-%m-%d”). For example: \\d{4}-\\d{2}-\\d{2}: 2018-01-11 &quot;\\\\\\\\{4}&quot; is \\\\{4}, which will match four backslashes. For example: &quot;\\\\\\\\{4}&quot;: &quot;\\\\\\\\\\\\\\\\&quot;. Exercise 14.3.4.3 Create regular expressions to find all words that: Start with three consonants. Have three or more vowels in a row. Have two or more vowel-consonant pairs in a row. The answer to each part follows. This regex finds all words starting with three consonants. str_view(words, &quot;^[^aeiou]{3}&quot;, match = TRUE) This regex finds three or more vowels in a row: str_view(words, &quot;[aeiou]{3,}&quot;, match = TRUE) This regex finds two or more vowel-consonant pairs in a row. str_view(words, &quot;([aeiou][^aeiou]){2,}&quot;, match = TRUE) Exercise 14.3.4.4 Solve the beginner regexp crosswords at https://regexcrossword.com/challenges/ Exercise left to reader. That site validates its solutions, so they aren’t repeated here. 14.3.5 Grouping and backreferences Exercise 14.3.5.1 Describe, in words, what these expressions will match: (.)\\1\\1 : &quot;(.)(.)\\\\2\\\\1&quot;: (..)\\1: &quot;(.).\\\\1.\\\\1&quot;: &quot;(.)(.)(.).*\\\\3\\\\2\\\\1&quot; The answer to each part follows. (.)\\1\\1: The same character appearing three times in a row. E.g. &quot;aaa&quot; &quot;(.)(.)\\\\2\\\\1&quot;: A pair of characters followed by the same pair of characters in reversed order. E.g. &quot;abba&quot;. (..)\\1: Any two characters repeated. E.g. &quot;a1a1&quot;. &quot;(.).\\\\1.\\\\1&quot;: A character followed by any character, the original character, any other character, the original character again. E.g. &quot;abaca&quot;, &quot;b8b.b&quot;. &quot;(.)(.)(.).*\\\\3\\\\2\\\\1&quot; Three characters followed by zero or more characters of any kind followed by the same three characters but in reverse order. E.g. &quot;abcsgasgddsadgsdgcba&quot; or &quot;abccba&quot; or &quot;abc1cba&quot;. Exercise 14.3.5.2 Construct regular expressions to match words that: Start and end with the same character. Contain a repeated pair of letters (e.g. church'' containsch’’ repeated twice.) Contain one letter repeated in at least three places (e.g. eleven'' contains threee’’s.) The answer to each part follows. This regular expression matches words that start and end with the same character. str_subset(words, &quot;^(.)((.*\\\\1$)|\\\\1?$)&quot;) #&gt; [1] &quot;a&quot; &quot;america&quot; &quot;area&quot; &quot;dad&quot; &quot;dead&quot; #&gt; [6] &quot;depend&quot; &quot;educate&quot; &quot;else&quot; &quot;encourage&quot; &quot;engine&quot; #&gt; [11] &quot;europe&quot; &quot;evidence&quot; &quot;example&quot; &quot;excuse&quot; &quot;exercise&quot; #&gt; [16] &quot;expense&quot; &quot;experience&quot; &quot;eye&quot; &quot;health&quot; &quot;high&quot; #&gt; [21] &quot;knock&quot; &quot;level&quot; &quot;local&quot; &quot;nation&quot; &quot;non&quot; #&gt; [26] &quot;rather&quot; &quot;refer&quot; &quot;remember&quot; &quot;serious&quot; &quot;stairs&quot; #&gt; [31] &quot;test&quot; &quot;tonight&quot; &quot;transport&quot; &quot;treat&quot; &quot;trust&quot; #&gt; [36] &quot;window&quot; &quot;yesterday&quot; This regular expression will match any pair of repeated letters, where letters is defined to be the ASCII letters A-Z. First, check that it works with the example in the problem. str_subset(&quot;church&quot;, &quot;([A-Za-z][A-Za-z]).*\\\\1&quot;) #&gt; [1] &quot;church&quot; Now, find all matching words in words. str_subset(words, &quot;([A-Za-z][A-Za-z]).*\\\\1&quot;) #&gt; [1] &quot;appropriate&quot; &quot;church&quot; &quot;condition&quot; &quot;decide&quot; &quot;environment&quot; #&gt; [6] &quot;london&quot; &quot;paragraph&quot; &quot;particular&quot; &quot;photograph&quot; &quot;prepare&quot; #&gt; [11] &quot;pressure&quot; &quot;remember&quot; &quot;represent&quot; &quot;require&quot; &quot;sense&quot; #&gt; [16] &quot;therefore&quot; &quot;understand&quot; &quot;whether&quot; The \\\\1 pattern is called a backreference. It matches whatever the first group matched. This allows the pattern to match a repeating pair of letters without having to specify exactly what pair letters is being repeated. Note that these patterns are case sensitive. Use the case insensitive flag if you want to check for repeated pairs of letters with different capitalization. This regex matches words that contain one letter repeated in at least three places. First, check that it works with th example given in the question. str_subset(&quot;eleven&quot;, &quot;([a-z]).*\\\\1.*\\\\1&quot;) #&gt; [1] &quot;eleven&quot; Now, retrieve the matching words in words. str_subset(words, &quot;([a-z]).*\\\\1.*\\\\1&quot;) #&gt; [1] &quot;appropriate&quot; &quot;available&quot; &quot;believe&quot; &quot;between&quot; &quot;business&quot; #&gt; [6] &quot;degree&quot; &quot;difference&quot; &quot;discuss&quot; &quot;eleven&quot; &quot;environment&quot; #&gt; [11] &quot;evidence&quot; &quot;exercise&quot; &quot;expense&quot; &quot;experience&quot; &quot;individual&quot; #&gt; [16] &quot;paragraph&quot; &quot;receive&quot; &quot;remember&quot; &quot;represent&quot; &quot;telephone&quot; #&gt; [21] &quot;therefore&quot; &quot;tomorrow&quot; 14.4 Tools 14.4.1 Detect matches Exercise 14.4.1.1 For each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls. Find all words that start or end with x. Find all words that start with a vowel and end with a consonant. Are there any words that contain at least one of each different vowel? The answer to each part follows. Words that start or end with x? # one regex words[str_detect(words, &quot;^x|x$&quot;)] #&gt; [1] &quot;box&quot; &quot;sex&quot; &quot;six&quot; &quot;tax&quot; # split regex into parts start_with_x &lt;- str_detect(words, &quot;^x&quot;) end_with_x &lt;- str_detect(words, &quot;x$&quot;) words[start_with_x | end_with_x] #&gt; [1] &quot;box&quot; &quot;sex&quot; &quot;six&quot; &quot;tax&quot; Words starting with vowel and ending with consonant. str_subset(words, &quot;^[aeiou].*[^aeiou]$&quot;) %&gt;% head() #&gt; [1] &quot;about&quot; &quot;accept&quot; &quot;account&quot; &quot;across&quot; &quot;act&quot; &quot;actual&quot; start_with_vowel &lt;- str_detect(words, &quot;^[aeiou]&quot;) end_with_consonant &lt;- str_detect(words, &quot;[^aeiou]$&quot;) words[start_with_vowel &amp; end_with_consonant] %&gt;% head() #&gt; [1] &quot;about&quot; &quot;accept&quot; &quot;account&quot; &quot;across&quot; &quot;act&quot; &quot;actual&quot; There is not a simple regular expression to match words that that contain at least one of each vowel. The regular expression would need to consider all possible orders in which the vowels could occur. pattern &lt;- cross(rerun(5, c(&quot;a&quot;, &quot;e&quot;, &quot;i&quot;, &quot;o&quot;, &quot;u&quot;)), .filter = function(...) { x &lt;- as.character(unlist(list(...))) length(x) != length(unique(x)) } ) %&gt;% map_chr(~str_c(unlist(.x), collapse = &quot;.*&quot;)) %&gt;% str_c(collapse = &quot;|&quot;) To check that this pattern works, test it on a pattern that should match str_subset(&quot;aseiouds&quot;, pattern) #&gt; [1] &quot;aseiouds&quot; Using multiple str_detect() calls, one pattern for each vowel, produces a much simpler and readable answer. str_subset(words, pattern) #&gt; character(0) words[str_detect(words, &quot;a&quot;) &amp; str_detect(words, &quot;e&quot;) &amp; str_detect(words, &quot;i&quot;) &amp; str_detect(words, &quot;o&quot;) &amp; str_detect(words, &quot;u&quot;)] #&gt; character(0) There appear to be none. Exercise 14.4.1.2 What word has the higher number of vowels? What word has the highest proportion of vowels? (Hint: what is the denominator?) The word with the highest number of vowels is vowels &lt;- str_count(words, &quot;[aeiou]&quot;) words[which(vowels == max(vowels))] #&gt; [1] &quot;appropriate&quot; &quot;associate&quot; &quot;available&quot; &quot;colleague&quot; &quot;encourage&quot; #&gt; [6] &quot;experience&quot; &quot;individual&quot; &quot;television&quot; The word with the highest proportion of vowels is prop_vowels &lt;- str_count(words, &quot;[aeiou]&quot;) / str_length(words) words[which(prop_vowels == max(prop_vowels))] #&gt; [1] &quot;a&quot; 14.4.2 Extract matches Exercise 14.4.2.1 In the previous example, you might have noticed that the regular expression matched “flickered”, which is not a color. Modify the regex to fix the problem. This was the original color match pattern: colours &lt;- c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) colour_match &lt;- str_c(colours, collapse = &quot;|&quot;) It matches “flickered” because it matches “red”. The problem is that the previous pattern will match any word with the name of a color inside it. We want to only match colors in which the entire word is the name of the color. We can do this by adding a \\b (to indicate a word boundary) before and after the pattern: colour_match2 &lt;- str_c(&quot;\\\\b(&quot;, str_c(colours, collapse = &quot;|&quot;), &quot;)\\\\b&quot;) colour_match2 #&gt; [1] &quot;\\\\b(red|orange|yellow|green|blue|purple)\\\\b&quot; more2 &lt;- sentences[str_count(sentences, colour_match) &gt; 1] str_view_all(more2, colour_match2, match = TRUE) Exercise 14.4.2.2 From the Harvard sentences data, extract: The first word from each sentence. All words ending in ing. All plurals. The answer to each part follows. Finding the first word in each sentence requires defining what a pattern constitutes a word. For the purposes of this question, I’ll consider a word any contiguous set of letters. Since str_extract() will extract the first match, if it is provided a regular expression for words, it will return the first word. str_extract(sentences, &quot;[A-ZAa-z]+&quot;) %&gt;% head() #&gt; [1] &quot;The&quot; &quot;Glue&quot; &quot;It&quot; &quot;These&quot; &quot;Rice&quot; &quot;The&quot; However, the third sentence begins with “It’s”. To catch this, I’ll change the regular expression to require the string to begin with a letter, but allow for a subsequent apostrophe. str_extract(sentences, &quot;[A-Za-z][A-Za-z&#39;]*&quot;) %&gt;% head() #&gt; [1] &quot;The&quot; &quot;Glue&quot; &quot;It&#39;s&quot; &quot;These&quot; &quot;Rice&quot; &quot;The&quot; This pattern finds all words ending in ing. pattern &lt;- &quot;\\\\b[A-Za-z]+ing\\\\b&quot; sentences_with_ing &lt;- str_detect(sentences, pattern) unique(unlist(str_extract_all(sentences[sentences_with_ing], pattern))) %&gt;% head() #&gt; [1] &quot;spring&quot; &quot;evening&quot; &quot;morning&quot; &quot;winding&quot; &quot;living&quot; &quot;king&quot; Finding all plurals cannot be correctly accomplished with regular expressions alone. Finding plural words would at least require morphological information about words in the language. See WordNet for a resource that would do that. However, identifying words that end in an “s” and with more than three characters, in order to remove “as”, “is”, “gas”, etc., is a reasonable heuristic. unique(unlist(str_extract_all(sentences, &quot;\\\\b[A-Za-z]{3,}s\\\\b&quot;))) %&gt;% head() #&gt; [1] &quot;planks&quot; &quot;days&quot; &quot;bowls&quot; &quot;lemons&quot; &quot;makes&quot; &quot;hogs&quot; 14.4.3 Grouped matches Exercise 14.4.3.1 Find all words that come after a “number” like “one”, “two”, “three” etc. Pull out both the number and the word. numword &lt;- &quot;\\\\b(one|two|three|four|five|six|seven|eight|nine|ten) +(\\\\w+)&quot; sentences[str_detect(sentences, numword)] %&gt;% str_extract(numword) #&gt; [1] &quot;seven books&quot; &quot;two met&quot; &quot;two factors&quot; &quot;three lists&quot; #&gt; [5] &quot;seven is&quot; &quot;two when&quot; &quot;ten inches&quot; &quot;one war&quot; #&gt; [9] &quot;one button&quot; &quot;six minutes&quot; &quot;ten years&quot; &quot;two shares&quot; #&gt; [13] &quot;two distinct&quot; &quot;five cents&quot; &quot;two pins&quot; &quot;five robins&quot; #&gt; [17] &quot;four kinds&quot; &quot;three story&quot; &quot;three inches&quot; &quot;six comes&quot; #&gt; [21] &quot;three batches&quot; &quot;two leaves&quot; Exercise 14.4.3.2 Find all contractions. Separate out the pieces before and after the apostrophe. This is done in two steps. First, identify the contractions. Second, split the string on the contraction. contraction &lt;- &quot;([A-Za-z]+)&#39;([A-Za-z]+)&quot; sentences[str_detect(sentences, contraction)] %&gt;% str_extract(contraction) %&gt;% str_split(&quot;&#39;&quot;) #&gt; [[1]] #&gt; [1] &quot;It&quot; &quot;s&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;man&quot; &quot;s&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;don&quot; &quot;t&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;store&quot; &quot;s&quot; #&gt; #&gt; [[5]] #&gt; [1] &quot;workmen&quot; &quot;s&quot; #&gt; #&gt; [[6]] #&gt; [1] &quot;Let&quot; &quot;s&quot; #&gt; #&gt; [[7]] #&gt; [1] &quot;sun&quot; &quot;s&quot; #&gt; #&gt; [[8]] #&gt; [1] &quot;child&quot; &quot;s&quot; #&gt; #&gt; [[9]] #&gt; [1] &quot;king&quot; &quot;s&quot; #&gt; #&gt; [[10]] #&gt; [1] &quot;It&quot; &quot;s&quot; #&gt; #&gt; [[11]] #&gt; [1] &quot;don&quot; &quot;t&quot; #&gt; #&gt; [[12]] #&gt; [1] &quot;queen&quot; &quot;s&quot; #&gt; #&gt; [[13]] #&gt; [1] &quot;don&quot; &quot;t&quot; #&gt; #&gt; [[14]] #&gt; [1] &quot;pirate&quot; &quot;s&quot; #&gt; #&gt; [[15]] #&gt; [1] &quot;neighbor&quot; &quot;s&quot; 14.4.4 Replacing matches Exercise 14.4.4.1 Replace all forward slashes in a string with backslashes. str_replace_all(&quot;past/present/future&quot;, &quot;/&quot;, &quot;\\\\\\\\&quot;) #&gt; [1] &quot;past\\\\present\\\\future&quot; Exercise 14.4.4.2 Implement a simple version of str_to_lower() using replace_all(). replacements &lt;- c(&quot;A&quot; = &quot;a&quot;, &quot;B&quot; = &quot;b&quot;, &quot;C&quot; = &quot;c&quot;, &quot;D&quot; = &quot;d&quot;, &quot;E&quot; = &quot;e&quot;, &quot;F&quot; = &quot;f&quot;, &quot;G&quot; = &quot;g&quot;, &quot;H&quot; = &quot;h&quot;, &quot;I&quot; = &quot;i&quot;, &quot;J&quot; = &quot;j&quot;, &quot;K&quot; = &quot;k&quot;, &quot;L&quot; = &quot;l&quot;, &quot;M&quot; = &quot;m&quot;, &quot;N&quot; = &quot;n&quot;, &quot;O&quot; = &quot;o&quot;, &quot;P&quot; = &quot;p&quot;, &quot;Q&quot; = &quot;q&quot;, &quot;R&quot; = &quot;r&quot;, &quot;S&quot; = &quot;s&quot;, &quot;T&quot; = &quot;t&quot;, &quot;U&quot; = &quot;u&quot;, &quot;V&quot; = &quot;v&quot;, &quot;W&quot; = &quot;w&quot;, &quot;X&quot; = &quot;x&quot;, &quot;Y&quot; = &quot;y&quot;, &quot;Z&quot; = &quot;z&quot;) lower_words &lt;- str_replace_all(words, pattern = replacements) head(lower_words) #&gt; [1] &quot;a&quot; &quot;able&quot; &quot;about&quot; &quot;absolute&quot; &quot;accept&quot; &quot;account&quot; Exercise 14.4.4.3 Switch the first and last letters in words. Which of those strings are still words? First, make a vector of all the words with first and last letters swapped, swapped &lt;- str_replace_all(words, &quot;^([A-Za-z])(.*)([A-Za-z])$&quot;, &quot;\\\\3\\\\2\\\\1&quot;) Next, find what of “swapped” is also in the original list using the function intersect(), intersect(swapped, words) #&gt; [1] &quot;a&quot; &quot;america&quot; &quot;area&quot; &quot;dad&quot; &quot;dead&quot; #&gt; [6] &quot;lead&quot; &quot;read&quot; &quot;depend&quot; &quot;god&quot; &quot;educate&quot; #&gt; [11] &quot;else&quot; &quot;encourage&quot; &quot;engine&quot; &quot;europe&quot; &quot;evidence&quot; #&gt; [16] &quot;example&quot; &quot;excuse&quot; &quot;exercise&quot; &quot;expense&quot; &quot;experience&quot; #&gt; [21] &quot;eye&quot; &quot;dog&quot; &quot;health&quot; &quot;high&quot; &quot;knock&quot; #&gt; [26] &quot;deal&quot; &quot;level&quot; &quot;local&quot; &quot;nation&quot; &quot;on&quot; #&gt; [31] &quot;non&quot; &quot;no&quot; &quot;rather&quot; &quot;dear&quot; &quot;refer&quot; #&gt; [36] &quot;remember&quot; &quot;serious&quot; &quot;stairs&quot; &quot;test&quot; &quot;tonight&quot; #&gt; [41] &quot;transport&quot; &quot;treat&quot; &quot;trust&quot; &quot;window&quot; &quot;yesterday&quot; Alternatively, the regex can be written using the POSIX character class for letter ([[:alpha:]]): swapped2 &lt;- str_replace_all(words, &quot;^([[:alpha:]])(.*)([[:alpha:]])$&quot;, &quot;\\\\3\\\\2\\\\1&quot;) intersect(swapped2, words) #&gt; [1] &quot;a&quot; &quot;america&quot; &quot;area&quot; &quot;dad&quot; &quot;dead&quot; #&gt; [6] &quot;lead&quot; &quot;read&quot; &quot;depend&quot; &quot;god&quot; &quot;educate&quot; #&gt; [11] &quot;else&quot; &quot;encourage&quot; &quot;engine&quot; &quot;europe&quot; &quot;evidence&quot; #&gt; [16] &quot;example&quot; &quot;excuse&quot; &quot;exercise&quot; &quot;expense&quot; &quot;experience&quot; #&gt; [21] &quot;eye&quot; &quot;dog&quot; &quot;health&quot; &quot;high&quot; &quot;knock&quot; #&gt; [26] &quot;deal&quot; &quot;level&quot; &quot;local&quot; &quot;nation&quot; &quot;on&quot; #&gt; [31] &quot;non&quot; &quot;no&quot; &quot;rather&quot; &quot;dear&quot; &quot;refer&quot; #&gt; [36] &quot;remember&quot; &quot;serious&quot; &quot;stairs&quot; &quot;test&quot; &quot;tonight&quot; #&gt; [41] &quot;transport&quot; &quot;treat&quot; &quot;trust&quot; &quot;window&quot; &quot;yesterday&quot; 14.4.5 Splitting Exercise 14.4.5.1 Split up a string like &quot;apples, pears, and bananas&quot; into individual components. x &lt;- c(&quot;apples, pears, and bananas&quot;) str_split(x, &quot;, +(and +)?&quot;)[[1]] #&gt; [1] &quot;apples&quot; &quot;pears&quot; &quot;bananas&quot; Exercise 14.4.5.2 Why is it better to split up by boundary(&quot;word&quot;) than &quot; &quot;? Splitting by boundary(&quot;word&quot;) is a more sophisticated method to split a string into words. It recognizes non-space punctuation that splits words, and also removes punctuation while retaining internal non-letter characters that are parts of the word, e.g., “can’t” See the ICU website for a description of the set of rules that are used to determine word boundaries. Consider this sentence from the official Unicode Report on word boundaries, sentence &lt;- &quot;The quick (“brown”) fox can’t jump 32.3 feet, right?&quot; Splitting the string on spaces considers will group the punctuation with the words, str_split(sentence, &quot; &quot;) #&gt; [[1]] #&gt; [1] &quot;The&quot; &quot;quick&quot; &quot;(“brown”)&quot; &quot;fox&quot; &quot;can’t&quot; &quot;jump&quot; #&gt; [7] &quot;32.3&quot; &quot;feet,&quot; &quot;right?&quot; However, splitting the string using boundary(&quot;word&quot;) correctly removes punctuation, while not separating “32.2” and “can’t”, str_split(sentence, boundary(&quot;word&quot;)) #&gt; [[1]] #&gt; [1] &quot;The&quot; &quot;quick&quot; &quot;brown&quot; &quot;fox&quot; &quot;can’t&quot; &quot;jump&quot; &quot;32.3&quot; &quot;feet&quot; &quot;right&quot; Exercise 14.4.5.3 What does splitting with an empty string (&quot;&quot;) do? Experiment, and then read the documentation. str_split(&quot;ab. cd|agt&quot;, &quot;&quot;)[[1]] #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;.&quot; &quot; &quot; &quot;c&quot; &quot;d&quot; &quot;|&quot; &quot;a&quot; &quot;g&quot; &quot;t&quot; It splits the string into individual characters. 14.4.6 Find matches No exercises 14.5 Other types of pattern Exercise 14.5.1 How would you find all strings containing \\ with regex() vs. with fixed()? str_subset(c(&quot;a\\\\b&quot;, &quot;ab&quot;), &quot;\\\\\\\\&quot;) #&gt; [1] &quot;a\\\\b&quot; str_subset(c(&quot;a\\\\b&quot;, &quot;ab&quot;), fixed(&quot;\\\\&quot;)) #&gt; [1] &quot;a\\\\b&quot; Exercise 14.5.2 What are the five most common words in sentences? Using str_extract_all() with the argument boundary(&quot;word&quot;) will extract all words. The rest of the code uses dplyr functions to count words and find the most common words. tibble(word = unlist(str_extract_all(sentences, boundary(&quot;word&quot;)))) %&gt;% mutate(word = str_to_lower(word)) %&gt;% count(word, sort = TRUE) %&gt;% head(5) #&gt; # A tibble: 5 x 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 the 751 #&gt; 2 a 202 #&gt; 3 of 132 #&gt; 4 to 123 #&gt; 5 and 118 14.6 Other uses of regular expressions No exercises 14.7 stringi library(&quot;stringi&quot;) Exercise 14.7.1 Find the stringi functions that: Count the number of words. Find duplicated strings. Generate random text. The answer to each part follows. To count the number of words use stringi::stri_count_words(). This code counts the words in the first five sentences of sentences. stri_count_words(head(sentences)) #&gt; [1] 8 8 9 9 7 7 The stringi::stri_duplicated() function finds duplicate strings. stri_duplicated(c(&quot;the&quot;, &quot;brown&quot;, &quot;cow&quot;, &quot;jumped&quot;, &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;fox&quot;)) #&gt; [1] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE The stringi package contains several functions beginning with stri_rand_* that generate random text. The function stringi::stri_rand_strings() generates random strings. The following code generates four random strings each of length five. stri_rand_strings(4, 5) #&gt; [1] &quot;5pb90&quot; &quot;SUHjl&quot; &quot;sA2JO&quot; &quot;CP3Oy&quot; The function stringi::stri_rand_shuffle() randomly shuffles the characters in the text. stri_rand_shuffle(&quot;The brown fox jumped over the lazy cow.&quot;) #&gt; [1] &quot;ot f.lween p jzwoom xyucobhv daheerrT&quot; The function stringi::stri_rand_lipsum() generates lorem ipsum text. Lorem ipsum text is nonsense text often used as placeholder text in publishing. The following code generates one paragraph of placeholder text. stri_rand_lipsum(1) #&gt; [1] &quot;Lorem ipsum dolor sit amet, hac non metus cras nam vitae tempus proin, sed. Diam gravida viverra eros mauris, magna lacinia dui nullam. Arcu proin aenean fringilla sed sollicitudin hac neque, egestas condimentum massa, elementum vivamus. Odio eget litora molestie eget eros pulvinar ac. Vel nec nullam vivamus, sociosqu lectus varius eleifend. Vitae in. Conubia ut hac maximus amet, conubia sed. Volutpat vitae class cursus, elit mauris porta. Mauris lacus donec odio eget quam inceptos, ridiculus cursus, ad massa. Rhoncus hac aenean at id consectetur molestie vitae! Sed, primis mi dictum lacinia eros. Ligula, feugiat consequat ut vivamus ut morbi et. Dolor, eget eleifend nec magnis aliquam egestas. Sollicitudin venenatis et aptent rhoncus nisl platea ligula cum.&quot; Exercise 14.7.2 How do you control the language that stri_sort() uses for sorting? You can set a locale to use when sorting with either stri_sort(..., opts_collator=stri_opts_collator(locale = ...)) or stri_sort(..., locale = ...). In this example from the stri_sort() documentation, the sorted order of the character vector depends on the locale. string1 &lt;- c(&quot;hladny&quot;, &quot;chladny&quot;) stri_sort(string1, locale = &quot;pl_PL&quot;) #&gt; [1] &quot;chladny&quot; &quot;hladny&quot; stri_sort(string1, locale = &quot;sk_SK&quot;) #&gt; [1] &quot;hladny&quot; &quot;chladny&quot; The output of stri_opts_collator() can also be used for the locale argument of str_sort. stri_sort(string1, opts_collator = stri_opts_collator(locale = &quot;pl_PL&quot;)) #&gt; [1] &quot;chladny&quot; &quot;hladny&quot; stri_sort(string1, opts_collator = stri_opts_collator(locale = &quot;sk_SK&quot;)) #&gt; [1] &quot;hladny&quot; &quot;chladny&quot; The stri_opts_collator() provides finer grained control over how strings are sorted. In addition to setting the locale, it has options to customize how cases, unicode, accents, and numeric values are handled when comparing strings. string2 &lt;- c(&quot;number100&quot;, &quot;number2&quot;) stri_sort(string2) #&gt; [1] &quot;number100&quot; &quot;number2&quot; stri_sort(string2, opts_collator = stri_opts_collator(numeric = TRUE)) #&gt; [1] &quot;number2&quot; &quot;number100&quot; "],
["factors.html", "15 Factors 15.1 Introduction 15.2 Creating factors 15.3 General Social Survey 15.4 Modifying factor order 15.5 Modifying factor levels", " 15 Factors 15.1 Introduction Functions and packages: library(&quot;tidyverse&quot;) The forcats package does not need to be explicitly loaded, since the recent versions of the tidyverse package now attach it. 15.2 Creating factors No exercises 15.3 General Social Survey Exercise 15.3.1 Explore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot? My first attempt is to use geom_bar() with the default settings. rincome_plot &lt;- gss_cat %&gt;% ggplot(aes(x = rincome)) + geom_bar() rincome_plot The problem with default bar chart settings, are that the labels overlapping and impossible to read. I’ll try changing the angle of the x-axis labels to vertical so that they will not overlap. rincome_plot + theme(axis.text.x = element_text(angle = 90, hjust = 1)) This is better because the labels are not overlapping, but also difficult to read because the labels are vertical. I could try angling the labels so that they are easier to read, but not overlapping. rincome_plot + theme(axis.text.x = element_text(angle = 45, hjust = 1)) But the solution I prefer for bar charts with long labels is to flip the axes, so that the bars are horizontal. Then the category labels are also horizontal, and easy to read. rincome_plot + coord_flip() Though more than asked for in this question, I could further improve this plot by removing the “Not applicable” responses, renaming “Lt $1000” to “Less than $1000”, using color to distinguish non-response categories (“Refused”, “Don’t know”, and “No answer”) from income levels (“Lt $1000”, …), adding meaningful y- and x-axis titles, and formatting the counts axis labels to use commas. gss_cat %&gt;% filter(!rincome %in% c(&quot;Not applicable&quot;)) %&gt;% mutate(rincome = fct_recode(rincome, &quot;Less than $1000&quot; = &quot;Lt $1000&quot; )) %&gt;% mutate(rincome_na = rincome %in% c(&quot;Refused&quot;, &quot;Don&#39;t know&quot;, &quot;No answer&quot;)) %&gt;% ggplot(aes(x = rincome, fill = rincome_na)) + geom_bar() + coord_flip() + scale_y_continuous(&quot;Number of Respondents&quot;, labels = scales::comma) + scale_x_discrete(&quot;Respondent&#39;s Income&quot;) + scale_fill_manual(values = c(&quot;FALSE&quot; = &quot;black&quot;, &quot;TRUE&quot; = &quot;gray&quot;)) + theme(legend.position = &quot;None&quot;) If I were only interested in non-missing responses, then I could drop all respondents who answered “Not applicable”, “Refused”, “Don’t know”, or “No answer”. gss_cat %&gt;% filter(!rincome %in% c(&quot;Not applicable&quot;, &quot;Don&#39;t know&quot;, &quot;No answer&quot;, &quot;Refused&quot;)) %&gt;% mutate(rincome = fct_recode(rincome, &quot;Less than $1000&quot; = &quot;Lt $1000&quot; )) %&gt;% ggplot(aes(x = rincome)) + geom_bar() + coord_flip() + scale_y_continuous(&quot;Number of Respondents&quot;, labels = scales::comma) + scale_x_discrete(&quot;Respondent&#39;s Income&quot;) A side-effect of coord_flip() is that the label ordering on the x-axis, from lowest (top) to highest (bottom) is counterintuitive. The next section introduces a function fct_reorder() which can help with this. Exercise 15.3.2 What is the most common relig in this survey? What’s the most common partyid? The most common relig is “Protestant” gss_cat %&gt;% count(relig) %&gt;% arrange(desc(n)) %&gt;% head(1) #&gt; # A tibble: 1 x 2 #&gt; relig n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Protestant 10846 The most common partyid is “Independent” gss_cat %&gt;% count(partyid) %&gt;% arrange(desc(n)) %&gt;% head(1) #&gt; # A tibble: 1 x 2 #&gt; partyid n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Independent 4119 Exercise 15.3.3 Which relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualization? levels(gss_cat$denom) #&gt; [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;No denomination&quot; #&gt; [4] &quot;Other&quot; &quot;Episcopal&quot; &quot;Presbyterian-dk wh&quot; #&gt; [7] &quot;Presbyterian, merged&quot; &quot;Other presbyterian&quot; &quot;United pres ch in us&quot; #&gt; [10] &quot;Presbyterian c in us&quot; &quot;Lutheran-dk which&quot; &quot;Evangelical luth&quot; #&gt; [13] &quot;Other lutheran&quot; &quot;Wi evan luth synod&quot; &quot;Lutheran-mo synod&quot; #&gt; [16] &quot;Luth ch in america&quot; &quot;Am lutheran&quot; &quot;Methodist-dk which&quot; #&gt; [19] &quot;Other methodist&quot; &quot;United methodist&quot; &quot;Afr meth ep zion&quot; #&gt; [22] &quot;Afr meth episcopal&quot; &quot;Baptist-dk which&quot; &quot;Other baptists&quot; #&gt; [25] &quot;Southern baptist&quot; &quot;Nat bapt conv usa&quot; &quot;Nat bapt conv of am&quot; #&gt; [28] &quot;Am bapt ch in usa&quot; &quot;Am baptist asso&quot; &quot;Not applicable&quot; From the context it is clear that denom refers to “Protestant” (and unsurprising given that it is the largest category in freq). Let’s filter out the non-responses, no answers, others, not-applicable, or no denomination, to leave only answers to denominations. After doing that, the only remaining responses are “Protestant”. gss_cat %&gt;% filter(!denom %in% c( &quot;No answer&quot;, &quot;Other&quot;, &quot;Don&#39;t know&quot;, &quot;Not applicable&quot;, &quot;No denomination&quot; )) %&gt;% count(relig) #&gt; # A tibble: 1 x 2 #&gt; relig n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Protestant 7025 This is also clear in a scatter plot of relig vs. denom where the points are proportional to the size of the number of answers (since otherwise there would be overplotting). gss_cat %&gt;% count(relig, denom) %&gt;% ggplot(aes(x = relig, y = denom, size = n)) + geom_point() + theme(axis.text.x = element_text(angle = 90)) 15.4 Modifying factor order Exercise 15.4.1 There are some suspiciously high numbers in tvhours. Is the mean a good summary? summary(gss_cat[[&quot;tvhours&quot;]]) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; 0 1 2 3 4 24 10146 gss_cat %&gt;% filter(!is.na(tvhours)) %&gt;% ggplot(aes(x = tvhours)) + geom_histogram(binwidth = 1) Whether the mean is the best summary depends on what you are using it for :-), i.e. your objective. But probably the median would be what most people prefer. And the hours of TV doesn’t look that surprising to me. Exercise 15.4.2 For each factor in gss_cat identify whether the order of the levels is arbitrary or principled. The following piece of code uses functions introduced in Ch 21, to print out the names of only the factors. keep(gss_cat, is.factor) %&gt;% names() #&gt; [1] &quot;marital&quot; &quot;race&quot; &quot;rincome&quot; &quot;partyid&quot; &quot;relig&quot; &quot;denom&quot; There are six categorical variables: marital, race, rincome, partyid, relig, and denom. The ordering of marital is “somewhat principled”. There is some sort of logic in that the levels are grouped “never married”, married at some point (separated, divorced, widowed), and “married”; though it would seem that “Never Married”, “Divorced”, “Widowed”, “Separated”, “Married” might be more natural. I find that the question of ordering can be determined by the level of aggregation in a categorical variable, and there can be more “partially ordered” factors than one would expect. levels(gss_cat[[&quot;marital&quot;]]) #&gt; [1] &quot;No answer&quot; &quot;Never married&quot; &quot;Separated&quot; &quot;Divorced&quot; #&gt; [5] &quot;Widowed&quot; &quot;Married&quot; gss_cat %&gt;% ggplot(aes(x = marital)) + geom_bar() The ordering of race is principled in that the categories are ordered by count of observations in the data. levels(gss_cat$race) #&gt; [1] &quot;Other&quot; &quot;Black&quot; &quot;White&quot; &quot;Not applicable&quot; gss_cat %&gt;% ggplot(aes(race)) + geom_bar() + scale_x_discrete(drop = FALSE) The levels of rincome are ordered in decreasing order of the income; however the placement of “No answer”, “Don’t know”, and “Refused” before, and “Not applicable” after the income levels is arbitrary. It would be better to place all the missing income level categories either before or after all the known values. levels(gss_cat$rincome) #&gt; [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Refused&quot; &quot;$25000 or more&quot; #&gt; [5] &quot;$20000 - 24999&quot; &quot;$15000 - 19999&quot; &quot;$10000 - 14999&quot; &quot;$8000 to 9999&quot; #&gt; [9] &quot;$7000 to 7999&quot; &quot;$6000 to 6999&quot; &quot;$5000 to 5999&quot; &quot;$4000 to 4999&quot; #&gt; [13] &quot;$3000 to 3999&quot; &quot;$1000 to 2999&quot; &quot;Lt $1000&quot; &quot;Not applicable&quot; The levels of relig is arbitrary: there is no natural ordering, and they don’t appear to be ordered by stats within the dataset. levels(gss_cat$relig) #&gt; [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; #&gt; [3] &quot;Inter-nondenominational&quot; &quot;Native american&quot; #&gt; [5] &quot;Christian&quot; &quot;Orthodox-christian&quot; #&gt; [7] &quot;Moslem/islam&quot; &quot;Other eastern&quot; #&gt; [9] &quot;Hinduism&quot; &quot;Buddhism&quot; #&gt; [11] &quot;Other&quot; &quot;None&quot; #&gt; [13] &quot;Jewish&quot; &quot;Catholic&quot; #&gt; [15] &quot;Protestant&quot; &quot;Not applicable&quot; gss_cat %&gt;% ggplot(aes(relig)) + geom_bar() + coord_flip() The same goes for denom. levels(gss_cat$denom) #&gt; [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;No denomination&quot; #&gt; [4] &quot;Other&quot; &quot;Episcopal&quot; &quot;Presbyterian-dk wh&quot; #&gt; [7] &quot;Presbyterian, merged&quot; &quot;Other presbyterian&quot; &quot;United pres ch in us&quot; #&gt; [10] &quot;Presbyterian c in us&quot; &quot;Lutheran-dk which&quot; &quot;Evangelical luth&quot; #&gt; [13] &quot;Other lutheran&quot; &quot;Wi evan luth synod&quot; &quot;Lutheran-mo synod&quot; #&gt; [16] &quot;Luth ch in america&quot; &quot;Am lutheran&quot; &quot;Methodist-dk which&quot; #&gt; [19] &quot;Other methodist&quot; &quot;United methodist&quot; &quot;Afr meth ep zion&quot; #&gt; [22] &quot;Afr meth episcopal&quot; &quot;Baptist-dk which&quot; &quot;Other baptists&quot; #&gt; [25] &quot;Southern baptist&quot; &quot;Nat bapt conv usa&quot; &quot;Nat bapt conv of am&quot; #&gt; [28] &quot;Am bapt ch in usa&quot; &quot;Am baptist asso&quot; &quot;Not applicable&quot; Ignoring “No answer”, “Don’t know”, and “Other party”, the levels of partyid are ordered from “Strong Republican”&quot; to “Strong Democrat”. levels(gss_cat$partyid) #&gt; [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Other party&quot; #&gt; [4] &quot;Strong republican&quot; &quot;Not str republican&quot; &quot;Ind,near rep&quot; #&gt; [7] &quot;Independent&quot; &quot;Ind,near dem&quot; &quot;Not str democrat&quot; #&gt; [10] &quot;Strong democrat&quot; Exercise 15.4.3 Why did moving “Not applicable” to the front of the levels move it to the bottom of the plot? Because that gives the level “Not applicable” an integer value of 1. 15.5 Modifying factor levels Exercise 15.5.1 How have the proportions of people identifying as Democrat, Republican, and Independent changed over time? To answer that, we need to combine the multiple levels into Democrat, Republican, and Independent levels(gss_cat$partyid) #&gt; [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Other party&quot; #&gt; [4] &quot;Strong republican&quot; &quot;Not str republican&quot; &quot;Ind,near rep&quot; #&gt; [7] &quot;Independent&quot; &quot;Ind,near dem&quot; &quot;Not str democrat&quot; #&gt; [10] &quot;Strong democrat&quot; gss_cat %&gt;% mutate( partyid = fct_collapse(partyid, other = c(&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;Other party&quot;), rep = c(&quot;Strong republican&quot;, &quot;Not str republican&quot;), ind = c(&quot;Ind,near rep&quot;, &quot;Independent&quot;, &quot;Ind,near dem&quot;), dem = c(&quot;Not str democrat&quot;, &quot;Strong democrat&quot;) ) ) %&gt;% count(year, partyid) %&gt;% group_by(year) %&gt;% mutate(p = n / sum(n)) %&gt;% ggplot(aes( x = year, y = p, colour = fct_reorder2(partyid, year, p) )) + geom_point() + geom_line() + labs(colour = &quot;Party ID.&quot;) Exercise 15.5.2 How could you collapse rincome into a small set of categories? Group all the non-responses into one category, and then group other categories into a smaller number. Since there is a clear ordering, we would not use fct_lump().` levels(gss_cat$rincome) #&gt; [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Refused&quot; &quot;$25000 or more&quot; #&gt; [5] &quot;$20000 - 24999&quot; &quot;$15000 - 19999&quot; &quot;$10000 - 14999&quot; &quot;$8000 to 9999&quot; #&gt; [9] &quot;$7000 to 7999&quot; &quot;$6000 to 6999&quot; &quot;$5000 to 5999&quot; &quot;$4000 to 4999&quot; #&gt; [13] &quot;$3000 to 3999&quot; &quot;$1000 to 2999&quot; &quot;Lt $1000&quot; &quot;Not applicable&quot; library(&quot;stringr&quot;) gss_cat %&gt;% mutate( rincome = fct_collapse( rincome, `Unknown` = c(&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;Refused&quot;, &quot;Not applicable&quot;), `Lt $5000` = c(&quot;Lt $1000&quot;, str_c( &quot;$&quot;, c(&quot;1000&quot;, &quot;3000&quot;, &quot;4000&quot;), &quot; to &quot;, c(&quot;2999&quot;, &quot;3999&quot;, &quot;4999&quot;) )), `$5000 to 10000` = str_c( &quot;$&quot;, c(&quot;5000&quot;, &quot;6000&quot;, &quot;7000&quot;, &quot;8000&quot;), &quot; to &quot;, c(&quot;5999&quot;, &quot;6999&quot;, &quot;7999&quot;, &quot;9999&quot;) ) ) ) %&gt;% ggplot(aes(x = rincome)) + geom_bar() + coord_flip() "],
["dates-and-times.html", "16 Dates and times 16.1 Introduction 16.2 Creating date/times 16.3 Date-time components 16.4 Time spans 16.5 Time zones", " 16 Dates and times 16.1 Introduction library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) library(&quot;nycflights13&quot;) 16.2 Creating date/times This code is needed by exercises. make_datetime_100 &lt;- function(year, month, day, time) { make_datetime(year, month, day, time %/% 100, time %% 100) } flights_dt &lt;- flights %&gt;% filter(!is.na(dep_time), !is.na(arr_time)) %&gt;% mutate( dep_time = make_datetime_100(year, month, day, dep_time), arr_time = make_datetime_100(year, month, day, arr_time), sched_dep_time = make_datetime_100(year, month, day, sched_dep_time), sched_arr_time = make_datetime_100(year, month, day, sched_arr_time) ) %&gt;% select(origin, dest, ends_with(&quot;delay&quot;), ends_with(&quot;time&quot;)) Exercise 16.2.1 What happens if you parse a string that contains invalid dates? ret &lt;- ymd(c(&quot;2010-10-10&quot;, &quot;bananas&quot;)) #&gt; Warning: 1 failed to parse. print(class(ret)) #&gt; [1] &quot;Date&quot; ret #&gt; [1] &quot;2010-10-10&quot; NA It produces an NA and a warning message. Exercise 16.2.2 What does the tzone argument to today() do? Why is it important? It determines the time-zone of the date. Since different time-zones can have different dates, the value of today() can vary depending on the time-zone specified. Exercise 16.2.3 Use the appropriate lubridate function to parse each of the following dates: d1 &lt;- &quot;January 1, 2010&quot; d2 &lt;- &quot;2015-Mar-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; d4 &lt;- c(&quot;August 19 (2015)&quot;, &quot;July 1 (2015)&quot;) d5 &lt;- &quot;12/30/14&quot; mdy(d1) #&gt; [1] &quot;2010-01-01&quot; ymd(d2) #&gt; [1] &quot;2015-03-07&quot; dmy(d3) #&gt; [1] &quot;2017-06-06&quot; mdy(d4) #&gt; [1] &quot;2015-08-19&quot; &quot;2015-07-01&quot; mdy(d5) #&gt; [1] &quot;2014-12-30&quot; 16.3 Date-time components The following code from the chapter is used sched_dep &lt;- flights_dt %&gt;% mutate(minute = minute(sched_dep_time)) %&gt;% group_by(minute) %&gt;% summarise( avg_delay = mean(arr_delay, na.rm = TRUE), n = n() ) #&gt; `summarise()` ungrouping output (override with `.groups` argument) In the previous code, the difference between rounded and un-rounded dates provides the within-period time. Exercise 16.3.1 How does the distribution of flight times within a day change over the course of the year? Let’s try plotting this by month: flights_dt %&gt;% filter(!is.na(dep_time)) %&gt;% mutate(dep_hour = update(dep_time, yday = 1)) %&gt;% mutate(month = factor(month(dep_time))) %&gt;% ggplot(aes(dep_hour, color = month)) + geom_freqpoly(binwidth = 60 * 60) This will look better if everything is normalized within groups. The reason that February is lower is that there are fewer days and thus fewer flights. flights_dt %&gt;% filter(!is.na(dep_time)) %&gt;% mutate(dep_hour = update(dep_time, yday = 1)) %&gt;% mutate(month = factor(month(dep_time))) %&gt;% ggplot(aes(dep_hour, color = month)) + geom_freqpoly(aes(y = ..density..), binwidth = 60 * 60) At least to me there doesn’t appear to much difference in within-day distribution over the year, but I maybe thinking about it incorrectly. Exercise 16.3.2 Compare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings. If they are consistent, then dep_time = sched_dep_time + dep_delay. flights_dt %&gt;% mutate(dep_time_ = sched_dep_time + dep_delay * 60) %&gt;% filter(dep_time_ != dep_time) %&gt;% select(dep_time_, dep_time, sched_dep_time, dep_delay) #&gt; # A tibble: 1,205 x 4 #&gt; dep_time_ dep_time sched_dep_time dep_delay #&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dbl&gt; #&gt; 1 2013-01-02 08:48:00 2013-01-01 08:48:00 2013-01-01 18:35:00 853 #&gt; 2 2013-01-03 00:42:00 2013-01-02 00:42:00 2013-01-02 23:59:00 43 #&gt; 3 2013-01-03 01:26:00 2013-01-02 01:26:00 2013-01-02 22:50:00 156 #&gt; 4 2013-01-04 00:32:00 2013-01-03 00:32:00 2013-01-03 23:59:00 33 #&gt; 5 2013-01-04 00:50:00 2013-01-03 00:50:00 2013-01-03 21:45:00 185 #&gt; 6 2013-01-04 02:35:00 2013-01-03 02:35:00 2013-01-03 23:59:00 156 #&gt; # … with 1,199 more rows There exist discrepancies. It looks like there are mistakes in the dates. These are flights in which the actual departure time is on the next day relative to the scheduled departure time. We forgot to account for this when creating the date-times using make_datetime_100() function in 16.2.2 From individual components. The code would have had to check if the departure time is less than the scheduled departure time plus departure delay (in minutes). Alternatively, simply adding the departure delay to the scheduled departure time is a more robust way to construct the departure time because it will automatically account for crossing into the next day. Exercise 16.3.3 Compare air_time with the duration between the departure and arrival. Explain your findings. flights_dt %&gt;% mutate( flight_duration = as.numeric(arr_time - dep_time), air_time_mins = air_time, diff = flight_duration - air_time_mins ) %&gt;% select(origin, dest, flight_duration, air_time_mins, diff) #&gt; # A tibble: 328,063 x 5 #&gt; origin dest flight_duration air_time_mins diff #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 EWR IAH 193 227 -34 #&gt; 2 LGA IAH 197 227 -30 #&gt; 3 JFK MIA 221 160 61 #&gt; 4 JFK BQN 260 183 77 #&gt; 5 LGA ATL 138 116 22 #&gt; 6 EWR ORD 106 150 -44 #&gt; # … with 328,057 more rows Exercise 16.3.4 How does the average delay time change over the course of a day? Should you use dep_time or sched_dep_time? Why? Use sched_dep_time because that is the relevant metric for someone scheduling a flight. Also, using dep_time will always bias delays to later in the day since delays will push flights later. flights_dt %&gt;% mutate(sched_dep_hour = hour(sched_dep_time)) %&gt;% group_by(sched_dep_hour) %&gt;% summarise(dep_delay = mean(dep_delay)) %&gt;% ggplot(aes(y = dep_delay, x = sched_dep_hour)) + geom_point() + geom_smooth() #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Exercise 16.3.5 On what day of the week should you leave if you want to minimize the chance of a delay? Saturday has the lowest average departure delay time and the lowest average arrival delay time. flights_dt %&gt;% mutate(dow = wday(sched_dep_time)) %&gt;% group_by(dow) %&gt;% summarise( dep_delay = mean(dep_delay), arr_delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% print(n = Inf) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 7 x 3 #&gt; dow dep_delay arr_delay #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 11.5 4.82 #&gt; 2 2 14.7 9.65 #&gt; 3 3 10.6 5.39 #&gt; 4 4 11.7 7.05 #&gt; 5 5 16.1 11.7 #&gt; 6 6 14.7 9.07 #&gt; 7 7 7.62 -1.45 flights_dt %&gt;% mutate(wday = wday(dep_time, label = TRUE)) %&gt;% group_by(wday) %&gt;% summarize(ave_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = wday, y = ave_dep_delay)) + geom_bar(stat = &quot;identity&quot;) #&gt; `summarise()` ungrouping output (override with `.groups` argument) flights_dt %&gt;% mutate(wday = wday(dep_time, label = TRUE)) %&gt;% group_by(wday) %&gt;% summarize(ave_arr_delay = mean(arr_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = wday, y = ave_arr_delay)) + geom_bar(stat = &quot;identity&quot;) #&gt; `summarise()` ungrouping output (override with `.groups` argument) Exercise 16.3.6 What makes the distribution of diamonds$carat and flights$sched_dep_time similar? ggplot(diamonds, aes(x = carat)) + geom_density() In both carat and sched_dep_time there are abnormally large numbers of values are at nice “human” numbers. In sched_dep_time it is at 00 and 30 minutes. In carats, it is at 0, 1/3, 1/2, 2/3, ggplot(diamonds, aes(x = carat %% 1 * 100)) + geom_histogram(binwidth = 1) In scheduled departure times it is 00 and 30 minutes, and minutes ending in 0 and 5. ggplot(flights_dt, aes(x = minute(sched_dep_time))) + geom_histogram(binwidth = 1) Exercise 16.3.7 Confirm my hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. Hint: create a binary variable that tells you whether or not a flight was delayed. First, I create a binary variable early that is equal to 1 if a flight leaves early, and 0 if it does not. Then, I group flights by the minute of departure. This shows that the proportion of flights that are early departures is highest between minutes 20–30 and 50–60. flights_dt %&gt;% mutate(minute = minute(dep_time), early = dep_delay &lt; 0) %&gt;% group_by(minute) %&gt;% summarise( early = mean(early, na.rm = TRUE), n = n()) %&gt;% ggplot(aes(minute, early)) + geom_line() #&gt; `summarise()` ungrouping output (override with `.groups` argument) 16.4 Time spans Exercise 16.4.1 Why is there months() but no dmonths()? There is no unambiguous value of months in terms of seconds since months have differing numbers of days. 31 days: January, March, May, July, August, October, December 30 days: April, June, September, November 28 or 29 days: February The month is not a duration of time defined independently of when it occurs, but a special interval between two dates. Exercise 16.4.2 Explain days(overnight * 1) to someone who has just started learning R. How does it work? The variable overnight is equal to TRUE or FALSE. If it is an overnight flight, this becomes 1 day, and if not, then overnight = 0, and no days are added to the date. Exercise 16.4.3 Create a vector of dates giving the first day of every month in 2015. Create a vector of dates giving the first day of every month in the current year. A vector of the first day of the month for every month in 2015: ymd(&quot;2015-01-01&quot;) + months(0:11) #&gt; [1] &quot;2015-01-01&quot; &quot;2015-02-01&quot; &quot;2015-03-01&quot; &quot;2015-04-01&quot; &quot;2015-05-01&quot; #&gt; [6] &quot;2015-06-01&quot; &quot;2015-07-01&quot; &quot;2015-08-01&quot; &quot;2015-09-01&quot; &quot;2015-10-01&quot; #&gt; [11] &quot;2015-11-01&quot; &quot;2015-12-01&quot; To get the vector of the first day of the month for this year, we first need to figure out what this year is, and get January 1st of it. I can do that by taking today() and truncating it to the year using floor_date(): floor_date(today(), unit = &quot;year&quot;) + months(0:11) #&gt; [1] &quot;2020-01-01&quot; &quot;2020-02-01&quot; &quot;2020-03-01&quot; &quot;2020-04-01&quot; &quot;2020-05-01&quot; #&gt; [6] &quot;2020-06-01&quot; &quot;2020-07-01&quot; &quot;2020-08-01&quot; &quot;2020-09-01&quot; &quot;2020-10-01&quot; #&gt; [11] &quot;2020-11-01&quot; &quot;2020-12-01&quot; Exercise 16.4.4 Write a function that given your birthday (as a date), returns how old you are in years. age &lt;- function(bday) { (bday %--% today()) %/% years(1) } age(ymd(&quot;1990-10-12&quot;)) #&gt; [1] 29 Exercise 16.4.5 Why can’t (today() %--% (today() + years(1)) / months(1) work? The code in the question is missing a parentheses. So, I will assume that that the correct code is, (today() %--% (today() + years(1))) / months(1) #&gt; [1] 12 While this code will not display a warning or message, it does not work exactly as expected. The problem is discussed in the Intervals section. The numerator of the expression, (today() %--% (today() + years(1)), is an interval, which includes both a duration of time and a starting point. The interval has an exact number of seconds. The denominator of the expression, months(1), is a period, which is meaningful to humans but not defined in terms of an exact number of seconds. Months can be 28, 29, 30, or 31 days, so it is not clear what months(1) divide by? The code does not produce a warning message, but it will not always produce the correct result. To find the number of months within an interval use %/% instead of /, (today() %--% (today() + years(1))) %/% months(1) #&gt; [1] 12 Alternatively, we could define a “month” as 30 days, and run (today() %--% (today() + years(1))) / days(30) #&gt; [1] 12.2 This approach will not work with today() + years(1), which is not defined for February 29th on leap years: as.Date(&quot;2016-02-29&quot;) + years(1) #&gt; [1] NA 16.5 Time zones No exercises "],
["program-intro.html", "17 Introduction", " 17 Introduction No exercises "],
["pipes.html", "18 Pipes", " 18 Pipes No exercises "],
["functions.html", "19 Functions 19.1 Introduction 19.2 When should you write a function? 19.3 Functions are for humans and computers 19.4 Conditional execution 19.5 Function arguments 19.6 Return values 19.7 Environment", " 19 Functions 19.1 Introduction library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) 19.2 When should you write a function? Exercise 19.2.1 Why is TRUE not a parameter to rescale01()? What would happen if x contained a single missing value, and na.rm was FALSE? The code for rescale01() is reproduced below. rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE, finite = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } If x contains a single missing value and na.rm = FALSE, then this function stills return a non-missing value. rescale01_alt &lt;- function(x, na.rm = FALSE) { rng &lt;- range(x, na.rm = na.rm, finite = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } rescale01_alt(c(NA, 1:5), na.rm = FALSE) #&gt; [1] NA 0.00 0.25 0.50 0.75 1.00 rescale01_alt(c(NA, 1:5), na.rm = TRUE) #&gt; [1] NA 0.00 0.25 0.50 0.75 1.00 The option finite = TRUE to range() will drop all non-finite elements, and NA is a non-finite element. However, if both finite = FALSE and na.rm = FALSE, then this function will return a vector of NA values. Recall, arithmetic operations involving NA values return NA. rescale01_alt2 &lt;- function(x, na.rm = FALSE, finite = FALSE) { rng &lt;- range(x, na.rm = na.rm, finite = finite) (x - rng[1]) / (rng[2] - rng[1]) } rescale01_alt2(c(NA, 1:5), na.rm = FALSE, finite = FALSE) #&gt; [1] NA NA NA NA NA NA Exercise 19.2.2 In the second variant of rescale01(), infinite values are left unchanged. Rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1. rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE, finite = TRUE) y &lt;- (x - rng[1]) / (rng[2] - rng[1]) y[y == -Inf] &lt;- 0 y[y == Inf] &lt;- 1 y } rescale01(c(Inf, -Inf, 0:5, NA)) #&gt; [1] 1.0 0.0 0.0 0.2 0.4 0.6 0.8 1.0 NA Exercise 19.2.3 Practice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? Can you rewrite it to be more expressive or less duplicative? mean(is.na(x)) x / sum(x, na.rm = TRUE) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) This code calculates the proportion of NA values in a vector. mean(is.na(x)) I will write it as a function named prop_na() that takes a single argument x, and returns a single numeric value between 0 and 1. prop_na &lt;- function(x) { mean(is.na(x)) } prop_na(c(0, 1, 2, NA, 4, NA)) #&gt; [1] 0.333 This code standardizes a vector so that it sums to one. x / sum(x, na.rm = TRUE) I’ll write a function named sum_to_one(), which is a function of a single argument, x, the vector to standardize, and an optional argument na.rm. The optional argument, na.rm, makes the function more expressive, since it can handle NA values in two ways (returning NA or dropping them). Additionally, this makes sum_to_one() consistent with sum(), mean(), and many other R functions which have a na.rm argument. While the example code had na.rm = TRUE, I set na.rm = FALSE by default in order to make the function behave the same as the built-in functions like sum() and mean() in its handling of missing values. sum_to_one &lt;- function(x, na.rm = FALSE) { x / sum(x, na.rm = na.rm) } # no missing values sum_to_one(1:5) #&gt; [1] 0.0667 0.1333 0.2000 0.2667 0.3333 # if any missing, return all missing sum_to_one(c(1:5, NA)) #&gt; [1] NA NA NA NA NA NA # drop missing values when standardizing sum_to_one(c(1:5, NA), na.rm = TRUE) #&gt; [1] 0.0667 0.1333 0.2000 0.2667 0.3333 NA This code calculates the coefficient of variation (assuming that x can only take non-negative values), which is the standard deviation divided by the mean. sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) I’ll write a function named coef_variation(), which takes a single argument x, and an optional na.rm argument. coef_variation &lt;- function(x, na.rm = FALSE) { sd(x, na.rm = na.rm) / mean(x, na.rm = na.rm) } coef_variation(1:5) #&gt; [1] 0.527 coef_variation(c(1:5, NA)) #&gt; [1] NA coef_variation(c(1:5, NA), na.rm = TRUE) #&gt; [1] 0.527 Exercise 19.2.4 Follow https://nicercode.github.io/intro/writing-functions.html to write your own functions to compute the variance and skew of a numeric vector. Note The math in https://nicercode.github.io/intro/writing-functions.html seems not to be rendering. The sample variance is defined as, \\[ \\mathrm{Var}(x) = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\bar{x}) ^2 \\text{,} \\] where \\(\\bar{x} = (\\sum_i^n x_i) / n\\) is the sample mean. The corresponding function is: variance &lt;- function(x, na.rm = TRUE) { n &lt;- length(x) m &lt;- mean(x, na.rm = TRUE) sq_err &lt;- (x - m)^2 sum(sq_err) / (n - 1) } var(1:10) #&gt; [1] 9.17 variance(1:10) #&gt; [1] 9.17 There are multiple definitions for skewness, but we will use the following one, \\[ \\mathrm{Skew}(x) = \\frac{\\frac{1}{n - 2}\\left(\\sum_{i=1}^{n}(x_{i} - \\bar x)^3\\right)}{\\mathrm{Var}(x)^{3 / 2}} \\text{.} \\] The corresponding function is: skewness &lt;- function(x, na.rm = FALSE) { n &lt;- length(x) m &lt;- mean(x, na.rm = na.rm) v &lt;- var(x, na.rm = na.rm) (sum((x - m) ^ 3) / (n - 2)) / v ^ (3 / 2) } skewness(c(1, 2, 5, 100)) #&gt; [1] 1.49 Exercise 19.2.5 Write both_na(), a function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors. both_na &lt;- function(x, y) { sum(is.na(x) &amp; is.na(y)) } both_na( c(NA, NA, 1, 2), c(NA, 1, NA, 2) ) #&gt; [1] 1 both_na( c(NA, NA, 1, 2, NA, NA, 1), c(NA, 1, NA, 2, NA, NA, 1) ) #&gt; [1] 3 Exercise 19.2.6 What do the following functions do? Why are they useful even though they are so short? is_directory &lt;- function(x) file.info(x)$isdir is_readable &lt;- function(x) file.access(x, 4) == 0 The function is_directory() checks whether the path in x is a directory. The function is_readable() checks whether the path in x is readable, meaning that the file exists and the user has permission to open it. These functions are useful even though they are short because their names make it much clearer what the code is doing. Exercise 19.2.7 Read the complete lyrics to ``Little Bunny Foo Foo’’. There’s a lot of duplication in this song. Extend the initial piping example to recreate the complete song, and use functions to reduce the duplication. The lyrics of one of the most common versions of this song are Little bunny Foo Foo Hopping through the forest Scooping up the field mice And bopping them on the head Down came the Good Fairy, and she said &quot;Little bunny Foo Foo I don’t want to see you   Scooping up the field mice And bopping them on the head. I’ll give you three chances, And if you don’t stop, I’ll turn you into a GOON!&quot; And the next day… The verses repeat with one chance fewer each time. When there are no chances left, the Good Fairy says “I gave you three chances, and you didn’t stop; so….” POOF. She turned him into a GOON! And the moral of this story is: hare today, goon tomorrow. Here’s one way of writing this threat &lt;- function(chances) { give_chances( from = Good_Fairy, to = foo_foo, number = chances, condition = &quot;Don&#39;t behave&quot;, consequence = turn_into_goon ) } lyric &lt;- function() { foo_foo %&gt;% hop(through = forest) %&gt;% scoop(up = field_mouse) %&gt;% bop(on = head) down_came(Good_Fairy) said( Good_Fairy, c( &quot;Little bunny Foo Foo&quot;, &quot;I don&#39;t want to see you&quot;, &quot;Scooping up the field mice&quot;, &quot;And bopping them on the head.&quot; ) ) } lyric() threat(3) lyric() threat(2) lyric() threat(1) lyric() turn_into_goon(Good_Fairy, foo_foo) 19.3 Functions are for humans and computers Exercise 19.3.1 Read the source code for each of the following three functions, puzzle out what they do, and then brainstorm better names. f1 &lt;- function(string, prefix) { substr(string, 1, nchar(prefix)) == prefix } f2 &lt;- function(x) { if (length(x) &lt;= 1) return(NULL) x[-length(x)] } f3 &lt;- function(x, y) { rep(y, length.out = length(x)) } The function f1 tests whether each element of the character vector nchar starts with the string prefix. For example, f1(c(&quot;abc&quot;, &quot;abcde&quot;, &quot;ad&quot;), &quot;ab&quot;) #&gt; [1] TRUE TRUE FALSE A better name for f1 is has_prefix() The function f2 drops the last element of the vector x. f2(1:3) #&gt; [1] 1 2 f2(1:2) #&gt; [1] 1 f2(1) #&gt; NULL A better name for f2 is drop_last(). The function f3 repeats y once for each element of x. f3(1:3, 4) #&gt; [1] 4 4 4 Good names would include recycle() (R’s name for this behavior) or expand(). Exercise 19.3.2 Take a function that you’ve written recently and spend 5 minutes brainstorming a better name for it and its arguments. Answer left to the reader. Exercise 19.3.3 Compare and contrast rnorm() and MASS::mvrnorm(). How could you make them more consistent? rnorm() samples from the univariate normal distribution, while MASS::mvrnorm samples from the multivariate normal distribution. The main arguments in rnorm() are n, mean, sd. The main arguments is MASS::mvrnorm are n, mu, Sigma. To be consistent they should have the same names. However, this is difficult. In general, it is better to be consistent with more widely used functions, e.g. rmvnorm() should follow the conventions of rnorm(). However, while mean is correct in the multivariate case, sd does not make sense in the multivariate case. However, both functions are internally consistent. It would not be good practice to have mu and sd as arguments or mean and Sigma as arguments. Exercise 19.3.4 Make a case for why norm_r(), norm_d() etc would be better than rnorm(), dnorm(). Make a case for the opposite. If named norm_r() and norm_d(), the naming convention groups functions by their distribution. If named rnorm(), and dnorm(), the naming convention groups functions by the action they perform. r* functions always sample from distributions: for example, rnorm(), rbinom(), runif(), and rexp(). d* functions calculate the probability density or mass of a distribution: For example, dnorm(), dbinom(), dunif(), and dexp(). R distributions use this latter naming convention. 19.4 Conditional execution Exercise 19.4.1 What’s the difference between if and ifelse()? &gt; Carefully read the help and construct three examples that illustrate the key differences. The keyword if tests a single condition, while ifelse() tests each element. Exercise 19.4.2 Write a greeting function that says “good morning”, “good afternoon”, or “good evening”, depending on the time of day. (Hint: use a time argument that defaults to lubridate::now(). That will make it easier to test your function.) greet &lt;- function(time = lubridate::now()) { hr &lt;- lubridate::hour(time) # I don&#39;t know what to do about times after midnight, # are they evening or morning? if (hr &lt; 12) { print(&quot;good morning&quot;) } else if (hr &lt; 17) { print(&quot;good afternoon&quot;) } else { print(&quot;good evening&quot;) } } greet() #&gt; [1] &quot;good morning&quot; greet(ymd_h(&quot;2017-01-08:05&quot;)) #&gt; [1] &quot;good morning&quot; greet(ymd_h(&quot;2017-01-08:13&quot;)) #&gt; [1] &quot;good afternoon&quot; greet(ymd_h(&quot;2017-01-08:20&quot;)) #&gt; [1] &quot;good evening&quot; Exercise 19.4.3 Implement a fizzbuzz() function. It takes a single number as input. If the number is divisible by three, it returns “fizz”. If it’s divisible by five it returns “buzz”. If it’s divisible by three and five, it returns “fizzbuzz”. Otherwise, it returns the number. Make sure you first write working code before you create the function. We can use modulo operator, %%, to check divisibility. The expression x %% y returns 0 if y divides x. 1:10 %% 3 == 0 #&gt; [1] FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE TRUE FALSE A more concise way of checking for divisibility is to note that the not operator will return TRUE for 0, and FALSE for all non-zero numbers. Thus, !(x %% y), will check whether y divides x. !(1:10 %% 3) #&gt; [1] FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE TRUE FALSE There are four cases to consider: If x is divisible by 3 and 5, then return “fizzbuzz”. If x is divisible by 3 and not 5, then return “fizz”. If x is divisible by 5 and not 3, then return “buzz”. Otherwise, which is the case in which x is not divisible by either 3 or 5, return x. The key to answering this question correctly, is to first check whether x is divisible by both 3 and 5. If the function checks whether x is divisible by 3 or 5 before considering the case that the number is divisible by both, then the function will never return &quot;fizzbuzz&quot;. fizzbuzz &lt;- function(x) { # these two lines check that x is a valid input stopifnot(length(x) == 1) stopifnot(is.numeric(x)) if (!(x %% 3) &amp;&amp; !(x %% 5)) { &quot;fizzbuzz&quot; } else if (!(x %% 3)) { &quot;fizz&quot; } else if (!(x %% 5)) { &quot;buzz&quot; } else { # ensure that the function returns a character vector as.character(x) } } fizzbuzz(6) #&gt; [1] &quot;fizz&quot; fizzbuzz(10) #&gt; [1] &quot;buzz&quot; fizzbuzz(15) #&gt; [1] &quot;fizzbuzz&quot; fizzbuzz(2) #&gt; [1] &quot;2&quot; This function can be slightly improved by combining the first two lines conditions so we only check whether x is divisible by 3 once. fizzbuzz2 &lt;- function(x) { # these two lines check that x is a valid input stopifnot(length(x) == 1) stopifnot(is.numeric(x)) if (!(x %% 3)) { if (!(x %% 5)) { &quot;fizzbuzz&quot; } else { &quot;fizz&quot; } } else if (!(x %% 5)) { &quot;buzz&quot; } else { # ensure that the function returns a character vector as.character(x) } } fizzbuzz2(6) #&gt; [1] &quot;fizz&quot; fizzbuzz2(10) #&gt; [1] &quot;buzz&quot; fizzbuzz2(15) #&gt; [1] &quot;fizzbuzz&quot; fizzbuzz2(2) #&gt; [1] &quot;2&quot; Instead of only accepting one number as an input, we could a FizzBuzz function that works on a vector. The case_when() function vectorizes multiple if-else conditions, so is perfect for this task. In fact, fizz-buzz is used in the examples in the documentation of case_when(). fizzbuzz_vec &lt;- function(x) { case_when(!(x %% 3) &amp; !(x %% 5) ~ &quot;fizzbuzz&quot;, !(x %% 3) ~ &quot;fizz&quot;, !(x %% 5) ~ &quot;buzz&quot;, TRUE ~ as.character(x) ) } fizzbuzz_vec(c(0, 1, 2, 3, 5, 9, 10, 12, 15)) #&gt; [1] &quot;fizzbuzz&quot; &quot;1&quot; &quot;2&quot; &quot;fizz&quot; &quot;buzz&quot; &quot;fizz&quot; &quot;buzz&quot; #&gt; [8] &quot;fizz&quot; &quot;fizzbuzz&quot; The following function is an example of a vectorized FizzBuzz function that only uses bracket assignment. fizzbuzz_vec2 &lt;- function(x) { y &lt;- as.character(x) # put the individual cases first - any elements divisible by both 3 and 5 # will be overwritten with fizzbuzz later y[!(x %% 3)] &lt;- &quot;fizz&quot; y[!(x %% 3)] &lt;- &quot;buzz&quot; y[!(x %% 3) &amp; !(x %% 5)] &lt;- &quot;fizzbuzz&quot; y } fizzbuzz_vec2(c(0, 1, 2, 3, 5, 9, 10, 12, 15)) #&gt; [1] &quot;fizzbuzz&quot; &quot;1&quot; &quot;2&quot; &quot;buzz&quot; &quot;5&quot; &quot;buzz&quot; &quot;10&quot; #&gt; [8] &quot;buzz&quot; &quot;fizzbuzz&quot; This question, called the “Fizz Buzz” question, is a common programming interview question used for screening out programmers who can’t program.[^fizzbuzz] Exercise 19.4.4 How could you use cut() to simplify this set of nested if-else statements? if (temp &lt;= 0) { &quot;freezing&quot; } else if (temp &lt;= 10) { &quot;cold&quot; } else if (temp &lt;= 20) { &quot;cool&quot; } else if (temp &lt;= 30) { &quot;warm&quot; } else { &quot;hot&quot; } How would you change the call to cut() if I’d used &lt; instead of &lt;=? What is the other chief advantage of cut() for this problem? (Hint: what happens if you have many values in temp?) temp &lt;- seq(-10, 50, by = 5) cut(temp, c(-Inf, 0, 10, 20, 30, Inf), right = TRUE, labels = c(&quot;freezing&quot;, &quot;cold&quot;, &quot;cool&quot;, &quot;warm&quot;, &quot;hot&quot;) ) #&gt; [1] freezing freezing freezing cold cold cool cool warm #&gt; [9] warm hot hot hot hot #&gt; Levels: freezing cold cool warm hot To have intervals open on the left (using &lt;), I change the argument to right = FALSE, temp &lt;- seq(-10, 50, by = 5) cut(temp, c(-Inf, 0, 10, 20, 30, Inf), right = FALSE, labels = c(&quot;freezing&quot;, &quot;cold&quot;, &quot;cool&quot;, &quot;warm&quot;, &quot;hot&quot;) ) #&gt; [1] freezing freezing cold cold cool cool warm warm #&gt; [9] hot hot hot hot hot #&gt; Levels: freezing cold cool warm hot Two advantages of using cut is that it works on vectors, whereas if only works on a single value (I already demonstrated this above), and that to change comparisons I only needed to change the argument to right, but I would have had to change four operators in the if expression. Exercise 19.4.5 What happens if you use switch() with numeric values? In switch(n, ...), if n is numeric, it will return the nth argument from .... This means that if n = 1, switch() will return the first argument in ..., if n = 2, the second, and so on. For example, switch(1, &quot;apple&quot;, &quot;banana&quot;, &quot;cantaloupe&quot;) #&gt; [1] &quot;apple&quot; switch(2, &quot;apple&quot;, &quot;banana&quot;, &quot;cantaloupe&quot;) #&gt; [1] &quot;banana&quot; If you use a non-integer number for the first argument of switch(), it will ignore the non-integer part. switch(1.2, &quot;apple&quot;, &quot;banana&quot;, &quot;cantaloupe&quot;) #&gt; [1] &quot;apple&quot; switch(2.8, &quot;apple&quot;, &quot;banana&quot;, &quot;cantaloupe&quot;) #&gt; [1] &quot;banana&quot; Note that switch() truncates the numeric value, it does not round to the nearest integer. While it is possible to use non-integer numbers with switch(), you should avoid it Exercise 19.4.6 What does this switch() call do? What happens if x is &quot;e&quot;? x &lt;- &quot;e&quot; switch(x, a = , b = &quot;ab&quot;, c = , d = &quot;cd&quot; ) Experiment, then carefully read the documentation. First, let’s write a function switcheroo(), and see what it returns for different values of x. switcheroo &lt;- function(x) { switch(x, a = , b = &quot;ab&quot;, c = , d = &quot;cd&quot; ) } switcheroo(&quot;a&quot;) #&gt; [1] &quot;ab&quot; switcheroo(&quot;b&quot;) #&gt; [1] &quot;ab&quot; switcheroo(&quot;c&quot;) #&gt; [1] &quot;cd&quot; switcheroo(&quot;d&quot;) #&gt; [1] &quot;cd&quot; switcheroo(&quot;e&quot;) switcheroo(&quot;f&quot;) The switcheroo() function returns &quot;ab&quot; for x = &quot;a&quot; or x = &quot;b&quot;, &quot;cd&quot; for x = &quot;c&quot; or x = &quot;d&quot;, and NULL for x = &quot;e&quot; or any other value of x not in c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;). How does this work? The switch() function returns the first non-missing argument value for the first name it matches. Thus, when switch() encounters an argument with a missing value, like a = ,, it will return the value of the next argument with a non missing value, which in this case is b = &quot;ab&quot;. If object in switch(object=) is not equal to the names of any of its arguments, switch() will return either the last (unnamed) argument if one is present or NULL. Since &quot;e&quot; is not one of the named arguments in switch() (a, b, c, d), and no other unnamed default value is present, this code will return NULL. The code in the question is shorter way of writing the following. switch(x, a = &quot;ab&quot;, b = &quot;ab&quot;, c = &quot;cd&quot;, d = &quot;cd&quot;, NULL # value to return if x not matched ) 19.5 Function arguments Exercise 19.5.1 What does commas(letters, collapse = &quot;-&quot;) do? Why? The commas() function in the chapter is defined as commas &lt;- function(...) { str_c(..., collapse = &quot;, &quot;) } When commas() is given a collapse argument, it throws an error. commas(letters, collapse = &quot;-&quot;) #&gt; Error in str_c(..., collapse = &quot;, &quot;): formal argument &quot;collapse&quot; matched by multiple actual arguments This is because when the argument collapse is given to commas(), it is passed to str_c() as part of .... In other words, the previous code is equivalent to str_c(letters, collapse = &quot;-&quot;, collapse = &quot;, &quot;) However, it is an error to give the same named argument to a function twice. One way to allow the user to override the separator in commas() is to add a collapse argument to the function. commas &lt;- function(..., collapse = &quot;, &quot;) { str_c(..., collapse = collapse) } Exercise 19.5.2 It’d be nice if you could supply multiple characters to the pad argument, e.g. rule(&quot;Title&quot;, pad = &quot;-+&quot;). Why doesn’t this currently work? How could you fix it? This is the definition of the rule function from the chapter. rule &lt;- function(..., pad = &quot;-&quot;) { title &lt;- paste0(...) width &lt;- getOption(&quot;width&quot;) - nchar(title) - 5 cat(title, &quot; &quot;, str_dup(pad, width), &quot;\\n&quot;, sep = &quot;&quot;) } rule(&quot;Important output&quot;) #&gt; Important output ----------------------------------------------------------- You can currently supply multiple characters to the pad argument, but the output will not be the desired width. The rule() function duplicates pad a number of times equal to the desired width minus the length of the title and five extra characters. This implicitly assumes that pad is only one character. If pad were two character, the output will be almost twice as long. rule(&quot;Valuable output&quot;, pad = &quot;-+&quot;) #&gt; Valuable output -+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ One way to handle this is to use str_trunc() to truncate the string, and str_length() to calculate the number of characters in the pad argument. rule &lt;- function(..., pad = &quot;-&quot;) { title &lt;- paste0(...) width &lt;- getOption(&quot;width&quot;) - nchar(title) - 5 padding &lt;- str_dup( pad, ceiling(width / str_length(title)) ) %&gt;% str_trunc(width) cat(title, &quot; &quot;, padding, &quot;\\n&quot;, sep = &quot;&quot;) } rule(&quot;Important output&quot;) #&gt; Important output ---- rule(&quot;Valuable output&quot;, pad = &quot;-+&quot;) #&gt; Valuable output -+-+-+-+ rule(&quot;Vital output&quot;, pad = &quot;-+-&quot;) #&gt; Vital output -+--+--+--+--+--+- Note that in the second output, there is only a single - at the end. Exercise 19.5.3 What does the trim argument to mean() do? When might you use it? The trim arguments trims a fraction of observations from each end of the vector (meaning the range) before calculating the mean. This is useful for calculating a measure of central tendency that is robust to outliers. Exercise 19.5.4 The default value for the method argument to cor() is c(&quot;pearson&quot;, &quot;kendall&quot;, &quot;spearman&quot;). What does that mean? What value is used by default? It means that the method argument can take one of those three values. The first value, &quot;pearson&quot;, is used by default. 19.6 Return values No exercises 19.7 Environment No exercises [fizzbuzz]: Read Why I’m still using “Fizz Buzz” to hire Software-Developers for more discussion on the use of the Fizz-Buzz question in programming interviews. "],
["vectors.html", "20 Vectors 20.1 Introduction 20.2 Vector basics 20.3 Important types of atomic vector 20.4 Using atomic vectors 20.5 Recursive vectors (lists) 20.6 Attributes 20.7 Augmented vectors", " 20 Vectors 20.1 Introduction library(&quot;tidyverse&quot;) 20.2 Vector basics No exercises 20.3 Important types of atomic vector Exercise 20.3.1 Describe the difference between is.finite(x) and !is.infinite(x). To find out, try the functions on a numeric vector that includes at least one number and the four special values (NA, NaN, Inf, -Inf). x &lt;- c(0, NA, NaN, Inf, -Inf) is.finite(x) #&gt; [1] TRUE FALSE FALSE FALSE FALSE !is.infinite(x) #&gt; [1] TRUE TRUE TRUE FALSE FALSE The is.finite() function considers non-missing numeric values to be finite, and missing (NA), not a number (NaN), and positive (Inf) and negative infinity (-Inf) to not be finite. The is.infinite() behaves slightly differently. It considers Inf and -Inf to be infinite, and everything else, including non-missing numbers, NA, and NaN to not be infinite. See Table 20.1. Table 20.1: Results of is.finite() and is.infinite() for numeric and special values. is.finite() is.infinite() 1 TRUE FALSE NA FALSE FALSE NaN FALSE FALSE Inf FALSE TRUE Exercise 20.3.2 Read the source code for dplyr::near() (Hint: to see the source code, drop the ()). How does it work? The source for dplyr::near is: dplyr::near #&gt; function (x, y, tol = .Machine$double.eps^0.5) #&gt; { #&gt; abs(x - y) &lt; tol #&gt; } #&gt; &lt;bytecode: 0x5d8e8a8&gt; #&gt; &lt;environment: namespace:dplyr&gt; Instead of checking for exact equality, it checks that two numbers are within a certain tolerance, tol. By default the tolerance is set to the square root of .Machine$double.eps, which is the smallest floating point number that the computer can represent. Exercise 20.3.3 A logical vector can take 3 possible values. How many possible values can an integer vector take? How many possible values can a double take? Use Google to do some research. For integers vectors, R uses a 32-bit representation. This means that it can represent up to \\(2^{32}\\) different values with integers. One of these values is set aside for NA_integer_. From the help for integer. Note that current implementations of R use 32-bit integers for integer vectors, so the range of representable integers is restricted to about +/-2*10^9: doubles can hold much larger integers exactly. The range of integers values that R can represent in an integer vector is \\(\\pm 2^{31} - 1\\), .Machine$integer.max #&gt; [1] 2147483647 The maximum integer is \\(2^{31} - 1\\) rather than \\(2^{32}\\) because 1 bit is used to represent the sign (\\(+\\), \\(-\\)) and one value is used to represent NA_integer_. If you try to represent an integer greater than that value, R will return NA values. .Machine$integer.max + 1L #&gt; Warning in .Machine$integer.max + 1L: NAs produced by integer overflow #&gt; [1] NA However, you can represent that value (exactly) with a numeric vector at the cost of about two times the memory. as.numeric(.Machine$integer.max) + 1 #&gt; [1] 2.15e+09 The same is true for the negative of the integer max. -.Machine$integer.max - 1L #&gt; Warning in -.Machine$integer.max - 1L: NAs produced by integer overflow #&gt; [1] NA For double vectors, R uses a 64-bit representation. This means that they can hold up to \\(2^{64}\\) values exactly. However, some of those values are allocated to special values such as -Inf, Inf, NA_real_, and NaN. From the help for double: All R platforms are required to work with values conforming to the IEC 60559 (also known as IEEE 754) standard. This basically works with a precision of 53 bits, and represents to that precision a range of absolute values from about 2e-308 to 2e+308. It also has special values NaN (many of them), plus and minus infinity and plus and minus zero (although R acts as if these are the same). There are also denormal(ized) (or subnormal) numbers with absolute values above or below the range given above but represented to less precision. The details of floating point representation and arithmetic are complicated, beyond the scope of this question, and better discussed in the references provided below. The double can represent numbers in the range of about \\(\\pm 2 \\times 10^{308}\\), which is provided in .Machine$double.xmax #&gt; [1] 1.8e+308 Many other details for the implementation of the double vectors are given in the .Machine variable (and its documentation). These include the base (radix) of doubles, .Machine$double.base #&gt; [1] 2 the number of bits used for the significand (mantissa), .Machine$double.digits #&gt; [1] 53 the number of bits used in the exponent, .Machine$double.exponent #&gt; [1] 11 and the smallest positive and negative numbers not equal to zero, .Machine$double.eps #&gt; [1] 2.22e-16 .Machine$double.neg.eps #&gt; [1] 1.11e-16 Computerphile, “Floating Point Numbers” https://en.wikipedia.org/wiki/IEEE_754 https://en.wikipedia.org/wiki/Double-precision_floating-point_format “Floating Point Numbers: Why floating-point numbers are needed” Fabien Sanglard, “Floating Point Numbers: Visually Explained” James Howard, “How Many Floating Point Numbers are There?” GeeksforGeeks, “Floating Point Representation Basics” Chris Hecker, “Lets Go to the (Floating) Point”, Game Developer Chua Hock-Chuan, A Tutorial on Data Representation Integers, Floating-point Numbers, and Characters John D. Cook, “Anatomy of a floating point number” John D. Cook, “Five Tips for Floating Point Programming” Exercise 20.3.4 Brainstorm at least four functions that allow you to convert a double to an integer. How do they differ? Be precise. The difference between to convert a double to an integer differ in how they deal with the fractional part of the double. There are are a variety of rules that could be used to do this. Round down, towards \\(-\\infty\\). This is also called taking the floor of a number. This is the method the floor() function uses. Round up, towards \\(+\\infty\\). This is also called taking the ceiling. This is the method the ceiling() function uses. Round towards zero. This is the method that the trunc() and as.integer() functions use. Round away from zero. Round to the nearest integer. There several different methods for handling ties, defined as numbers with a fractional part of 0.5. Round half down, towards \\(-\\infty\\). Round half up, towards \\(+\\infty\\). Round half towards zero Round half away from zero Round half towards the even integer. This is the method that the round() function uses. Round half towards the odd integer. function(x, method) { if (method == &quot;round down&quot;) { floor(x) } else if (method == &quot;round up&quot;) { ceiling(x) } else if (method == &quot;round towards zero&quot;) { trunc(x) } else if (method == &quot;round away from zero&quot;) { sign(x) * ceiling(abs(x)) } else if (method == &quot;nearest, round half up&quot;) { floor(x + 0.5) } else if (method == &quot;nearest, round half down&quot;) { ceiling(x - 0.5) } else if (method == &quot;nearest, round half towards zero&quot;) { sign(x) * ceiling(abs(x) - 0.5) } else if (method == &quot;nearest, round half away from zero&quot;) { sign(x) * floor(abs(x) + 0.5) } else if (method == &quot;nearest, round half to even&quot;) { round(x, digits = 0) } else if (method == &quot;nearest, round half to odd&quot;) { case_when( # smaller integer is odd - round half down floor(x) %% 2 ~ ceiling(x - 0.5), # otherwise, round half up TRUE ~ floor(x + 0.5) ) } else if (method == &quot;nearest, round half randomly&quot;) { round_half_up &lt;- sample(c(TRUE, FALSE), length(x), replace = TRUE) y &lt;- x y[round_half_up] &lt;- ceiling(x[round_half_up] - 0.5) y[!round_half_up] &lt;- floor(x[!round_half_up] + 0.5) y } } #&gt; function(x, method) { #&gt; if (method == &quot;round down&quot;) { #&gt; floor(x) #&gt; } else if (method == &quot;round up&quot;) { #&gt; ceiling(x) #&gt; } else if (method == &quot;round towards zero&quot;) { #&gt; trunc(x) #&gt; } else if (method == &quot;round away from zero&quot;) { #&gt; sign(x) * ceiling(abs(x)) #&gt; } else if (method == &quot;nearest, round half up&quot;) { #&gt; floor(x + 0.5) #&gt; } else if (method == &quot;nearest, round half down&quot;) { #&gt; ceiling(x - 0.5) #&gt; } else if (method == &quot;nearest, round half towards zero&quot;) { #&gt; sign(x) * ceiling(abs(x) - 0.5) #&gt; } else if (method == &quot;nearest, round half away from zero&quot;) { #&gt; sign(x) * floor(abs(x) + 0.5) #&gt; } else if (method == &quot;nearest, round half to even&quot;) { #&gt; round(x, digits = 0) #&gt; } else if (method == &quot;nearest, round half to odd&quot;) { #&gt; case_when( #&gt; # smaller integer is odd - round half down #&gt; floor(x) %% 2 ~ ceiling(x - 0.5), #&gt; # otherwise, round half up #&gt; TRUE ~ floor(x + 0.5) #&gt; ) #&gt; } else if (method == &quot;nearest, round half randomly&quot;) { #&gt; round_half_up &lt;- sample(c(TRUE, FALSE), length(x), replace = TRUE) #&gt; y &lt;- x #&gt; y[round_half_up] &lt;- ceiling(x[round_half_up] - 0.5) #&gt; y[!round_half_up] &lt;- floor(x[!round_half_up] + 0.5) #&gt; y #&gt; } #&gt; } #&gt; &lt;environment: 0x2b114b8&gt; tibble( x = c(1.8, 1.5, 1.2, 0.8, 0.5, 0.2, -0.2, -0.5, -0.8, -1.2, -1.5, -1.8), `Round down` = floor(x), `Round up` = ceiling(x), `Round towards zero` = trunc(x), `Nearest, round half to even` = round(x) ) #&gt; # A tibble: 12 x 5 #&gt; x `Round down` `Round up` `Round towards zero` `Nearest, round half to ev… #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1.8 1 2 1 2 #&gt; 2 1.5 1 2 1 2 #&gt; 3 1.2 1 2 1 1 #&gt; 4 0.8 0 1 0 1 #&gt; 5 0.5 0 1 0 0 #&gt; 6 0.2 0 1 0 0 #&gt; # … with 6 more rows See the Wikipedia articles, Rounding and IEEE floating point for more discussion of these rounding rules. For rounding, R and many programming languages use the IEEE standard. This method is called “round to nearest, ties to even.”8 This rule rounds ties, numbers with a remainder of 0.5, to the nearest even number. In this rule, half the ties are rounded up, and half are rounded down. The following function, round2(), manually implements the “round to nearest, ties to even” method. x &lt;- seq(-10, 10, by = 0.5) round2 &lt;- function(x, to_even = TRUE) { q &lt;- x %/% 1 r &lt;- x %% 1 q + (r &gt;= 0.5) } x &lt;- c(-12.5, -11.5, 11.5, 12.5) round(x) #&gt; [1] -12 -12 12 12 round2(x, to_even = FALSE) #&gt; [1] -12 -11 12 13 This rounding method may be different than the one you learned in grade school, which is, at least for me, was to always round ties upwards, or, alternatively away from zero. This rule is called the “round half up” rule. The problem with the “round half up” rule is that it is biased upwards for positive numbers. Rounding to nearest with ties towards even is not. Consider this sequence which sums to zero. x &lt;- seq(-100.5, 100.5, by = 1) x #&gt; [1] -100.5 -99.5 -98.5 -97.5 -96.5 -95.5 -94.5 -93.5 -92.5 -91.5 #&gt; [11] -90.5 -89.5 -88.5 -87.5 -86.5 -85.5 -84.5 -83.5 -82.5 -81.5 #&gt; [21] -80.5 -79.5 -78.5 -77.5 -76.5 -75.5 -74.5 -73.5 -72.5 -71.5 #&gt; [31] -70.5 -69.5 -68.5 -67.5 -66.5 -65.5 -64.5 -63.5 -62.5 -61.5 #&gt; [41] -60.5 -59.5 -58.5 -57.5 -56.5 -55.5 -54.5 -53.5 -52.5 -51.5 #&gt; [51] -50.5 -49.5 -48.5 -47.5 -46.5 -45.5 -44.5 -43.5 -42.5 -41.5 #&gt; [61] -40.5 -39.5 -38.5 -37.5 -36.5 -35.5 -34.5 -33.5 -32.5 -31.5 #&gt; [71] -30.5 -29.5 -28.5 -27.5 -26.5 -25.5 -24.5 -23.5 -22.5 -21.5 #&gt; [81] -20.5 -19.5 -18.5 -17.5 -16.5 -15.5 -14.5 -13.5 -12.5 -11.5 #&gt; [91] -10.5 -9.5 -8.5 -7.5 -6.5 -5.5 -4.5 -3.5 -2.5 -1.5 #&gt; [101] -0.5 0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 #&gt; [111] 9.5 10.5 11.5 12.5 13.5 14.5 15.5 16.5 17.5 18.5 #&gt; [121] 19.5 20.5 21.5 22.5 23.5 24.5 25.5 26.5 27.5 28.5 #&gt; [131] 29.5 30.5 31.5 32.5 33.5 34.5 35.5 36.5 37.5 38.5 #&gt; [141] 39.5 40.5 41.5 42.5 43.5 44.5 45.5 46.5 47.5 48.5 #&gt; [151] 49.5 50.5 51.5 52.5 53.5 54.5 55.5 56.5 57.5 58.5 #&gt; [161] 59.5 60.5 61.5 62.5 63.5 64.5 65.5 66.5 67.5 68.5 #&gt; [171] 69.5 70.5 71.5 72.5 73.5 74.5 75.5 76.5 77.5 78.5 #&gt; [181] 79.5 80.5 81.5 82.5 83.5 84.5 85.5 86.5 87.5 88.5 #&gt; [191] 89.5 90.5 91.5 92.5 93.5 94.5 95.5 96.5 97.5 98.5 #&gt; [201] 99.5 100.5 sum(x) #&gt; [1] 0 A nice property of rounding preserved that sum. Using the “ties towards even”, the sum is still zero. However, the “ties towards \\(+\\infty\\)” produces a non-zero number. sum(x) #&gt; [1] 0 sum(round(x)) #&gt; [1] 0 sum(round2(x)) #&gt; [1] 101 Rounding rules can have real world impacts. One notable example was that in 1983, the Vancouver stock exchange adjusted its index from 524.811 to 1098.892 to correct for accumulated error due to rounding to three decimal points (see Vancouver Stock Exchange). This site lists several more examples of the dangers of rounding rules. Exercise 20.3.5 What functions from the readr package allow you to turn a string into logical, integer, and double vector? The function parse_logical() parses logical values, which can appear as variations of TRUE/FALSE or 1/0. parse_logical(c(&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;1&quot;, &quot;0&quot;, &quot;true&quot;, &quot;t&quot;, &quot;NA&quot;)) #&gt; [1] TRUE FALSE TRUE FALSE TRUE TRUE NA The function parse_integer() parses integer values. parse_integer(c(&quot;1235&quot;, &quot;0134&quot;, &quot;NA&quot;)) #&gt; [1] 1235 134 NA However, if there are any non-numeric characters in the string, including currency symbols, commas, and decimals, parse_integer() will raise an error. parse_integer(c(&quot;1000&quot;, &quot;$1,000&quot;, &quot;10.00&quot;)) #&gt; Warning: 2 parsing failures. #&gt; row col expected actual #&gt; 2 -- an integer $1,000 #&gt; 3 -- no trailing characters .00 #&gt; [1] 1000 NA NA #&gt; attr(,&quot;problems&quot;) #&gt; # A tibble: 2 x 4 #&gt; row col expected actual #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2 NA an integer $1,000 #&gt; 2 3 NA no trailing characters .00 The function parse_number() parses numeric values. Unlike parse_integer(), the function parse_number() is more forgiving about the format of the numbers. It ignores all non-numeric characters before or after the first number, as with &quot;$1,000.00&quot; in the example. Within the number, parse_number() will only ignore grouping marks such as &quot;,&quot;. This allows it to easily parse numeric fields that include currency symbols and comma separators in number strings without any intervention by the user. parse_number(c(&quot;1.0&quot;, &quot;3.5&quot;, &quot;$1,000.00&quot;, &quot;NA&quot;, &quot;ABCD12234.90&quot;, &quot;1234ABC&quot;, &quot;A123B&quot;, &quot;A1B2C&quot;)) #&gt; [1] 1.0 3.5 1000.0 NA 12234.9 1234.0 123.0 1.0 20.4 Using atomic vectors Exercise 20.4.1 What does mean(is.na(x)) tell you about a vector x? What about sum(!is.finite(x))? I’ll use the numeric vector x to compare the behaviors of is.na() and is.finite(). It contains numbers (-1, 0, 1) as well as all the special numeric values: infinity (Inf), missing (NA), and not-a-number (NaN). x &lt;- c(-Inf, -1, 0, 1, Inf, NA, NaN) The expression mean(is.na(x)) calculates the proportion of missing (NA) and not-a-number NaN values in a vector: mean(is.na(x)) #&gt; [1] 0.286 The result of 0.286 is equal to 2 / 7 as expected. There are seven elements in the vector x, and two elements that are either NA or NaN. The expression sum(!is.finite(x)) calculates the number of elements in the vector that are equal to missing (NA), not-a-number (NaN), or infinity (Inf). sum(!is.finite(x)) #&gt; [1] 4 Review the Numeric section for the differences between is.na() and is.finite(). Exercise 20.4.2 Carefully read the documentation of is.vector(). What does it actually test for? Why does is.atomic() not agree with the definition of atomic vectors above? The function is.vector() only checks whether the object has no attributes other than names. Thus a list is a vector: is.vector(list(a = 1, b = 2)) #&gt; [1] TRUE But any object that has an attribute (other than names) is not: x &lt;- 1:10 attr(x, &quot;something&quot;) &lt;- TRUE is.vector(x) #&gt; [1] FALSE The idea behind this is that object oriented classes will include attributes, including, but not limited to &quot;class&quot;. The function is.atomic() explicitly checks whether an object is one of the atomic types (“logical”, “integer”, “numeric”, “complex”, “character”, and “raw”) or NULL. is.atomic(1:10) #&gt; [1] TRUE is.atomic(list(a = 1)) #&gt; [1] FALSE The function is.atomic() will consider objects to be atomic even if they have extra attributes. is.atomic(x) #&gt; [1] TRUE Exercise 20.4.3 Compare and contrast setNames() with purrr::set_names(). The function setNames() takes two arguments, a vector to be named and a vector of names to apply to its elements. setNames(1:4, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) #&gt; a b c d #&gt; 1 2 3 4 You can use the values of the vector as its names if the nm argument is used. setNames(nm = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) #&gt; a b c d #&gt; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; The function set_names() has more ways to set the names than setNames(). The names can be specified in the same manner as setNames(). purrr::set_names(1:4, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) #&gt; a b c d #&gt; 1 2 3 4 The names can also be specified as unnamed arguments, purrr::set_names(1:4, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) #&gt; a b c d #&gt; 1 2 3 4 The function set_names() will name an object with itself if no nm argument is provided (the opposite of setNames() behavior). purrr::set_names(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) #&gt; a b c d #&gt; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; The biggest difference between set_names() and setNames() is that set_names() allows for using a function or formula to transform the existing names. purrr::set_names(c(a = 1, b = 2, c = 3), toupper) #&gt; A B C #&gt; 1 2 3 purrr::set_names(c(a = 1, b = 2, c = 3), ~toupper(.)) #&gt; A B C #&gt; 1 2 3 The set_names() function also checks that the length of the names argument is the same length as the vector that is being named, and will raise an error if it is not. purrr::set_names(1:4, c(&quot;a&quot;, &quot;b&quot;)) #&gt; Error: `nm` must be `NULL` or a character vector the same length as `x` The setNames() function will allow the names to be shorter than the vector being named, and will set the missing names to NA. setNames(1:4, c(&quot;a&quot;, &quot;b&quot;)) #&gt; a b &lt;NA&gt; &lt;NA&gt; #&gt; 1 2 3 4 Exercise 20.4.4 Create functions that take a vector as input and returns: The last value. Should you use [ or [[? The elements at even numbered positions. Every element except the last value. Only even numbers (and no missing values). The answers to the parts follow. This function find the last value in a vector. last_value &lt;- function(x) { # check for case with no length if (length(x)) { x[[length(x)]] } else { x } } last_value(numeric()) #&gt; numeric(0) last_value(1) #&gt; [1] 1 last_value(1:10) #&gt; [1] 10 The function uses [[ in order to extract a single element. This function returns the elements at even number positions. even_indices &lt;- function(x) { if (length(x)) { x[seq_along(x) %% 2 == 0] } else { x } } even_indices(numeric()) #&gt; numeric(0) even_indices(1) #&gt; numeric(0) even_indices(1:10) #&gt; [1] 2 4 6 8 10 # test using case to ensure that values not indices # are being returned even_indices(letters) #&gt; [1] &quot;b&quot; &quot;d&quot; &quot;f&quot; &quot;h&quot; &quot;j&quot; &quot;l&quot; &quot;n&quot; &quot;p&quot; &quot;r&quot; &quot;t&quot; &quot;v&quot; &quot;x&quot; &quot;z&quot; This function returns a vector with every element except the last. not_last &lt;- function(x) { n &lt;- length(x) if (n) { x[-n] } else { # n == 0 x } } not_last(1:3) #&gt; [1] 1 2 We should also confirm that the function works with some edge cases, like a vector with one element, and a vector with zero elements. not_last(1) #&gt; numeric(0) not_last(numeric()) #&gt; numeric(0) In both these cases, not_last() correctly returns an empty vector. This function returns the elements of a vector that are even numbers. even_numbers &lt;- function(x) { x[x %% 2 == 0] } even_numbers(-4:4) #&gt; [1] -4 -2 0 2 4 We could improve this function by handling the special numeric values: NA, NaN, Inf. However, first we need to decide how to handle them. Neither NaN nor Inf are numbers, and so they are neither even nor odd. In other words, since NaN nor Inf aren’t even numbers, they aren’t even numbers. What about NA? Well, we don’t know. NA is a number, but we don’t know its value. The missing number could be even or odd, but we don’t know. Another reason to return NA is that it is consistent with the behavior of other R functions, which generally return NA values instead of dropping them. even_numbers2 &lt;- function(x) { x[!is.infinite(x) &amp; !is.nan(x) &amp; (x %% 2 == 0)] } even_numbers2(c(0:4, NA, NaN, Inf, -Inf)) #&gt; [1] 0 2 4 NA Exercise 20.4.5 Why is x[-which(x &gt; 0)] not the same as x[x &lt;= 0]? These expressions differ in the way that they treat missing values. Let’s test how they work by creating a vector with positive and negative integers, and special values (NA, NaN, and Inf). These values should encompass all relevant types of values that these expressions would encounter. x &lt;- c(-1:1, Inf, -Inf, NaN, NA) x[-which(x &gt; 0)] #&gt; [1] -1 0 -Inf NaN NA x[x &lt;= 0] #&gt; [1] -1 0 -Inf NA NA The expressions x[-which(x &gt; 0)] and x[x &lt;= 0] return the same values except for a NaN instead of an NA in the expression using which. So what is going on here? Let’s work through each part of these expressions and see where the different occurs. Let’s start with the expression x[x &lt;= 0]. x &lt;= 0 #&gt; [1] TRUE TRUE FALSE FALSE TRUE NA NA Recall how the logical relational operators (&lt;, &lt;=, ==, !=, &gt;, &gt;=) treat NA values. Any relational operation that includes a NA returns an NA. Is NA &lt;= 0? We don’t know because it depends on the unknown value of NA, so the answer is NA. This same argument applies to NaN. Asking whether NaN &lt;= 0 does not make sense because you can’t compare a number to “Not a Number”. Now recall how indexing treats NA values. Indexing can take a logical vector as an input. When the indexing vector is logical, the output vector includes those elements where the logical vector is TRUE, and excludes those elements where the logical vector is FALSE. Logical vectors can also include NA values, and it is not clear how they should be treated. Well, since the value is NA, it could be TRUE or FALSE, we don’t know. Keeping elements with NA would treat the NA as TRUE, and dropping them would treat the NA as FALSE. The way R decides to handle the NA values so that they are treated differently than TRUE or FALSE values is to include elements where the indexing vector is NA, but set their values to NA. Now consider the expression x[-which(x &gt; 0)]. As before, to understand this expression we’ll work from the inside out. Consider x &gt; 0. x &gt; 0 #&gt; [1] FALSE FALSE TRUE TRUE FALSE NA NA As with x &lt;= 0, it returns NA for comparisons involving NA and NaN. What does which() do? which(x &gt; 0) #&gt; [1] 3 4 The which() function returns the indexes for which the argument is TRUE. This means that it is not including the indexes for which the argument is FALSE or NA. Now consider the full expression x[-which(x &gt; 0)]? The which() function returned a vector of integers. How does indexing treat negative integers? x[1:2] #&gt; [1] -1 0 x[-(1:2)] #&gt; [1] 1 Inf -Inf NaN NA If indexing gets a vector of positive integers, it will select those indexes; if it receives a vector of negative integers, it will drop those indexes. Thus, x[-which(x &gt; 0)] ends up dropping the elements for which x &gt; 0 is true, and keeps all the other elements and their original values, including NA and NaN. There’s one other special case that we should consider. How do these two expressions work with an empty vector? x &lt;- numeric() x[x &lt;= 0] #&gt; numeric(0) x[-which(x &gt; 0)] #&gt; numeric(0) Thankfully, they both handle empty vectors the same. This exercise is a reminder to always test your code. Even though these two expressions looked equivalent, they are not in practice. And when you do test code, consider both how it works on typical values as well as special values and edge cases, like a vector with NA or NaN or Inf values, or an empty vector. These are where unexpected behavior is most likely to occur. Exercise 20.4.6 What happens when you subset with a positive integer that’s bigger than the length of the vector? What happens when you subset with a name that doesn’t exist? Let’s consider the named vector, x &lt;- c(a = 10, b = 20) If we subset it by an integer larger than its length, it returns a vector of missing values. x[3] #&gt; &lt;NA&gt; #&gt; NA This also applies to ranges. x[3:5] #&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; NA NA NA If some indexes are larger than the length of the vector, those elements are NA. x[1:5] #&gt; a b &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 10 20 NA NA NA Likewise, when [ is provided names not in the vector’s names, it will return NA for those elements. x[&quot;c&quot;] #&gt; &lt;NA&gt; #&gt; NA x[c(&quot;c&quot;, &quot;d&quot;, &quot;e&quot;)] #&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; NA NA NA x[c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)] #&gt; a b &lt;NA&gt; #&gt; 10 20 NA Though not yet discussed much in this chapter, the [[ behaves differently. With an atomic vector, if [[ is given an index outside the range of the vector or an invalid name, it raises an error. x[[&quot;c&quot;]] #&gt; Error in x[[&quot;c&quot;]]: subscript out of bounds x[[5]] #&gt; Error in x[[5]]: subscript out of bounds 20.5 Recursive vectors (lists) Exercise 20.5.1 Draw the following lists as nested sets: list(a, b, list(c, d), list(e, f)) list(list(list(list(list(list(a)))))) There are a variety of ways to draw these graphs. The original diagrams in R for Data Science were produced with Graffle. You could also use various diagramming, drawing, or presentation software, including Adobe Illustrator, Inkscape, PowerPoint, Keynote, and Google Slides. For these examples, I generated these diagrams programmatically using the DiagrammeR R package to render Graphviz diagrams. The nested set diagram for list(a, b, list(c, d), list(e, f)) is:9 The nested set diagram for list(list(list(list(list(list(a)))))) is: Exercise 20.5.2 What happens if you subset a tibble as if you’re subsetting a list? What are the key differences between a list and a tibble? Subsetting a tibble works the same way as a list; a data frame can be thought of as a list of columns. The key difference between a list and a tibble is that all the elements (columns) of a tibble must have the same length (number of rows). Lists can have vectors with different lengths as elements. x &lt;- tibble(a = 1:2, b = 3:4) x[[&quot;a&quot;]] #&gt; [1] 1 2 x[&quot;a&quot;] #&gt; # A tibble: 2 x 1 #&gt; a #&gt; &lt;int&gt; #&gt; 1 1 #&gt; 2 2 x[1] #&gt; # A tibble: 2 x 1 #&gt; a #&gt; &lt;int&gt; #&gt; 1 1 #&gt; 2 2 x[1, ] #&gt; # A tibble: 1 x 2 #&gt; a b #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 3 20.6 Attributes No exercises 20.7 Augmented vectors Exercise 20.7.1 What does hms::hms(3600) return? How does it print? What primitive type is the augmented vector built on top of? What attributes does it use? x &lt;- hms::hms(3600) class(x) #&gt; [1] &quot;hms&quot; &quot;difftime&quot; x #&gt; 01:00:00 hms::hms returns an object of class, and prints the time in “%H:%M:%S” format. The primitive type is a double typeof(x) #&gt; [1] &quot;double&quot; The attributes is uses are &quot;units&quot; and &quot;class&quot;. attributes(x) #&gt; $units #&gt; [1] &quot;secs&quot; #&gt; #&gt; $class #&gt; [1] &quot;hms&quot; &quot;difftime&quot; Exercise 20.7.2 Try and make a tibble that has columns with different lengths. What happens? If I try to create a tibble with a scalar and column of a different length there are no issues, and the scalar is repeated to the length of the longer vector. tibble(x = 1, y = 1:5) #&gt; # A tibble: 5 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 1 #&gt; 2 1 2 #&gt; 3 1 3 #&gt; 4 1 4 #&gt; 5 1 5 However, if I try to create a tibble with two vectors of different lengths (other than one), the tibble function throws an error. tibble(x = 1:3, y = 1:4) #&gt; Error: Tibble columns must have compatible sizes. #&gt; * Size 3: Existing data. #&gt; * Size 4: Column `y`. #&gt; ℹ Only values of size one are recycled. Exercise 20.7.3 Based on the definition above, is it OK to have a list as a column of a tibble? If I didn’t already know the answer, what I would do is try it out. From the above, the error message was about vectors having different lengths. But there is nothing that prevents a tibble from having vectors of different types: doubles, character, integers, logical, factor, date. The later are still atomic, but they have additional attributes. So, maybe there won’t be an issue with a list vector as long as it is the same length. tibble(x = 1:3, y = list(&quot;a&quot;, 1, list(1:3))) #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;list&gt; #&gt; 1 1 &lt;chr [1]&gt; #&gt; 2 2 &lt;dbl [1]&gt; #&gt; 3 3 &lt;list [1]&gt; It works! I even used a list with heterogeneous types and there wasn’t an issue. In following chapters we’ll see that list vectors can be very useful: for example, when processing many different models. See the documentation for .Machine$double.rounding.↩ These diagrams were created with the DiagrammeR package.↩ "],
["iteration.html", "21 Iteration 21.1 Introduction 21.2 For loops 21.3 For loop variations 21.4 For loops vs. functionals 21.5 The map functions 21.6 Dealing with failure 21.7 Mapping over multiple arguments 21.8 Walk 21.9 Other patterns of for loops", " 21 Iteration 21.1 Introduction The microbenchmark package is used for timing code. library(&quot;tidyverse&quot;) library(&quot;stringr&quot;) library(&quot;microbenchmark&quot;) The map() function appears in both the purrr and maps packages. See the “Prerequisites” section of the Introduction. If you see errors like the following, you are using the wrong map() function. &gt; map(c(TRUE, FALSE, TRUE), ~ !.) Error: $ operator is invalid for atomic vectors &gt; map(-2:2, rnorm, n = 5) Error in map(-2:2, rnorm, n = 5) : argument 3 matches multiple formal arguments You can check the package in which a function is defined using the environment() function: environment(map) #&gt; &lt;environment: namespace:purrr&gt; The result should include namespace:purrr if map() is coming from the purrr package. To explicitly reference the package to get a function from, use the colon operator ::. For example, purrr::map(c(TRUE, FALSE, TRUE), ~ !.) #&gt; [[1]] #&gt; [1] FALSE #&gt; #&gt; [[2]] #&gt; [1] TRUE #&gt; #&gt; [[3]] #&gt; [1] FALSE 21.2 For loops Exercise 21.2.1 Write for-loops to: Compute the mean of every column in mtcars. Determine the type of each column in nycflights13::flights. Compute the number of unique values in each column of iris. Generate 10 random normals for each of \\(\\mu\\) = -10, 0, 10, and 100. The answers for each part are below. To compute the mean of every column in mtcars. output &lt;- vector(&quot;double&quot;, ncol(mtcars)) names(output) &lt;- names(mtcars) for (i in names(mtcars)) { output[i] &lt;- mean(mtcars[[i]]) } output #&gt; mpg cyl disp hp drat wt qsec vs am gear #&gt; 20.091 6.188 230.722 146.688 3.597 3.217 17.849 0.438 0.406 3.688 #&gt; carb #&gt; 2.812 Determine the type of each column in nycflights13::flights. output &lt;- vector(&quot;list&quot;, ncol(nycflights13::flights)) names(output) &lt;- names(nycflights13::flights) for (i in names(nycflights13::flights)) { output[[i]] &lt;- class(nycflights13::flights[[i]]) } output #&gt; $year #&gt; [1] &quot;integer&quot; #&gt; #&gt; $month #&gt; [1] &quot;integer&quot; #&gt; #&gt; $day #&gt; [1] &quot;integer&quot; #&gt; #&gt; $dep_time #&gt; [1] &quot;integer&quot; #&gt; #&gt; $sched_dep_time #&gt; [1] &quot;integer&quot; #&gt; #&gt; $dep_delay #&gt; [1] &quot;numeric&quot; #&gt; #&gt; $arr_time #&gt; [1] &quot;integer&quot; #&gt; #&gt; $sched_arr_time #&gt; [1] &quot;integer&quot; #&gt; #&gt; $arr_delay #&gt; [1] &quot;numeric&quot; #&gt; #&gt; $carrier #&gt; [1] &quot;character&quot; #&gt; #&gt; $flight #&gt; [1] &quot;integer&quot; #&gt; #&gt; $tailnum #&gt; [1] &quot;character&quot; #&gt; #&gt; $origin #&gt; [1] &quot;character&quot; #&gt; #&gt; $dest #&gt; [1] &quot;character&quot; #&gt; #&gt; $air_time #&gt; [1] &quot;numeric&quot; #&gt; #&gt; $distance #&gt; [1] &quot;numeric&quot; #&gt; #&gt; $hour #&gt; [1] &quot;numeric&quot; #&gt; #&gt; $minute #&gt; [1] &quot;numeric&quot; #&gt; #&gt; $time_hour #&gt; [1] &quot;POSIXct&quot; &quot;POSIXt&quot; I used a list, not a character vector, since the class of an object can have multiple values. For example, the class of the time_hour column is POSIXct, POSIXt. To compute the number of unique values in each column of the iris dataset. data(&quot;iris&quot;) iris_uniq &lt;- vector(&quot;double&quot;, ncol(iris)) names(iris_uniq) &lt;- names(iris) for (i in names(iris)) { iris_uniq[i] &lt;- n_distinct(iris[[i]]) } iris_uniq #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 35 23 43 22 3 To generate 10 random normals for each of \\(\\mu\\) = -10, 0, 10, and 100. # number to draw n &lt;- 10 # values of the mean mu &lt;- c(-10, 0, 10, 100) normals &lt;- vector(&quot;list&quot;, length(mu)) for (i in seq_along(normals)) { normals[[i]] &lt;- rnorm(n, mean = mu[i]) } normals #&gt; [[1]] #&gt; [1] -11.40 -9.74 -12.44 -10.01 -9.38 -8.85 -11.82 -10.25 -10.24 -10.28 #&gt; #&gt; [[2]] #&gt; [1] -0.5537 0.6290 2.0650 -1.6310 0.5124 -1.8630 -0.5220 -0.0526 0.5430 #&gt; [10] -0.9141 #&gt; #&gt; [[3]] #&gt; [1] 10.47 10.36 8.70 10.74 11.89 9.90 9.06 9.98 9.17 8.49 #&gt; #&gt; [[4]] #&gt; [1] 100.9 100.2 100.2 101.6 100.1 99.9 98.1 99.7 99.7 101.1 However, we don’t need a for loop for this since rnorm() recycle the mean argument. matrix(rnorm(n * length(mu), mean = mu), ncol = n) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] -9.930 -9.56 -9.88 -10.2061 -12.27 -8.926 -11.178 -9.51 -8.663 -9.39 #&gt; [2,] -0.639 2.76 -1.91 0.0192 2.68 -0.665 -0.976 -1.70 0.237 -0.11 #&gt; [3,] 9.950 10.05 10.86 10.0296 9.64 11.114 11.065 8.53 11.318 10.17 #&gt; [4,] 99.749 100.58 99.76 100.5498 100.21 99.754 100.132 100.28 100.524 99.91 Exercise 21.2.2 Eliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors: out &lt;- &quot;&quot; for (x in letters) { out &lt;- str_c(out, x) } out #&gt; [1] &quot;abcdefghijklmnopqrstuvwxyz&quot; Since str_c() already works with vectors, use str_c() with the collapse argument to return a single string. str_c(letters, collapse = &quot;&quot;) #&gt; [1] &quot;abcdefghijklmnopqrstuvwxyz&quot; For this I’m going to rename the variable sd to something different because sd is the name of the function we want to use. x &lt;- sample(100) sd. &lt;- 0 for (i in seq_along(x)) { sd. &lt;- sd. + (x[i] - mean(x))^2 } sd. &lt;- sqrt(sd. / (length(x) - 1)) sd. #&gt; [1] 29 We could simply use the sd function. sd(x) #&gt; [1] 29 Or if there was a need to use the equation (e.g. for pedagogical reasons), then the functions mean() and sum() already work with vectors: sqrt(sum((x - mean(x))^2) / (length(x) - 1)) #&gt; [1] 29 x &lt;- runif(100) out &lt;- vector(&quot;numeric&quot;, length(x)) out[1] &lt;- x[1] for (i in 2:length(x)) { out[i] &lt;- out[i - 1] + x[i] } out #&gt; [1] 0.854 1.268 2.019 2.738 3.253 4.228 4.589 4.759 5.542 5.573 #&gt; [11] 6.363 6.529 6.558 7.344 8.169 9.134 9.513 9.687 10.291 11.097 #&gt; [21] 11.133 11.866 12.082 12.098 12.226 12.912 13.554 13.882 14.269 14.976 #&gt; [31] 15.674 16.600 17.059 17.655 17.820 18.387 19.285 19.879 20.711 21.304 #&gt; [41] 22.083 22.481 23.331 24.073 24.391 24.502 24.603 25.403 25.783 25.836 #&gt; [51] 26.823 27.427 27.576 28.114 28.240 29.203 29.250 29.412 30.348 31.319 #&gt; [61] 32.029 32.914 33.891 33.926 34.365 35.009 36.004 36.319 37.175 37.715 #&gt; [71] 38.588 39.104 39.973 40.830 41.176 41.176 41.381 42.326 42.607 43.488 #&gt; [81] 44.449 44.454 45.006 45.226 45.872 46.600 47.473 47.855 48.747 49.591 #&gt; [91] 50.321 50.359 50.693 51.443 52.356 52.560 53.032 53.417 53.810 54.028 The code above is calculating a cumulative sum. Use the function cumsum() all.equal(cumsum(x), out) #&gt; [1] TRUE Exercise 21.2.3 Combine your function writing and for loop skills: Write a for loop that prints() the lyrics to the children’s song “Alice the camel”. Convert the nursery rhyme “ten in the bed” to a function. Generalize it to any number of people in any sleeping structure. Convert the song “99 bottles of beer on the wall” to a function. Generalize to any number of any vessel containing any liquid on surface. The answers to each part follow. The lyrics for Alice the Camel are: Alice the camel has five humps. Alice the camel has five humps. Alice the camel has five humps. So go, Alice, go. This verse is repeated, each time with one fewer hump, until there are no humps. The last verse, with no humps, is: Alice the camel has no humps. Alice the camel has no humps. Alice the camel has no humps. Now Alice is a horse. We’ll iterate from five to no humps, and print out a different last line if there are no humps. humps &lt;- c(&quot;five&quot;, &quot;four&quot;, &quot;three&quot;, &quot;two&quot;, &quot;one&quot;, &quot;no&quot;) for (i in humps) { cat(str_c(&quot;Alice the camel has &quot;, rep(i, 3), &quot; humps.&quot;, collapse = &quot;\\n&quot; ), &quot;\\n&quot;) if (i == &quot;no&quot;) { cat(&quot;Now Alice is a horse.\\n&quot;) } else { cat(&quot;So go, Alice, go.\\n&quot;) } cat(&quot;\\n&quot;) } #&gt; Alice the camel has five humps. #&gt; Alice the camel has five humps. #&gt; Alice the camel has five humps. #&gt; So go, Alice, go. #&gt; #&gt; Alice the camel has four humps. #&gt; Alice the camel has four humps. #&gt; Alice the camel has four humps. #&gt; So go, Alice, go. #&gt; #&gt; Alice the camel has three humps. #&gt; Alice the camel has three humps. #&gt; Alice the camel has three humps. #&gt; So go, Alice, go. #&gt; #&gt; Alice the camel has two humps. #&gt; Alice the camel has two humps. #&gt; Alice the camel has two humps. #&gt; So go, Alice, go. #&gt; #&gt; Alice the camel has one humps. #&gt; Alice the camel has one humps. #&gt; Alice the camel has one humps. #&gt; So go, Alice, go. #&gt; #&gt; Alice the camel has no humps. #&gt; Alice the camel has no humps. #&gt; Alice the camel has no humps. #&gt; Now Alice is a horse. The lyrics for Ten in the Bed are: Here we go! There were ten in the bed and the little one said, “Roll over, roll over.” So they all rolled over and one fell out. This verse is repeated, each time with one fewer in the bed, until there is one left. That last verse is: One! There was one in the bed and the little one said, “I’m lonely…” numbers &lt;- c( &quot;ten&quot;, &quot;nine&quot;, &quot;eight&quot;, &quot;seven&quot;, &quot;six&quot;, &quot;five&quot;, &quot;four&quot;, &quot;three&quot;, &quot;two&quot;, &quot;one&quot; ) for (i in numbers) { cat(str_c(&quot;There were &quot;, i, &quot; in the bed\\n&quot;)) cat(&quot;and the little one said\\n&quot;) if (i == &quot;one&quot;) { cat(&quot;I&#39;m lonely...&quot;) } else { cat(&quot;Roll over, roll over\\n&quot;) cat(&quot;So they all rolled over and one fell out.\\n&quot;) } cat(&quot;\\n&quot;) } #&gt; There were ten in the bed #&gt; and the little one said #&gt; Roll over, roll over #&gt; So they all rolled over and one fell out. #&gt; #&gt; There were nine in the bed #&gt; and the little one said #&gt; Roll over, roll over #&gt; So they all rolled over and one fell out. #&gt; #&gt; There were eight in the bed #&gt; and the little one said #&gt; Roll over, roll over #&gt; So they all rolled over and one fell out. #&gt; #&gt; There were seven in the bed #&gt; and the little one said #&gt; Roll over, roll over #&gt; So they all rolled over and one fell out. #&gt; #&gt; There were six in the bed #&gt; and the little one said #&gt; Roll over, roll over #&gt; So they all rolled over and one fell out. #&gt; #&gt; There were five in the bed #&gt; and the little one said #&gt; Roll over, roll over #&gt; So they all rolled over and one fell out. #&gt; #&gt; There were four in the bed #&gt; and the little one said #&gt; Roll over, roll over #&gt; So they all rolled over and one fell out. #&gt; #&gt; There were three in the bed #&gt; and the little one said #&gt; Roll over, roll over #&gt; So they all rolled over and one fell out. #&gt; #&gt; There were two in the bed #&gt; and the little one said #&gt; Roll over, roll over #&gt; So they all rolled over and one fell out. #&gt; #&gt; There were one in the bed #&gt; and the little one said #&gt; I&#39;m lonely... The lyrics of Ninety-Nine Bottles of Beer on the Wall are 99 bottles of beer on the wall, 99 bottles of beer. Take one down, pass it around, 98 bottles of beer on the wall This verse is repeated, each time with one few bottle, until there are no more bottles of beer. The last verse is No more bottles of beer on the wall, no more bottles of beer. We’ve taken them down and passed them around; now we’re drunk and passed out! For the bottles of beer, I define a helper function to correctly print the number of bottles. bottles &lt;- function(n) { if (n &gt; 1) { str_c(n, &quot; bottles&quot;) } else if (n == 1) { &quot;1 bottle&quot; } else { &quot;no more bottles&quot; } } beer_bottles &lt;- function(total_bottles) { # print each lyric for (current_bottles in seq(total_bottles, 0)) { # first line cat(str_to_sentence(str_c(bottles(current_bottles), &quot; of beer on the wall, &quot;, bottles(current_bottles), &quot; of beer.\\n&quot;))) # second line if (current_bottles &gt; 0) { cat(str_c( &quot;Take one down and pass it around, &quot;, bottles(current_bottles - 1), &quot; of beer on the wall.\\n&quot; )) } else { cat(str_c(&quot;Go to the store and buy some more, &quot;, bottles(total_bottles), &quot; of beer on the wall.\\n&quot;)) } cat(&quot;\\n&quot;) } } beer_bottles(3) #&gt; 3 Bottles of beer on the wall, 3 bottles of beer. #&gt; Take one down and pass it around, 2 bottles of beer on the wall. #&gt; #&gt; 2 Bottles of beer on the wall, 2 bottles of beer. #&gt; Take one down and pass it around, 1 bottle of beer on the wall. #&gt; #&gt; 1 Bottle of beer on the wall, 1 bottle of beer. #&gt; Take one down and pass it around, no more bottles of beer on the wall. #&gt; #&gt; No more bottles of beer on the wall, no more bottles of beer. #&gt; Go to the store and buy some more, 3 bottles of beer on the wall. Exercise 21.2.4 It’s common to see for loops that don’t preallocate the output and instead increase the length of a vector at each step: output &lt;- vector(&quot;integer&quot;, 0) for (i in seq_along(x)) { output &lt;- c(output, lengths(x[[i]])) } output How does this affect performance? Design and execute an experiment. In order to compare these two approaches, I’ll define two functions: add_to_vector will append to a vector, like the example in the question, and add_to_vector_2 which pre-allocates a vector. add_to_vector &lt;- function(n) { output &lt;- vector(&quot;integer&quot;, 0) for (i in seq_len(n)) { output &lt;- c(output, i) } output } add_to_vector_2 &lt;- function(n) { output &lt;- vector(&quot;integer&quot;, n) for (i in seq_len(n)) { output[[i]] &lt;- i } output } I’ll use the package microbenchmark to run these functions several times and compare the time it takes. The package microbenchmark contains utilities for benchmarking R expressions. In particular, the microbenchmark() function will run an R expression a number of times and time it. timings &lt;- microbenchmark(add_to_vector(10000), add_to_vector_2(10000), times = 10) timings #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval #&gt; add_to_vector(10000) 111658 113151 119034 117233 120429 143037 10 #&gt; add_to_vector_2(10000) 337 348 1400 360 486 6264 10 In this example, appending to a vector takes 325 times longer than pre-allocating the vector. You may get different answers, but the longer the vector and the larger the objects, the more that pre-allocation will outperform appending. 21.3 For loop variations Exercise 21.3.1 Imagine you have a directory full of CSV files that you want to read in. You have their paths in a vector, files &lt;- dir(&quot;data/&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE), and now want to read each one with read_csv(). Write the for loop that will load them into a single data frame. files &lt;- dir(&quot;data/&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE) files #&gt; [1] &quot;data//file1.csv&quot; &quot;data//file2.csv&quot; &quot;data//file3.csv&quot; Since, the number of files is known, pre-allocate a list with a length equal to the number of files. df_list &lt;- vector(&quot;list&quot;, length(files)) Then, read each file into a data frame, and assign it to an element in that list. The result is a list of data frames. for (i in seq_along(files)) { df_list[[i]] &lt;- read_csv(files[[i]]) } #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_double(), #&gt; X2 = col_character() #&gt; ) #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_double(), #&gt; X2 = col_character() #&gt; ) #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_double(), #&gt; X2 = col_character() #&gt; ) print(df_list) #&gt; [[1]] #&gt; # A tibble: 2 x 2 #&gt; X1 X2 #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 a #&gt; 2 2 b #&gt; #&gt; [[2]] #&gt; # A tibble: 2 x 2 #&gt; X1 X2 #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 3 c #&gt; 2 4 d #&gt; #&gt; [[3]] #&gt; # A tibble: 2 x 2 #&gt; X1 X2 #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 5 e #&gt; 2 6 f Finally, use use bind_rows() to combine the list of data frames into a single data frame. df &lt;- bind_rows(df_list) print(df) #&gt; # A tibble: 6 x 2 #&gt; X1 X2 #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 a #&gt; 2 2 b #&gt; 3 3 c #&gt; 4 4 d #&gt; 5 5 e #&gt; 6 6 f Alternatively, I could have pre-allocated a list with the names of the files. df2_list &lt;- vector(&quot;list&quot;, length(files)) names(df2_list) &lt;- files for (fname in files) { df2_list[[fname]] &lt;- read_csv(fname) } #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_double(), #&gt; X2 = col_character() #&gt; ) #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_double(), #&gt; X2 = col_character() #&gt; ) #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_double(), #&gt; X2 = col_character() #&gt; ) df2 &lt;- bind_rows(df2_list) Exercise 21.3.2 What happens if you use for (nm in names(x)) and x has no names? What if only some of the elements are named? What if the names are not unique? Let’s try it out and see what happens. When there are no names for the vector, it does not run the code in the loop. In other words, it runs zero iterations of the loop. x &lt;- c(11, 12, 13) print(names(x)) #&gt; NULL for (nm in names(x)) { print(nm) print(x[[nm]]) } Note that the length of NULL is zero: length(NULL) #&gt; [1] 0 If there only some names, then we get an error for trying to access an element without a name. x &lt;- c(a = 11, 12, c = 13) names(x) #&gt; [1] &quot;a&quot; &quot;&quot; &quot;c&quot; for (nm in names(x)) { print(nm) print(x[[nm]]) } #&gt; [1] &quot;a&quot; #&gt; [1] 11 #&gt; [1] &quot;&quot; #&gt; Error in x[[nm]]: subscript out of bounds Finally, if the vector contains duplicate names, then x[[nm]] returns the first element with that name. x &lt;- c(a = 11, a = 12, c = 13) names(x) #&gt; [1] &quot;a&quot; &quot;a&quot; &quot;c&quot; for (nm in names(x)) { print(nm) print(x[[nm]]) } #&gt; [1] &quot;a&quot; #&gt; [1] 11 #&gt; [1] &quot;a&quot; #&gt; [1] 11 #&gt; [1] &quot;c&quot; #&gt; [1] 13 Exercise 21.3.3 Write a function that prints the mean of each numeric column in a data frame, along with its name. For example, show_mean(iris) would print: show_mean(iris) # &gt; Sepal.Length: 5.84 # &gt; Sepal.Width: 3.06 # &gt; Petal.Length: 3.76 # &gt; Petal.Width: 1.20 Extra challenge: what function did I use to make sure that the numbers lined up nicely, even though the variable names had different lengths? There may be other functions to do this, but I’ll use str_pad(), and str_length() to ensure that the space given to the variable names is the same. I messed around with the options to format() until I got two digits. show_mean &lt;- function(df, digits = 2) { # Get max length of all variable names in the dataset maxstr &lt;- max(str_length(names(df))) for (nm in names(df)) { if (is.numeric(df[[nm]])) { cat( str_c(str_pad(str_c(nm, &quot;:&quot;), maxstr + 1L, side = &quot;right&quot;), format(mean(df[[nm]]), digits = digits, nsmall = digits), sep = &quot; &quot; ), &quot;\\n&quot; ) } } } show_mean(iris) #&gt; Sepal.Length: 5.84 #&gt; Sepal.Width: 3.06 #&gt; Petal.Length: 3.76 #&gt; Petal.Width: 1.20 Exercise 21.3.4 What does this code do? How does it work? trans &lt;- list( disp = function(x) x * 0.0163871, am = function(x) { factor(x, labels = c(&quot;auto&quot;, &quot;manual&quot;)) } ) for (var in names(trans)) { mtcars[[var]] &lt;- trans[[var]](mtcars[[var]]) } This code mutates the disp and am columns: disp is multiplied by 0.0163871 am is replaced by a factor variable. The code works by looping over a named list of functions. It calls the named function in the list on the column of mtcars with the same name, and replaces the values of that column. This is a function. trans[[&quot;disp&quot;]] This applies the function to the column of mtcars with the same name trans[[&quot;disp&quot;]](mtcars[[&quot;disp&quot;]]) 21.4 For loops vs. functionals Exercise 21.4.1 Read the documentation for apply(). In the 2nd case, what two for-loops does it generalize. For an object with two-dimensions, such as a matrix or data frame, apply() replaces looping over the rows or columns of a matrix or data-frame. The apply() function is used like apply(X, MARGIN, FUN, ...), where X is a matrix or array, FUN is a function to apply, and ... are additional arguments passed to FUN. When MARGIN = 1, then the function is applied to each row. For example, the following example calculates the row means of a matrix. X &lt;- matrix(rnorm(15), nrow = 5) X #&gt; [,1] [,2] [,3] #&gt; [1,] -1.4523 0.124 0.709 #&gt; [2,] 0.9412 -0.998 -1.529 #&gt; [3,] -0.3389 1.233 0.237 #&gt; [4,] -0.0756 0.340 -1.313 #&gt; [5,] 0.0402 -0.473 0.747 apply(X, 1, mean) #&gt; [1] -0.206 -0.529 0.377 -0.349 0.105 That is equivalent to this for-loop. X_row_means &lt;- vector(&quot;numeric&quot;, length = nrow(X)) for (i in seq_len(nrow(X))) { X_row_means[[i]] &lt;- mean(X[i, ]) } X_row_means #&gt; [1] -0.206 -0.529 0.377 -0.349 0.105 X &lt;- matrix(rnorm(15), nrow = 5) X #&gt; [,1] [,2] [,3] #&gt; [1,] -1.5625 1.153 1.20377 #&gt; [2,] 0.0711 -1.687 -1.43127 #&gt; [3,] -0.6395 -0.903 1.38291 #&gt; [4,] -0.8452 1.318 0.00313 #&gt; [5,] 0.6752 1.100 -0.07789 When MARGIN = 2, apply() is equivalent to a for-loop looping over columns. apply(X, 2, mean) #&gt; [1] -0.460 0.196 0.216 X_col_means &lt;- vector(&quot;numeric&quot;, length = ncol(X)) for (i in seq_len(ncol(X))) { X_col_means[[i]] &lt;- mean(X[, i]) } X_col_means #&gt; [1] -0.460 0.196 0.216 Exercise 21.4.2 Adapt col_summary() so that it only applies to numeric columns. You might want to start with an is_numeric() function that returns a logical vector that has a TRUE corresponding to each numeric column. The original col_summary() function is col_summary &lt;- function(df, fun) { out &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { out[i] &lt;- fun(df[[i]]) } out } The adapted version adds extra logic to only apply the function to numeric columns. col_summary2 &lt;- function(df, fun) { # create an empty vector which will store whether each # column is numeric numeric_cols &lt;- vector(&quot;logical&quot;, length(df)) # test whether each column is numeric for (i in seq_along(df)) { numeric_cols[[i]] &lt;- is.numeric(df[[i]]) } # find the indexes of the numeric columns idxs &lt;- which(numeric_cols) # find the number of numeric columns n &lt;- sum(numeric_cols) # create a vector to hold the results out &lt;- vector(&quot;double&quot;, n) # apply the function only to numeric vectors for (i in seq_along(idxs)) { out[[i]] &lt;- fun(df[[idxs[[i]]]]) } # name the vector names(out) &lt;- names(df)[idxs] out } Let’s test that col_summary2() works by creating a small data frame with some numeric and non-numeric columns. df &lt;- tibble( X1 = c(1, 2, 3), X2 = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), X3 = c(0, -1, 5), X4 = c(TRUE, FALSE, TRUE) ) col_summary2(df, mean) #&gt; X1 X3 #&gt; 2.00 1.33 As expected, it only calculates the mean of the numeric columns, X1 and X3. Let’s test that it works with another function. col_summary2(df, median) #&gt; X1 X3 #&gt; 2 0 21.5 The map functions Exercise 21.5.1 Write code that uses one of the map functions to: Compute the mean of every column in mtcars. Determine the type of each column in nycflights13::flights. Compute the number of unique values in each column of iris. Generate 10 random normals for each of \\(\\mu = -10\\), \\(0\\), \\(10\\), and \\(100\\). To calculate the mean of every column in mtcars, apply the function mean() to each column, and use map_dbl, since the results are numeric. map_dbl(mtcars, mean) #&gt; mpg cyl disp hp drat wt qsec vs am gear #&gt; 20.091 6.188 230.722 146.688 3.597 3.217 17.849 0.438 0.406 3.688 #&gt; carb #&gt; 2.812 To calculate the type of every column in nycflights13::flights apply the function typeof(), discussed in the section on Vector basics, and use map_chr(), since the results are character. map_chr(nycflights13::flights, typeof) #&gt; year month day dep_time sched_dep_time #&gt; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; #&gt; dep_delay arr_time sched_arr_time arr_delay carrier #&gt; &quot;double&quot; &quot;integer&quot; &quot;integer&quot; &quot;double&quot; &quot;character&quot; #&gt; flight tailnum origin dest air_time #&gt; &quot;integer&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;double&quot; #&gt; distance hour minute time_hour #&gt; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; The function n_distinct() calculates the number of unique values in a vector. map_int(iris, n_distinct) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 35 23 43 22 3 The map_int() function is used since length() returns an integer. However, the map_dbl() function will also work. map_dbl(iris, n_distinct) An alternative to the n_distinct() function is the expression, length(unique(...)). The n_distinct() function is more concise and faster, but length(unique(...)) provides an example of using anonymous functions with map functions. An anonymous function can be written using the standard R syntax for a function: map_int(iris, function(x) length(unique(x))) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 35 23 43 22 3 Additionally, map functions accept one-sided formulas as a more concise alternative to specify an anonymous function: map_int(iris, ~length(unique(.x))) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 35 23 43 22 3 In this case, the anonymous function accepts one argument, which is referenced by .x in the expression length(unique(.x)). To generate 10 random normals for each of \\(\\mu = -10\\), \\(0\\), \\(10\\), and \\(100\\): The result is a list of numeric vectors. map(c(-10, 0, 10, 100), ~rnorm(n = 10, mean = .)) #&gt; [[1]] #&gt; [1] -9.56 -9.87 -10.83 -10.50 -11.19 -10.75 -8.54 -10.83 -9.71 -10.48 #&gt; #&gt; [[2]] #&gt; [1] -0.6048 1.4601 0.1497 -1.4333 -0.0103 -0.2122 -0.9063 -2.1022 1.8934 #&gt; [10] -0.9681 #&gt; #&gt; [[3]] #&gt; [1] 9.90 10.24 10.06 7.82 9.88 10.11 10.01 11.88 12.16 10.71 #&gt; #&gt; [[4]] #&gt; [1] 100.8 99.7 101.0 99.1 100.6 100.3 100.4 101.1 99.1 100.2 Since a single call of rnorm() returns a numeric vector with a length greater than one we cannot use map_dbl, which requires the function to return a numeric vector that is only length one (see Exercise 21.5.4). The map functions pass any additional arguments to the function being called. Exercise 21.5.2 How can you create a single vector that for each column in a data frame indicates whether or not it’s a factor? The function is.factor() indicates whether a vector is a factor. is.factor(diamonds$color) #&gt; [1] TRUE Checking all columns in a data frame is a job for a map_*() function. Since the result of is.factor() is logical, we will use map_lgl() to apply is.factor() to the columns of the data frame. map_lgl(diamonds, is.factor) #&gt; carat cut color clarity depth table price x y z #&gt; FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE Exercise 21.5.3 What happens when you use the map functions on vectors that aren’t lists? What does map(1:5, runif) do? Why? Map functions work with any vectors, not just lists. As with lists, the map functions will apply the function to each element of the vector. In the following examples, the inputs to map() are atomic vectors (logical, character, integer, double). map(c(TRUE, FALSE, TRUE), ~ !.) #&gt; [[1]] #&gt; [1] FALSE #&gt; #&gt; [[2]] #&gt; [1] TRUE #&gt; #&gt; [[3]] #&gt; [1] FALSE map(c(&quot;Hello&quot;, &quot;World&quot;), str_to_upper) #&gt; [[1]] #&gt; [1] &quot;HELLO&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;WORLD&quot; map(1:5, ~ rnorm(.)) #&gt; [[1]] #&gt; [1] 1.42 #&gt; #&gt; [[2]] #&gt; [1] -0.384 -0.174 #&gt; #&gt; [[3]] #&gt; [1] -0.222 -1.010 0.481 #&gt; #&gt; [[4]] #&gt; [1] 1.604 -1.515 -1.416 0.877 #&gt; #&gt; [[5]] #&gt; [1] 0.624 2.112 -0.356 -1.064 1.077 map(c(-0.5, 0, 1), ~ rnorm(1, mean = .)) #&gt; [[1]] #&gt; [1] 0.682 #&gt; #&gt; [[2]] #&gt; [1] 0.198 #&gt; #&gt; [[3]] #&gt; [1] 0.6 It is important to be aware that while the input of map() can be any vector, the output is always a list. map(1:5, runif) #&gt; [[1]] #&gt; [1] 0.731 #&gt; #&gt; [[2]] #&gt; [1] 0.852 0.976 #&gt; #&gt; [[3]] #&gt; [1] 0.113 0.970 0.648 #&gt; #&gt; [[4]] #&gt; [1] 0.0561 0.4731 0.2946 0.6103 #&gt; #&gt; [[5]] #&gt; [1] 0.1211 0.6294 0.7120 0.6121 0.0344 This expression is equivalent to running the following. list( runif(1), runif(2), runif(3), runif(4), runif(5) ) #&gt; [[1]] #&gt; [1] 0.666 #&gt; #&gt; [[2]] #&gt; [1] 0.653 0.452 #&gt; #&gt; [[3]] #&gt; [1] 0.517 0.677 0.881 #&gt; #&gt; [[4]] #&gt; [1] 0.731 0.399 0.431 0.145 #&gt; #&gt; [[5]] #&gt; [1] 0.4511 0.5788 0.0704 0.7423 0.5492 The map() function loops through the numbers 1 to 5. For each value, it calls the runif() with that number as the first argument, which is the number of sample to draw. The result is a length five list with numeric vectors of sizes one through five, each with random samples from a uniform distribution. Note that although input to map() was an integer vector, the return value was a list. Exercise 21.5.4 What does map(-2:2, rnorm, n = 5) do? Why? What does map_dbl(-2:2, rnorm, n = 5) do? Why? Consider the first expression. map(-2:2, rnorm, n = 5) #&gt; [[1]] #&gt; [1] -1.656 -0.522 -1.928 0.126 -3.476 #&gt; #&gt; [[2]] #&gt; [1] -0.5921 0.3940 -0.6397 -0.3454 0.0522 #&gt; #&gt; [[3]] #&gt; [1] -1.980 1.208 -0.169 0.295 1.266 #&gt; #&gt; [[4]] #&gt; [1] -0.135 -0.131 1.110 1.853 0.766 #&gt; #&gt; [[5]] #&gt; [1] 4.087 1.889 0.607 0.858 3.705 This expression takes samples of size five from five normal distributions, with means of (-2, -1, 0, 1, and 2), but the same standard deviation (1). It returns a list with each element a numeric vectors of length 5. However, if instead, we use map_dbl(), the expression raises an error. map_dbl(-2:2, rnorm, n = 5) #&gt; Error: Result 1 must be a single double, not a double vector of length 5 This is because the map_dbl() function requires the function it applies to each element to return a numeric vector of length one. If the function returns either a non-numeric vector or a numeric vector with a length greater than one, map_dbl() will raise an error. The reason for this strictness is that map_dbl() guarantees that it will return a numeric vector of the same length as its input vector. This concept applies to the other map_*() functions. The function map_chr() requires that the function always return a character vector of length one; map_int() requires that the function always return an integer vector of length one; map_lgl() requires that the function always return an logical vector of length one. Use the map() function if the function will return values of varying types or lengths. To return a numeric vector, use flatten_dbl() to coerce the list returned by map() to a numeric vector. map(-2:2, rnorm, n = 5) %&gt;% flatten_dbl() #&gt; [1] -2.145 -1.474 -0.266 -0.551 -0.482 -1.384 0.827 -1.551 -1.866 -1.344 #&gt; [11] 1.063 0.813 1.803 -0.105 0.982 -0.713 0.168 2.100 0.826 1.179 #&gt; [21] 1.302 1.040 1.025 1.661 3.152 Exercise 21.5.5 Rewrite map(x, function(df) lm(mpg ~ wt, data = df)) to eliminate the anonymous function. This code in this question does not run, so I will use the following code. x &lt;- split(mtcars, mtcars$cyl) map(x, function(df) lm(mpg ~ wt, data = df)) #&gt; $`4` #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ wt, data = df) #&gt; #&gt; Coefficients: #&gt; (Intercept) wt #&gt; 39.57 -5.65 #&gt; #&gt; #&gt; $`6` #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ wt, data = df) #&gt; #&gt; Coefficients: #&gt; (Intercept) wt #&gt; 28.41 -2.78 #&gt; #&gt; #&gt; $`8` #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ wt, data = df) #&gt; #&gt; Coefficients: #&gt; (Intercept) wt #&gt; 23.87 -2.19 We can eliminate the use of an anonymous function using the ~ shortcut. map(x, ~ lm(mpg ~ wt, data = .)) #&gt; $`4` #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ wt, data = .) #&gt; #&gt; Coefficients: #&gt; (Intercept) wt #&gt; 39.57 -5.65 #&gt; #&gt; #&gt; $`6` #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ wt, data = .) #&gt; #&gt; Coefficients: #&gt; (Intercept) wt #&gt; 28.41 -2.78 #&gt; #&gt; #&gt; $`8` #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ wt, data = .) #&gt; #&gt; Coefficients: #&gt; (Intercept) wt #&gt; 23.87 -2.19 Though not the intent of this question, the other way to eliminate anonymous function is to create a named one. run_reg &lt;- function(df) { lm(mpg ~ wt, data = df) } map(x, run_reg) #&gt; $`4` #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ wt, data = df) #&gt; #&gt; Coefficients: #&gt; (Intercept) wt #&gt; 39.57 -5.65 #&gt; #&gt; #&gt; $`6` #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ wt, data = df) #&gt; #&gt; Coefficients: #&gt; (Intercept) wt #&gt; 28.41 -2.78 #&gt; #&gt; #&gt; $`8` #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ wt, data = df) #&gt; #&gt; Coefficients: #&gt; (Intercept) wt #&gt; 23.87 -2.19 21.6 Dealing with failure No exercises 21.7 Mapping over multiple arguments No exercises 21.8 Walk No exercises 21.9 Other patterns of for loops Exercise 21.9.1 Implement your own version of every() using a for loop. Compare it with purrr::every(). What does purrr’s version do that your version doesn’t? # Use ... to pass arguments to the function every2 &lt;- function(.x, .p, ...) { for (i in .x) { if (!.p(i, ...)) { # If any is FALSE we know not all of then were TRUE return(FALSE) } } # if nothing was FALSE, then it is TRUE TRUE } every2(1:3, function(x) { x &gt; 1 }) #&gt; [1] FALSE every2(1:3, function(x) { x &gt; 0 }) #&gt; [1] TRUE The function purrr::every() does fancy things with the predicate function argument .p, like taking a logical vector instead of a function, or being able to test part of a string if the elements of .x are lists. Exercise 21.9.2 Create an enhanced col_summary() that applies a summary function to every numeric column in a data frame. I will use map to apply the function to all the columns, and keep to only select numeric columns. col_sum2 &lt;- function(df, f, ...) { map(keep(df, is.numeric), f, ...) } col_sum2(iris, mean) #&gt; $Sepal.Length #&gt; [1] 5.84 #&gt; #&gt; $Sepal.Width #&gt; [1] 3.06 #&gt; #&gt; $Petal.Length #&gt; [1] 3.76 #&gt; #&gt; $Petal.Width #&gt; [1] 1.2 Exercise 21.9.3 A possible base R equivalent of col_summary() is: col_sum3 &lt;- function(df, f) { is_num &lt;- sapply(df, is.numeric) df_num &lt;- df[, is_num] sapply(df_num, f) } But it has a number of bugs as illustrated with the following inputs: df &lt;- tibble( x = 1:3, y = 3:1, z = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ) # OK col_sum3(df, mean) # Has problems: don&#39;t always return numeric vector col_sum3(df[1:2], mean) col_sum3(df[1], mean) col_sum3(df[0], mean) What causes these bugs? The cause of these bugs is the behavior of sapply(). The sapply() function does not guarantee the type of vector it returns, and will returns different types of vectors depending on its inputs. If no columns are selected, instead of returning an empty numeric vector, it returns an empty list. This causes an error since we can’t use a list with [. sapply(df[0], is.numeric) #&gt; named list() sapply(df[1], is.numeric) #&gt; X1 #&gt; TRUE sapply(df[1:2], is.numeric) #&gt; X1 X2 #&gt; TRUE FALSE The sapply() function tries to be helpful by simplifying the results, but this behavior can be counterproductive. It is okay to use the sapply() function interactively, but avoid programming with it. "],
["model-intro.html", "22 Introduction", " 22 Introduction No exercises "],
["model-basics.html", "23 Model basics 23.1 Introduction 23.2 A simple model 23.3 Visualising models 23.4 Formulas and model families 23.5 Missing values 23.6 Other model families", " 23 Model basics 23.1 Introduction library(&quot;tidyverse&quot;) library(&quot;modelr&quot;) The option na.action determines how missing values are handled. It is a function. na.warn sets it so that there is a warning if there are any missing values. If it is not set (the default), R will silently drop them. options(na.action = na.warn) 23.2 A simple model Exercise 23.2.1 One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualize the results. Rerun a few times to generate different simulated datasets. What do you notice about the model? sim1a &lt;- tibble( x = rep(1:10, each = 3), y = x * 1.5 + 6 + rt(length(x), df = 2) ) Let’s run it once and plot the results: ggplot(sim1a, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; We can also do this more systematically, by generating several simulations and plotting the line. simt &lt;- function(i) { tibble( x = rep(1:10, each = 3), y = x * 1.5 + 6 + rt(length(x), df = 2), .id = i ) } sims &lt;- map_df(1:12, simt) ggplot(sims, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, colour = &quot;red&quot;) + facet_wrap(~.id, ncol = 4) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; What if we did the same things with normal distributions? sim_norm &lt;- function(i) { tibble( x = rep(1:10, each = 3), y = x * 1.5 + 6 + rnorm(length(x)), .id = i ) } simdf_norm &lt;- map_df(1:12, sim_norm) ggplot(simdf_norm, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, colour = &quot;red&quot;) + facet_wrap(~.id, ncol = 4) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; There are not large outliers, and the slopes are more similar. The reason for this is that the Student’s \\(t\\)-distribution, from which we sample with rt() has heavier tails than the normal distribution (rnorm()). This means that the Student’s t-distribution assigns a larger probability to values further from the center of the distribution. tibble( x = seq(-5, 5, length.out = 100), normal = dnorm(x), student_t = dt(x, df = 2) ) %&gt;% pivot_longer(-x, names_to=&quot;distribution&quot;, values_to=&quot;density&quot;) %&gt;% ggplot(aes(x = x, y = density, colour = distribution)) + geom_line() For a normal distribution with mean zero and standard deviation one, the probability of being greater than 2 is, pnorm(2, lower.tail = FALSE) #&gt; [1] 0.0228 For a Student’s \\(t\\) distribution with degrees of freedom = 2, it is more than 3 times higher, pt(2, df = 2, lower.tail = FALSE) #&gt; [1] 0.0918 Exercise 23.2.2 One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance: measure_distance &lt;- function(mod, data) { diff &lt;- data$y - make_prediction(mod, data) mean(abs(diff)) } For the above function to work, we need to define a function, make_prediction(), that takes a numeric vector of length two (the intercept and slope) and returns the predictions, make_prediction &lt;- function(mod, data) { mod[1] + mod[2] * data$x } Using the sim1a data, the best parameters of the least absolute deviation are: best &lt;- optim(c(0, 0), measure_distance, data = sim1a) best$par #&gt; [1] 5.25 1.66 Using the sim1a data, while the parameters the minimize the least squares objective function are: measure_distance_ls &lt;- function(mod, data) { diff &lt;- data$y - (mod[1] + mod[2] * data$x) sqrt(mean(diff^2)) } best &lt;- optim(c(0, 0), measure_distance_ls, data = sim1a) best$par #&gt; [1] 5.87 1.56 In practice, I suggest not using optim() to fit this model, and instead using an existing implementation. The rlm() and lqs() functions in the MASS fit robust and resistant linear models. Exercise 23.2.3 One challenge with performing numerical optimization is that it’s only guaranteed to find a local optimum. What’s the problem with optimizing a three parameter model like this? model3 &lt;- function(a, data) { a[1] + data$x * a[2] + a[3] } The problem is that you for any values a[1] = a1 and a[3] = a3, any other values of a[1] and a[3] where a[1] + a[3] == (a1 + a3) will have the same fit. measure_distance_3 &lt;- function(a, data) { diff &lt;- data$y - model3(a, data) sqrt(mean(diff^2)) } Depending on our starting points, we can find different optimal values: best3a &lt;- optim(c(0, 0, 0), measure_distance_3, data = sim1) best3a$par #&gt; [1] 3.367 2.052 0.853 best3b &lt;- optim(c(0, 0, 1), measure_distance_3, data = sim1) best3b$par #&gt; [1] -3.47 2.05 7.69 best3c &lt;- optim(c(0, 0, 5), measure_distance_3, data = sim1) best3c$par #&gt; [1] -1.12 2.05 5.35 In fact there are an infinite number of optimal values for this model. 23.3 Visualising models Exercise 23.3.1 Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualization on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()? I’ll use add_predictions() and add_residuals() to add the predictions and residuals from a loess regression to the sim1 data. sim1_loess &lt;- loess(y ~ x, data = sim1) sim1_lm &lt;- lm(y ~ x, data = sim1) grid_loess &lt;- sim1 %&gt;% add_predictions(sim1_loess) sim1 &lt;- sim1 %&gt;% add_residuals(sim1_lm) %&gt;% add_predictions(sim1_lm) %&gt;% add_residuals(sim1_loess, var = &quot;resid_loess&quot;) %&gt;% add_predictions(sim1_loess, var = &quot;pred_loess&quot;) This plots the loess predictions. The loess produces a nonlinear, smooth line through the data. plot_sim1_loess &lt;- ggplot(sim1, aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = pred), data = grid_loess, colour = &quot;red&quot;) plot_sim1_loess The predictions of loess are the same as the default method for geom_smooth() because geom_smooth() uses loess() by default; the message even tells us that. plot_sim1_loess + geom_smooth(method = &quot;loess&quot;, colour = &quot;blue&quot;, se = FALSE, alpha = 0.20) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; We can plot the residuals (red), and compare them to the residuals from lm() (black). In general, the loess model has smaller residuals within the sample (out of sample is a different issue, and we haven’t considered the uncertainty of these estimates). ggplot(sim1, aes(x = x)) + geom_ref_line(h = 0) + geom_point(aes(y = resid)) + geom_point(aes(y = resid_loess), colour = &quot;red&quot;) Exercise 23.3.2 add_predictions() is paired with gather_predictions() and spread_predictions(). How do these three functions differ? The functions gather_predictions() and spread_predictions() allow for adding predictions from multiple models at once. Taking the sim1_mod example, sim1_mod &lt;- lm(y ~ x, data = sim1) grid &lt;- sim1 %&gt;% data_grid(x) The function add_predictions() adds only a single model at a time. To add two models: grid %&gt;% add_predictions(sim1_mod, var = &quot;pred_lm&quot;) %&gt;% add_predictions(sim1_loess, var = &quot;pred_loess&quot;) #&gt; # A tibble: 10 x 3 #&gt; x pred_lm pred_loess #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 6.27 5.34 #&gt; 2 2 8.32 8.27 #&gt; 3 3 10.4 10.8 #&gt; 4 4 12.4 12.8 #&gt; 5 5 14.5 14.6 #&gt; 6 6 16.5 16.6 #&gt; # … with 4 more rows The function gather_predictions() adds predictions from multiple models by stacking the results and adding a column with the model name, grid %&gt;% gather_predictions(sim1_mod, sim1_loess) #&gt; # A tibble: 20 x 3 #&gt; model x pred #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 sim1_mod 1 6.27 #&gt; 2 sim1_mod 2 8.32 #&gt; 3 sim1_mod 3 10.4 #&gt; 4 sim1_mod 4 12.4 #&gt; 5 sim1_mod 5 14.5 #&gt; 6 sim1_mod 6 16.5 #&gt; # … with 14 more rows The function spread_predictions() adds predictions from multiple models by adding multiple columns (postfixed with the model name) with predictions from each model. grid %&gt;% spread_predictions(sim1_mod, sim1_loess) #&gt; # A tibble: 10 x 3 #&gt; x sim1_mod sim1_loess #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 6.27 5.34 #&gt; 2 2 8.32 8.27 #&gt; 3 3 10.4 10.8 #&gt; 4 4 12.4 12.8 #&gt; 5 5 14.5 14.6 #&gt; 6 6 16.5 16.6 #&gt; # … with 4 more rows The function spread_predictions() is similar to the example which runs add_predictions() for each model, and is equivalent to running spread() after running gather_predictions(): grid %&gt;% gather_predictions(sim1_mod, sim1_loess) %&gt;% spread(model, pred) #&gt; # A tibble: 10 x 3 #&gt; x sim1_loess sim1_mod #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 5.34 6.27 #&gt; 2 2 8.27 8.32 #&gt; 3 3 10.8 10.4 #&gt; 4 4 12.8 12.4 #&gt; 5 5 14.6 14.5 #&gt; 6 6 16.6 16.5 #&gt; # … with 4 more rows Exercise 23.3.3 What does geom_ref_line() do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important? The geom geom_ref_line() adds as reference line to a plot. It is equivalent to running geom_hline() or geom_vline() with default settings that are useful for visualizing models. Putting a reference line at zero for residuals is important because good models (generally) should have residuals centered at zero, with approximately the same variance (or distribution) over the support of x, and no correlation. A zero reference line makes it easier to judge these characteristics visually. Exercise 23.3.4 Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals? Showing the absolute values of the residuals makes it easier to view the spread of the residuals. The model assumes that the residuals have mean zero, and using the absolute values of the residuals effectively doubles the number of residuals. sim1_mod &lt;- lm(y ~ x, data = sim1) sim1 &lt;- sim1 %&gt;% add_residuals(sim1_mod) ggplot(sim1, aes(x = abs(resid))) + geom_freqpoly(binwidth = 0.5) However, using the absolute values of residuals throws away information about the sign, meaning that the frequency polygon cannot show whether the model systematically over- or under-estimates the residuals. 23.4 Formulas and model families Exercise 23.4.1 What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions? To run a model without an intercept, add - 1 or + 0 to the right-hand-side o f the formula: mod2a &lt;- lm(y ~ x - 1, data = sim2) mod2 &lt;- lm(y ~ x, data = sim2) The predictions are exactly the same in the models with and without an intercept: grid &lt;- sim2 %&gt;% data_grid(x) %&gt;% spread_predictions(mod2, mod2a) grid #&gt; # A tibble: 4 x 3 #&gt; x mod2 mod2a #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 a 1.15 1.15 #&gt; 2 b 8.12 8.12 #&gt; 3 c 6.13 6.13 #&gt; 4 d 1.91 1.91 Exercise 23.4.2 Use model_matrix() to explore the equations generated for the models I fit to sim3 and sim4. Why is * a good shorthand for interaction? For x1 * x2 when x2 is a categorical variable produces indicator variables x2b, x2c, x2d and variables x1:x2b, x1:x2c, and x1:x2d which are the products of x1 and x2* variables: x3 &lt;- model_matrix(y ~ x1 * x2, data = sim3) x3 #&gt; # A tibble: 120 x 8 #&gt; `(Intercept)` x1 x2b x2c x2d `x1:x2b` `x1:x2c` `x1:x2d` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 0 0 0 0 0 0 #&gt; 2 1 1 0 0 0 0 0 0 #&gt; 3 1 1 0 0 0 0 0 0 #&gt; 4 1 1 1 0 0 1 0 0 #&gt; 5 1 1 1 0 0 1 0 0 #&gt; 6 1 1 1 0 0 1 0 0 #&gt; # … with 114 more rows We can confirm that the variables x1:x2b is the product of x1 and x2b, all(x3[[&quot;x1:x2b&quot;]] == (x3[[&quot;x1&quot;]] * x3[[&quot;x2b&quot;]])) #&gt; [1] TRUE and similarly for x1:x2c and x2c, and x1:x2d and x2d: all(x3[[&quot;x1:x2c&quot;]] == (x3[[&quot;x1&quot;]] * x3[[&quot;x2c&quot;]])) #&gt; [1] TRUE all(x3[[&quot;x1:x2d&quot;]] == (x3[[&quot;x1&quot;]] * x3[[&quot;x2d&quot;]])) #&gt; [1] TRUE For x1 * x2 where both x1 and x2 are continuous variables, model_matrix() creates variables x1, x2, and x1:x2: x4 &lt;- model_matrix(y ~ x1 * x2, data = sim4) x4 #&gt; # A tibble: 300 x 4 #&gt; `(Intercept)` x1 x2 `x1:x2` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 -1 -1 1 #&gt; 2 1 -1 -1 1 #&gt; 3 1 -1 -1 1 #&gt; 4 1 -1 -0.778 0.778 #&gt; 5 1 -1 -0.778 0.778 #&gt; 6 1 -1 -0.778 0.778 #&gt; # … with 294 more rows Confirm that x1:x2 is the product of the x1 and x2, all(x4[[&quot;x1&quot;]] * x4[[&quot;x2&quot;]] == x4[[&quot;x1:x2&quot;]]) #&gt; [1] TRUE The asterisk * is good shorthand for an interaction since an interaction between x1 and x2 includes terms for x1, x2, and the product of x1 and x2. Exercise 23.4.3 Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.) mod1 &lt;- lm(y ~ x1 + x2, data = sim3) mod2 &lt;- lm(y ~ x1 * x2, data = sim3) The problem is to convert the formulas in the models into functions. I will assume that the function is only handling the conversion of the right hand side of the formula into a model matrix. The functions will take one argument, a data frame with x1 and x2 columns, and it will return a data frame. In other words, the functions will be special cases of the model_matrix() function. Consider the right hand side of the first formula, ~ x1 + x2. In the sim3 data frame, the column x1 is an integer, and the variable x2 is a factor with four levels. levels(sim3$x2) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; Since x1 is numeric it is unchanged. Since x2 is a factor it is replaced with columns of indicator variables for all but one of its levels. I will first consider the special case in which x2 only takes the levels of x2 in sim3. In this case, “a” is considered the reference level and omitted, and new columns are made for “b”, “c”, and “d”. model_matrix_mod1 &lt;- function(.data) { mutate(.data, x2b = as.numeric(x2 == &quot;b&quot;), x2c = as.numeric(x2 == &quot;c&quot;), x2d = as.numeric(x2 == &quot;d&quot;), `(Intercept)` = 1 ) %&gt;% select(`(Intercept)`, x1, x2b, x2c, x2d) } model_matrix_mod1(sim3) #&gt; # A tibble: 120 x 5 #&gt; `(Intercept)` x1 x2b x2c x2d #&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 0 0 0 #&gt; 2 1 1 0 0 0 #&gt; 3 1 1 0 0 0 #&gt; 4 1 1 1 0 0 #&gt; 5 1 1 1 0 0 #&gt; 6 1 1 1 0 0 #&gt; # … with 114 more rows A more general function for ~ x1 + x2 would not hard-code the specific levels in x2. model_matrix_mod1b &lt;- function(.data) { # the levels of x2 lvls &lt;- levels(.data$x2) # drop the first level # this assumes that there are at least two levels lvls &lt;- lvls[2:length(lvls)] # create an indicator variable for each level of x2 for (lvl in lvls) { # new column name x2 + level name varname &lt;- str_c(&quot;x2&quot;, lvl) # add indicator variable for lvl .data[[varname]] &lt;- as.numeric(.data$x2 == lvl) } # generate the list of variables to keep x2_variables &lt;- str_c(&quot;x2&quot;, lvls) # Add an intercept .data[[&quot;(Intercept)&quot;]] &lt;- 1 # keep x1 and x2 indicator variables select(.data, `(Intercept)`, x1, all_of(x2_variables)) } model_matrix_mod1b(sim3) #&gt; # A tibble: 120 x 5 #&gt; `(Intercept)` x1 x2b x2c x2d #&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 0 0 0 #&gt; 2 1 1 0 0 0 #&gt; 3 1 1 0 0 0 #&gt; 4 1 1 1 0 0 #&gt; 5 1 1 1 0 0 #&gt; 6 1 1 1 0 0 #&gt; # … with 114 more rows Consider the right hand side of the first formula, ~ x1 * x2. The output data frame will consist of x1, columns with indicator variables for each level (except the reference level) of x2, and columns with the x2 indicator variables multiplied by x1. As with the previous formula, first I’ll write a function that hard-codes the levels of x2. model_matrix_mod2 &lt;- function(.data) { mutate(.data, `(Intercept)` = 1, x2b = as.numeric(x2 == &quot;b&quot;), x2c = as.numeric(x2 == &quot;c&quot;), x2d = as.numeric(x2 == &quot;d&quot;), `x1:x2b` = x1 * x2b, `x1:x2c` = x1 * x2c, `x1:x2d` = x1 * x2d ) %&gt;% select(`(Intercept)`, x1, x2b, x2c, x2d, `x1:x2b`, `x1:x2c`, `x1:x2d`) } model_matrix_mod2(sim3) #&gt; # A tibble: 120 x 8 #&gt; `(Intercept)` x1 x2b x2c x2d `x1:x2b` `x1:x2c` `x1:x2d` #&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 0 0 0 0 0 0 #&gt; 2 1 1 0 0 0 0 0 0 #&gt; 3 1 1 0 0 0 0 0 0 #&gt; 4 1 1 1 0 0 1 0 0 #&gt; 5 1 1 1 0 0 1 0 0 #&gt; 6 1 1 1 0 0 1 0 0 #&gt; # … with 114 more rows For a more general function which will handle arbitrary levels in x2, I will extend the model_matrix_mod1b() function that I wrote earlier. model_matrix_mod2b &lt;- function(.data) { # get dataset with x1 and x2 indicator variables out &lt;- model_matrix_mod1b(.data) # get names of the x2 indicator columns x2cols &lt;- str_subset(colnames(out), &quot;^x2&quot;) # create interactions between x1 and the x2 indicator columns for (varname in x2cols) { # name of the interaction variable newvar &lt;- str_c(&quot;x1:&quot;, varname) out[[newvar]] &lt;- out$x1 * out[[varname]] } out } model_matrix_mod2b(sim3) #&gt; # A tibble: 120 x 8 #&gt; `(Intercept)` x1 x2b x2c x2d `x1:x2b` `x1:x2c` `x1:x2d` #&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 0 0 0 0 0 0 #&gt; 2 1 1 0 0 0 0 0 0 #&gt; 3 1 1 0 0 0 0 0 0 #&gt; 4 1 1 1 0 0 1 0 0 #&gt; 5 1 1 1 0 0 1 0 0 #&gt; 6 1 1 1 0 0 1 0 0 #&gt; # … with 114 more rows These functions could be further generalized to allow for x1 and x2 to be either numeric or factors. However, generalizing much more than that and we will soon start reimplementing all of the matrix_model() function. Exercise 23.4.4 For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but it’s pretty subtle. Can you come up with a plot to support my claim? Estimate models mod1 and mod2 on sim4, mod1 &lt;- lm(y ~ x1 + x2, data = sim4) mod2 &lt;- lm(y ~ x1 * x2, data = sim4) and add the residuals from these models to the sim4 data, sim4_mods &lt;- gather_residuals(sim4, mod1, mod2) Frequency plots of both the residuals, ggplot(sim4_mods, aes(x = resid, colour = model)) + geom_freqpoly(binwidth = 0.5) + geom_rug() and the absolute values of the residuals, ggplot(sim4_mods, aes(x = abs(resid), colour = model)) + geom_freqpoly(binwidth = 0.5) + geom_rug() does not show much difference in the residuals between the models. However, mod2 appears to have fewer residuals in the tails of the distribution between 2.5 and 5 (although the most extreme residuals are from mod2. This is confirmed by checking the standard deviation of the residuals of these models, sim4_mods %&gt;% group_by(model) %&gt;% summarise(resid = sd(resid)) #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; # A tibble: 2 x 2 #&gt; model resid #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mod1 2.10 #&gt; 2 mod2 2.07 The standard deviation of the residuals of mod2 is smaller than that of mod1. 23.5 Missing values No exercises 23.6 Other model families No exercises "],
["model-building.html", "24 Model building 24.1 Introduction 24.2 Why are low quality diamonds more expensive? 24.3 What affects the number of daily flights? 24.4 Learning more about models", " 24 Model building 24.1 Introduction The splines package is needed for the ns() function used in one of the solutions. library(&quot;tidyverse&quot;) library(&quot;modelr&quot;) library(&quot;lubridate&quot;) library(&quot;broom&quot;) library(&quot;nycflights13&quot;) library(&quot;splines&quot;) options(na.action = na.warn) 24.2 Why are low quality diamonds more expensive? This code appears in the section and is necessary for the exercises. diamonds2 &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) %&gt;% mutate( lprice = log2(price), lcarat = log2(carat) ) mod_diamond2 &lt;- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2) diamonds2 &lt;- add_residuals(diamonds2, mod_diamond2, &quot;lresid2&quot;) Exercise 24.2.1 In the plot of lcarat vs. lprice, there are some bright vertical strips. What do they represent? The distribution of diamonds has more diamonds at round or otherwise human-friendly numbers (fractions). Exercise 24.2.2 If log(price) = a_0 + a_1 * log(carat), what does that say about the relationship between price and carat? Following the examples in the chapter, I use a base-2 logarithm. mod_log &lt;- lm(log2(price) ~ log2(carat), data = diamonds) mod_log #&gt; #&gt; Call: #&gt; lm(formula = log2(price) ~ log2(carat), data = diamonds) #&gt; #&gt; Coefficients: #&gt; (Intercept) log2(carat) #&gt; 12.19 1.68 The estimated relationship between carat and price looks like this. tibble(carat = seq(0.25, 5, by = 0.25)) %&gt;% add_predictions(mod_log) %&gt;% ggplot(aes(x = carat, y = 2^pred)) + geom_line() + labs(x = &quot;carat&quot;, y = &quot;price&quot;) The plot shows that the estimated relationship between carat and price is not linear. The exact relationship in this model is if \\(x\\) increases \\(r\\) times, then \\(y\\) increases \\(r^{a_1}\\) times. For example, a two times increase in carat is associated with the following increase in price: 2^coef(mod_log)[2] #&gt; log2(carat) #&gt; 3.2 Let’s confirm this relationship by checking it for a few values of the carat variable. Let’s increase carat from 1 to 2. 2^(predict(mod_log, newdata = tibble(carat = 2)) - predict(mod_log, newdata = tibble(carat = 1))) #&gt; 1 #&gt; 3.2 Note that, since predict() predicts log2(carat) rather than carat, the prediction is exponentiated by 2. Now let’s increase carat from 4 to 2. 2^(predict(mod_log, newdata = tibble(carat = 4)) - predict(mod_log, newdata = tibble(carat = 2))) #&gt; 1 #&gt; 3.2 Finally, let’s increase carat from 0.5 to 1. 2^(predict(mod_log, newdata = tibble(carat = 1)) - predict(mod_log, newdata = tibble(carat = 0.5))) #&gt; 1 #&gt; 3.2 All of these examples return the same value, \\(2 ^ {a_1} = 3.2\\). So why is this? Let’s ignore the names of the variables in this case and consider the equation: \\[ \\log_b y = a_0 + a_1 \\log x \\] We want to understand how the difference in \\(y\\) is related to the difference in \\(x\\). Now, consider this equation at two different values \\(x_1\\) and \\(x_0\\), \\[ \\log_b y_0 = a_0 + \\log_b x_0 \\\\ \\log_b y_1 = a_0 + \\log_b y_1 \\] What is the value of the difference, \\(\\log y_1 - \\log y_0\\)? \\[ \\begin{aligned}[t] \\log_b(y_1) - \\log_b(y_0) &amp;= (a_0 + a_1 \\log_b x_1) - (a_0 + a_1 \\log x_0) ,\\\\ &amp;= a_1 (\\log_b x_1 - \\log x_0) , \\\\ \\log_b \\left(\\frac{y_1}{y_0} \\right) &amp;= \\log_b \\left(\\frac{x_1}{x_0} \\right)^{a_1} , \\\\ \\frac{y_1}{y_0} &amp;= \\left( \\frac{x_1}{x_0} \\right)^{a_1} . \\end{aligned} \\] Let \\(s = y_1 / y_0\\) and \\(r = x_1 / x_0\\). Then, \\[ s = r^{a_1} \\text{.} \\] In other words, an \\(r\\) times increase in \\(x\\), is associated with a \\(r^{a_1}\\) times increase in \\(y\\). Note that this relationship does not depend on the base of the logarithm, \\(b\\). There is another approximation that is commonly used when logarithms appear in regressions. The first way to show this is using the approximation that \\(x\\) is small, meaning that \\(x \\approx 0\\), \\[ \\log (1 + x) \\approx x \\] This approximation is the first order Taylor expansion of the function at \\(x = 0\\). Now consider the relationship between the percent change in \\(x\\) and the percent change in \\(y\\), \\[ \\begin{aligned}[t] \\log (y + \\Delta y) - \\log y &amp;= (\\alpha + \\beta \\log (x + \\Delta x)) - (\\alpha + \\beta \\log x) \\\\ \\log \\left(\\frac{y + \\Delta y}{y} \\right) &amp;= \\beta \\log\\left( \\frac{x + \\Delta x}{x} \\right) \\\\ \\log \\left(1 + \\frac{\\Delta y}{y} \\right) &amp;= \\beta \\log\\left( 1 + \\frac{\\Delta x}{x} \\right) \\\\ \\frac{\\Delta y}{y} &amp;\\approx \\beta \\left(\\frac{\\Delta x}{x} \\right) \\end{aligned} \\] Thus a 1% percentage change in \\(x\\) is associated with a \\(\\beta\\) percent change in \\(y\\). This relationship can also be derived by taking the derivative of \\(\\log y\\) with respect to \\(x\\). First, rewrite the equation in terms of \\(y\\), \\[ y = \\exp(a_0 + a_1 \\log(x)) \\] Then differentiate \\(y\\) with respect to \\(x\\), \\[ \\begin{aligned}[t] dy &amp;= \\exp(a_0 + a_1 \\log x) \\left(\\frac{a_1}{x}\\right) dx \\\\ &amp;= a_1 y \\left(\\frac{dx}{x} \\right) \\\\ (dy / y) &amp;= a_1 (dx / x) \\\\ \\%\\Delta y &amp;= a_1\\%\\Delta x \\end{aligned} \\] Exercise 24.2.3 Extract the diamonds that have very high and very low residuals. Is there anything unusual about these diamonds? Are they particularly bad or good, or do you think these are pricing errors? The answer to this question is provided in section 24.2.2. diamonds2 %&gt;% filter(abs(lresid2) &gt; 1) %&gt;% add_predictions(mod_diamond2) %&gt;% mutate(pred = round(2^pred)) %&gt;% select(price, pred, carat:table, x:z) %&gt;% arrange(price) #&gt; # A tibble: 16 x 11 #&gt; price pred carat cut color clarity depth table x y z #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1013 264 0.25 Fair F SI2 54.4 64 4.3 4.23 2.32 #&gt; 2 1186 284 0.25 Premium G SI2 59 60 5.33 5.28 3.12 #&gt; 3 1186 284 0.25 Premium G SI2 58.8 60 5.33 5.28 3.12 #&gt; 4 1262 2644 1.03 Fair E I1 78.2 54 5.72 5.59 4.42 #&gt; 5 1415 639 0.35 Fair G VS2 65.9 54 5.57 5.53 3.66 #&gt; 6 1415 639 0.35 Fair G VS2 65.9 54 5.57 5.53 3.66 #&gt; # … with 10 more rows I did not see anything too unusual. Do you? Exercise 24.2.4 Does the final model, mod_diamonds2, do a good job of predicting diamond prices? Would you trust it to tell you how much to spend if you were buying a diamond? Section 24.2.2 already provides part of the answer to this question. Plotting the residuals of the model shows that there are some large outliers for small carat sizes. The largest of these residuals are a little over two, which means that the true value was four times lower; see Exercise 24.2.2. Most of the mass of the residuals is between -0.5 and 0.5, which corresponds to about \\(\\pm 40%\\). There seems to be a slight downward bias in the residuals as carat size increases. ggplot(diamonds2, aes(lcarat, lresid2)) + geom_hex(bins = 50) lresid2_summary &lt;- summarise(diamonds2, rmse = sqrt(mean(lresid2^2)), mae = mean(abs(lresid2)), p025 = quantile(lresid2, 0.025), p975 = quantile(lresid2, 0.975) ) lresid2_summary #&gt; # A tibble: 1 x 4 #&gt; rmse mae p025 p975 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.192 0.149 -0.369 0.384 While in some cases the model can be wrong, overall the model seems to perform well. The root mean squared error is 0.19 meaning that the average error is about -14%. Another summary statistics of errors is the mean absolute error (MAE), which is the mean of the absolute values of the errors. The MAE is 0.15, which is -11%. Finally, 95% of the residuals are between -0.37 and 0.38, which correspond to 23—31. Whether you think that this is a good model depends on factors outside the statistical model itself. It will depend on the how the model is being used. I have no idea how to price diamonds, so this would be useful to me in order to understand a reasonable price range for a diamond, so I don’t get ripped off. However, if I were buying and selling diamonds as a business, I would probably require a better model. 24.3 What affects the number of daily flights? This code is copied from the book and needed for the exercises. library(&quot;nycflights13&quot;) daily &lt;- flights %&gt;% mutate(date = make_date(year, month, day)) %&gt;% group_by(date) %&gt;% summarise(n = n()) #&gt; `summarise()` ungrouping output (override with `.groups` argument) daily #&gt; # A tibble: 365 x 2 #&gt; date n #&gt; &lt;date&gt; &lt;int&gt; #&gt; 1 2013-01-01 842 #&gt; 2 2013-01-02 943 #&gt; 3 2013-01-03 914 #&gt; 4 2013-01-04 915 #&gt; 5 2013-01-05 720 #&gt; 6 2013-01-06 832 #&gt; # … with 359 more rows daily &lt;- daily %&gt;% mutate(wday = wday(date, label = TRUE)) term &lt;- function(date) { cut(date, breaks = ymd(20130101, 20130605, 20130825, 20140101), labels = c(&quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;) ) } daily &lt;- daily %&gt;% mutate(term = term(date)) mod &lt;- lm(n ~ wday, data = daily) daily &lt;- daily %&gt;% add_residuals(mod) mod1 &lt;- lm(n ~ wday, data = daily) mod2 &lt;- lm(n ~ wday * term, data = daily) Exercise 24.3.1 Use your Google sleuthing skills to brainstorm why there were fewer than expected flights on Jan 20, May 26, and Sep 1. (Hint: they all have the same explanation.) How would these days generalize to another year? These are the Sundays before Monday holidays Martin Luther King Jr. Day, Memorial Day, and Labor Day. For other years, use the dates of the holidays for those years—the third Monday of January for Martin Luther King Jr. Day, the last Monday of May for Memorial Day, and the first Monday in September for Labor Day. Exercise 24.3.2 What do the three days with high positive residuals represent? How would these days generalize to another year? The top three days correspond to the Saturday after Thanksgiving (November 30th), the Sunday after Thanksgiving (December 1st), and the Saturday after Christmas (December 28th). top_n(daily, 3, resid) #&gt; # A tibble: 3 x 5 #&gt; date n wday term resid #&gt; &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 2013-11-30 857 Sat fall 112. #&gt; 2 2013-12-01 987 Sun fall 95.5 #&gt; 3 2013-12-28 814 Sat fall 69.4 We could generalize these to other years using the dates of those holidays on those years. Exercise 24.3.3 Create a new variable that splits the wday variable into terms, but only for Saturdays, i.e., it should have Thurs, Fri, but Sat-summer, Sat-spring, Sat-fall How does this model compare with the model with every combination of wday and term? I’ll use the function case_when() to do this, though there are other ways which it could be solved. daily &lt;- daily %&gt;% mutate( wday2 = case_when( wday == &quot;Sat&quot; &amp; term == &quot;summer&quot; ~ &quot;Sat-summer&quot;, wday == &quot;Sat&quot; &amp; term == &quot;fall&quot; ~ &quot;Sat-fall&quot;, wday == &quot;Sat&quot; &amp; term == &quot;spring&quot; ~ &quot;Sat-spring&quot;, TRUE ~ as.character(wday) ) ) mod3 &lt;- lm(n ~ wday2, data = daily) daily %&gt;% gather_residuals(sat_term = mod3, all_interact = mod2) %&gt;% ggplot(aes(date, resid, colour = model)) + geom_line(alpha = 0.75) I think the overlapping plot is hard to understand. If we are interested in the differences, it is better to plot the differences directly. In this code, I use spread_residuals() to add one column per model, rather than gather_residuals() which creates a new row for each model. daily %&gt;% spread_residuals(sat_term = mod3, all_interact = mod2) %&gt;% mutate(resid_diff = sat_term - all_interact) %&gt;% ggplot(aes(date, resid_diff)) + geom_line(alpha = 0.75) The model with terms × Saturday has higher residuals in the fall and lower residuals in the spring than the model with all interactions. Comparing models, mod3 has a lower \\(R^2\\) and regression standard error, \\(\\hat{\\sigma}\\), despite using fewer variables. More importantly for prediction purposes, this model has a higher AIC, which is an estimate of the out of sample error. glance(mod3) %&gt;% select(r.squared, sigma, AIC, df) #&gt; # A tibble: 1 x 4 #&gt; r.squared sigma AIC df #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 0.736 47.4 3863. 9 glance(mod2) %&gt;% select(r.squared, sigma, AIC, df) #&gt; # A tibble: 1 x 4 #&gt; r.squared sigma AIC df #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 0.757 46.2 3856. 21 Exercise 24.3.4 Create a new wday variable that combines the day of week, term (for Saturdays), and public holidays. What do the residuals of that model look like? The question is unclear how to handle public holidays. There are several questions to consider. First, what are the public holidays? I include all federal holidays in the United States in 2013. Other holidays to consider would be Easter and Good Friday which is US stock market holiday and widely celebrated religious holiday, Mothers Day, Fathers Day, and Patriots’ Day, which is a holiday in several states, and other state holidays. holidays_2013 &lt;- tribble( ~holiday, ~date, &quot;New Year&#39;s Day&quot;, 20130101, &quot;Martin Luther King Jr. Day&quot;, 20130121, &quot;Washington&#39;s Birthday&quot;, 20130218, &quot;Memorial Day&quot;, 20130527, &quot;Independence Day&quot;, 20130704, &quot;Labor Day&quot;, 20130902, &quot;Columbus Day&quot;, 20131028, &quot;Veteran&#39;s Day&quot;, 20131111, &quot;Thanksgiving&quot;, 20131128, &quot;Christmas&quot;, 20131225 ) %&gt;% mutate(date = lubridate::ymd(date)) The model could include a single dummy variable which indicates a day was a public holiday. Alternatively, I could include a dummy variable for each public holiday. I would expect that Veteran’s Day and Washington’s Birthday have a different effect on travel than Thanksgiving, Christmas, and New Year’s Day. Another question is whether and how I should handle the days before and after holidays. Travel could be lighter on the day of the holiday, but heavier the day before or after. daily &lt;- daily %&gt;% mutate( wday3 = case_when( date %in% (holidays_2013$date - 1L) ~ &quot;day before holiday&quot;, date %in% (holidays_2013$date + 1L) ~ &quot;day after holiday&quot;, date %in% holidays_2013$date ~ &quot;holiday&quot;, .$wday == &quot;Sat&quot; &amp; .$term == &quot;summer&quot; ~ &quot;Sat-summer&quot;, .$wday == &quot;Sat&quot; &amp; .$term == &quot;fall&quot; ~ &quot;Sat-fall&quot;, .$wday == &quot;Sat&quot; &amp; .$term == &quot;spring&quot; ~ &quot;Sat-spring&quot;, TRUE ~ as.character(.$wday) ) ) mod4 &lt;- lm(n ~ wday3, data = daily) daily %&gt;% spread_residuals(resid_sat_terms = mod3, resid_holidays = mod4) %&gt;% mutate(resid_diff = resid_holidays - resid_sat_terms) %&gt;% ggplot(aes(date, resid_diff)) + geom_line(alpha = 0.75) Exercise 24.3.5 What happens if you fit a day of week effect that varies by month (i.e., n ~ wday * month)? Why is this not very helpful? daily &lt;- mutate(daily, month = factor(lubridate::month(date))) mod6 &lt;- lm(n ~ wday * month, data = daily) print(summary(mod6)) #&gt; #&gt; Call: #&gt; lm(formula = n ~ wday * month, data = daily) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -269.2 -5.0 1.5 8.8 113.2 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 867.400 7.598 114.16 &lt; 2e-16 *** #&gt; wday.L -64.074 20.874 -3.07 0.00235 ** #&gt; wday.Q -165.600 20.156 -8.22 7.8e-15 *** #&gt; wday.C -68.259 20.312 -3.36 0.00089 *** #&gt; wday^4 -92.081 20.499 -4.49 1.0e-05 *** #&gt; wday^5 9.793 19.733 0.50 0.62011 #&gt; wday^6 -20.438 18.992 -1.08 0.28280 #&gt; month2 23.707 10.995 2.16 0.03191 * #&gt; month3 67.886 10.746 6.32 1.0e-09 *** #&gt; month4 74.593 10.829 6.89 3.7e-11 *** #&gt; month5 56.279 10.746 5.24 3.2e-07 *** #&gt; month6 80.307 10.829 7.42 1.4e-12 *** #&gt; month7 77.114 10.746 7.18 6.4e-12 *** #&gt; month8 81.636 10.746 7.60 4.5e-13 *** #&gt; month9 51.371 10.829 4.74 3.3e-06 *** #&gt; month10 60.136 10.746 5.60 5.2e-08 *** #&gt; month11 46.914 10.829 4.33 2.1e-05 *** #&gt; month12 38.779 10.746 3.61 0.00036 *** #&gt; wday.L:month2 -3.723 29.627 -0.13 0.90009 #&gt; wday.Q:month2 -3.819 29.125 -0.13 0.89578 #&gt; wday.C:month2 0.490 29.233 0.02 0.98664 #&gt; wday^4:month2 4.569 29.364 0.16 0.87646 #&gt; wday^5:month2 -4.255 28.835 -0.15 0.88278 #&gt; wday^6:month2 12.057 28.332 0.43 0.67076 #&gt; wday.L:month3 -14.571 28.430 -0.51 0.60870 #&gt; wday.Q:month3 15.439 28.207 0.55 0.58458 #&gt; wday.C:month3 8.226 28.467 0.29 0.77282 #&gt; wday^4:month3 22.720 28.702 0.79 0.42926 #&gt; wday^5:month3 -15.330 28.504 -0.54 0.59113 #&gt; wday^6:month3 11.373 28.268 0.40 0.68776 #&gt; wday.L:month4 -16.668 29.359 -0.57 0.57067 #&gt; wday.Q:month4 10.725 28.962 0.37 0.71142 #&gt; wday.C:month4 -0.245 28.725 -0.01 0.99320 #&gt; wday^4:month4 23.288 28.871 0.81 0.42056 #&gt; wday^5:month4 -17.872 28.076 -0.64 0.52494 #&gt; wday^6:month4 5.352 27.888 0.19 0.84794 #&gt; wday.L:month5 3.666 29.359 0.12 0.90071 #&gt; wday.Q:month5 -20.665 28.670 -0.72 0.47163 #&gt; wday.C:month5 4.634 28.725 0.16 0.87196 #&gt; wday^4:month5 5.999 28.511 0.21 0.83349 #&gt; wday^5:month5 -16.912 28.076 -0.60 0.54742 #&gt; wday^6:month5 12.764 27.194 0.47 0.63916 #&gt; wday.L:month6 -4.526 28.651 -0.16 0.87459 #&gt; wday.Q:month6 23.813 28.207 0.84 0.39927 #&gt; wday.C:month6 13.758 28.725 0.48 0.63234 #&gt; wday^4:month6 24.118 29.187 0.83 0.40932 #&gt; wday^5:month6 -17.648 28.798 -0.61 0.54048 #&gt; wday^6:month6 10.526 28.329 0.37 0.71051 #&gt; wday.L:month7 -28.791 29.359 -0.98 0.32760 #&gt; wday.Q:month7 49.585 28.670 1.73 0.08482 . #&gt; wday.C:month7 54.501 28.725 1.90 0.05881 . #&gt; wday^4:month7 50.847 28.511 1.78 0.07559 . #&gt; wday^5:month7 -33.698 28.076 -1.20 0.23106 #&gt; wday^6:month7 -13.894 27.194 -0.51 0.60979 #&gt; wday.L:month8 -20.448 28.871 -0.71 0.47938 #&gt; wday.Q:month8 6.765 28.504 0.24 0.81258 #&gt; wday.C:month8 6.001 28.467 0.21 0.83319 #&gt; wday^4:month8 19.074 28.781 0.66 0.50806 #&gt; wday^5:month8 -19.312 28.058 -0.69 0.49183 #&gt; wday^6:month8 9.507 27.887 0.34 0.73341 #&gt; wday.L:month9 -30.341 28.926 -1.05 0.29511 #&gt; wday.Q:month9 -42.034 28.670 -1.47 0.14373 #&gt; wday.C:month9 -20.719 28.725 -0.72 0.47134 #&gt; wday^4:month9 -20.375 28.791 -0.71 0.47973 #&gt; wday^5:month9 -18.238 28.523 -0.64 0.52308 #&gt; wday^6:month9 11.726 28.270 0.41 0.67861 #&gt; wday.L:month10 -61.051 29.520 -2.07 0.03954 * #&gt; wday.Q:month10 -26.235 28.504 -0.92 0.35815 #&gt; wday.C:month10 -32.435 28.725 -1.13 0.25979 #&gt; wday^4:month10 -12.212 28.990 -0.42 0.67389 #&gt; wday^5:month10 -27.686 27.907 -0.99 0.32201 #&gt; wday^6:month10 0.123 26.859 0.00 0.99634 #&gt; wday.L:month11 -54.947 28.926 -1.90 0.05851 . #&gt; wday.Q:month11 16.012 28.670 0.56 0.57696 #&gt; wday.C:month11 54.950 28.725 1.91 0.05677 . #&gt; wday^4:month11 47.286 28.791 1.64 0.10164 #&gt; wday^5:month11 -44.740 28.523 -1.57 0.11787 #&gt; wday^6:month11 -20.688 28.270 -0.73 0.46491 #&gt; wday.L:month12 -9.506 28.871 -0.33 0.74221 #&gt; wday.Q:month12 75.209 28.504 2.64 0.00879 ** #&gt; wday.C:month12 -25.026 28.467 -0.88 0.38010 #&gt; wday^4:month12 -23.780 28.781 -0.83 0.40938 #&gt; wday^5:month12 20.447 28.058 0.73 0.46676 #&gt; wday^6:month12 9.586 27.887 0.34 0.73128 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 42 on 281 degrees of freedom #&gt; Multiple R-squared: 0.836, Adjusted R-squared: 0.787 #&gt; F-statistic: 17.2 on 83 and 281 DF, p-value: &lt;2e-16 If we fit a day of week effect that varies by month, there will be 12 * 7 = 84 parameters in the model. Since each month has only four to five weeks, each of these day of week \\(\\times\\) month effects is the average of only four or five observations. These estimates have large standard errors and likely not generalize well beyond the sample data, since they are estimated from only a few observations. Exercise 24.3.6 What would you expect the model n ~ wday + ns(date, 5) to look like? Knowing what you know about the data, why would you expect it to be not particularly effective? Previous models fit in the chapter and exercises show that the effects of days of the week vary across different times of the year. The model wday + ns(date, 5) does not interact the day of week effect (wday) with the time of year effects (ns(date, 5)). I estimate a model which does not interact the day of week effects (mod7) with the spline to that which does (mod8). I need to load the splines package to use the ns() function. mod7 &lt;- lm(n ~ wday + ns(date, 5), data = daily) mod8 &lt;- lm(n ~ wday * ns(date, 5), data = daily) The residuals of the model that does not interact day of week with time of year (mod7) are larger than those of the model that does (mod8). The model mod7 underestimates weekends during the summer and overestimates weekends during the autumn. daily %&gt;% gather_residuals(mod7, mod8) %&gt;% ggplot(aes(x = date, y = resid, color = model)) + geom_line(alpha = 0.75) Exercise 24.3.7 We hypothesized that people leaving on Sundays are more likely to be business travelers who need to be somewhere on Monday. Explore that hypothesis by seeing how it breaks down based on distance and time: if it’s true, you’d expect to see more Sunday evening flights to places that are far away. Comparing the average distances of flights by day of week, Sunday flights are the second longest. Saturday flights are the longest on average. Saturday may have the longest flights on average because there are fewer regularly scheduled short business/commuter flights on the weekends but that is speculation. flights %&gt;% mutate( date = make_date(year, month, day), wday = wday(date, label = TRUE) ) %&gt;% ggplot(aes(y = distance, x = wday)) + geom_boxplot() + labs(x = &quot;Day of Week&quot;, y = &quot;Average Distance&quot;) Hide outliers. flights %&gt;% mutate( date = make_date(year, month, day), wday = wday(date, label = TRUE) ) %&gt;% ggplot(aes(y = distance, x = wday)) + geom_boxplot(outlier.shape = NA) + labs(x = &quot;Day of Week&quot;, y = &quot;Average Distance&quot;) Try pointrange with mean and standard error of the mean (sd / sqrt(n)). flights %&gt;% mutate( date = make_date(year, month, day), wday = wday(date, label = TRUE) ) %&gt;% ggplot(aes(y = distance, x = wday)) + stat_summary() + labs(x = &quot;Day of Week&quot;, y = &quot;Average Distance&quot;) #&gt; No summary function supplied, defaulting to `mean_se()` Try pointrange with mean and standard error of the mean (sd / sqrt(n)). flights %&gt;% mutate( date = make_date(year, month, day), wday = wday(date, label = TRUE) ) %&gt;% ggplot(aes(y = distance, x = wday)) + geom_violin() + labs(x = &quot;Day of Week&quot;, y = &quot;Average Distance&quot;) flights %&gt;% mutate( date = make_date(year, month, day), wday = wday(date, label = TRUE) ) %&gt;% filter( distance &lt; 3000, hour &gt;= 5, hour &lt;= 21 ) %&gt;% ggplot(aes(x = hour, color = wday, y = ..density..)) + geom_freqpoly(binwidth = 1) flights %&gt;% mutate( date = make_date(year, month, day), wday = wday(date, label = TRUE) ) %&gt;% filter( distance &lt; 3000, hour &gt;= 5, hour &lt;= 21 ) %&gt;% group_by(wday, hour) %&gt;% summarise(distance = mean(distance)) %&gt;% ggplot(aes(x = hour, color = wday, y = distance)) + geom_line() #&gt; `summarise()` regrouping output by &#39;wday&#39; (override with `.groups` argument) flights %&gt;% mutate( date = make_date(year, month, day), wday = wday(date, label = TRUE) ) %&gt;% filter( distance &lt; 3000, hour &gt;= 5, hour &lt;= 21 ) %&gt;% group_by(wday, hour) %&gt;% summarise(distance = sum(distance)) %&gt;% group_by(wday) %&gt;% mutate(prop_distance = distance / sum(distance)) %&gt;% ungroup() %&gt;% ggplot(aes(x = hour, color = wday, y = prop_distance)) + geom_line() #&gt; `summarise()` regrouping output by &#39;wday&#39; (override with `.groups` argument) Exercise 24.3.8 It’s a little frustrating that Sunday and Saturday are on separate ends of the plot. Write a small function to set the levels of the factor so that the week starts on Monday. See the chapter Factors for the function fct_relevel(). Use fct_relevel() to put all levels in-front of the first level (“Sunday”). monday_first &lt;- function(x) { fct_relevel(x, levels(x)[-1]) } Now Monday is the first day of the week. daily &lt;- daily %&gt;% mutate(wday = wday(date, label = TRUE)) ggplot(daily, aes(monday_first(wday), n)) + geom_boxplot() + labs(x = &quot;Day of Week&quot;, y = &quot;Number of flights&quot;) 24.4 Learning more about models No exercises "],
["many-models.html", "25 Many models 25.1 Introduction 25.2 gapminder 25.3 List-columns 25.4 Creating list-columns 25.5 Simplifying list-columns 25.6 Making tidy data with broom", " 25 Many models 25.1 Introduction library(&quot;modelr&quot;) library(&quot;tidyverse&quot;) library(&quot;gapminder&quot;) 25.2 gapminder Exercise 25.2.1 A linear trend seems to be slightly too simple for the overall trend. Can you do better with a quadratic polynomial? How can you interpret the coefficients of the quadratic? Hint you might want to transform year so that it has mean zero.) The following code replicates the analysis in the chapter but replaces the function country_model() with a regression that includes the year squared. lifeExp ~ poly(year, 2) country_model &lt;- function(df) { lm(lifeExp ~ poly(year - median(year), 2), data = df) } by_country &lt;- gapminder %&gt;% group_by(country, continent) %&gt;% nest() by_country &lt;- by_country %&gt;% mutate(model = map(data, country_model)) by_country &lt;- by_country %&gt;% mutate( resids = map2(data, model, add_residuals) ) by_country #&gt; # A tibble: 142 x 5 #&gt; # Groups: country, continent [142] #&gt; country continent data model resids #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Afghanistan Asia &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 2 Albania Europe &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 3 Algeria Africa &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 4 Angola Africa &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 5 Argentina Americas &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 6 Australia Oceania &lt;tibble [12 × 4]&gt; &lt;lm&gt; &lt;tibble [12 × 5]&gt; #&gt; # … with 136 more rows unnest(by_country, resids) %&gt;% ggplot(aes(year, resid)) + geom_line(aes(group = country), alpha = 1 / 3) + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; by_country %&gt;% mutate(glance = map(model, broom::glance)) %&gt;% unnest(glance, .drop = TRUE) %&gt;% ggplot(aes(continent, r.squared)) + geom_jitter(width = 0.5) #&gt; Warning: The `.drop` argument of `unnest()` is deprecated as of tidyr 1.0.0. #&gt; All list-columns are now preserved. #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_warnings()` to see where this warning was generated. Exercise 25.2.2 Explore other methods for visualizing the distribution of \\(R^2\\) per continent. You might want to try the ggbeeswarm package, which provides similar methods for avoiding overlaps as jitter, but uses deterministic methods. See exercise 7.5.1.1.6 for more on ggbeeswarm library(&quot;ggbeeswarm&quot;) by_country %&gt;% mutate(glance = map(model, broom::glance)) %&gt;% unnest(glance, .drop = TRUE) %&gt;% ggplot(aes(continent, r.squared)) + geom_beeswarm() Exercise 25.2.3 To create the last plot (showing the data for the countries with the worst model fits), we needed two steps: we created a data frame with one row per country and then semi-joined it to the original dataset. It’s possible to avoid this join if we use unnest() instead of unnest(.drop = TRUE). How? gapminder %&gt;% group_by(country, continent) %&gt;% nest() %&gt;% mutate(model = map(data, ~lm(lifeExp ~ year, .))) %&gt;% mutate(glance = map(model, broom::glance)) %&gt;% unnest(glance) %&gt;% unnest(data) %&gt;% filter(r.squared &lt; 0.25) %&gt;% ggplot(aes(year, lifeExp)) + geom_line(aes(color = country)) 25.3 List-columns No exercises 25.4 Creating list-columns Exercise 25.4.1 List all the functions that you can think of that take a atomic vector and return a list. Many functions in the stringr package take a character vector as input and return a list. str_split(sentences[1:3], &quot; &quot;) #&gt; [[1]] #&gt; [1] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; #&gt; [8] &quot;planks.&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; #&gt; [6] &quot;dark&quot; &quot;blue&quot; &quot;background.&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;It&#39;s&quot; &quot;easy&quot; &quot;to&quot; &quot;tell&quot; &quot;the&quot; &quot;depth&quot; &quot;of&quot; &quot;a&quot; &quot;well.&quot; str_match_all(c(&quot;abc&quot;, &quot;aa&quot;, &quot;aabaa&quot;, &quot;abbbc&quot;), &quot;a+&quot;) #&gt; [[1]] #&gt; [,1] #&gt; [1,] &quot;a&quot; #&gt; #&gt; [[2]] #&gt; [,1] #&gt; [1,] &quot;aa&quot; #&gt; #&gt; [[3]] #&gt; [,1] #&gt; [1,] &quot;aa&quot; #&gt; [2,] &quot;aa&quot; #&gt; #&gt; [[4]] #&gt; [,1] #&gt; [1,] &quot;a&quot; The map() function takes a vector and always returns a list. map(1:3, runif) #&gt; [[1]] #&gt; [1] 0.601 #&gt; #&gt; [[2]] #&gt; [1] 0.1572 0.0074 #&gt; #&gt; [[3]] #&gt; [1] 0.466 0.498 0.290 Exercise 25.4.2 Brainstorm useful summary functions that, like quantile(), return multiple values. Some examples of summary functions that return multiple values are the following. range(mtcars$mpg) #&gt; [1] 10.4 33.9 fivenum(mtcars$mpg) #&gt; [1] 10.4 15.3 19.2 22.8 33.9 boxplot.stats(mtcars$mpg) #&gt; $stats #&gt; [1] 10.4 15.3 19.2 22.8 33.9 #&gt; #&gt; $n #&gt; [1] 32 #&gt; #&gt; $conf #&gt; [1] 17.1 21.3 #&gt; #&gt; $out #&gt; numeric(0) Exercise 25.4.3 What’s missing in the following data frame? How does quantile() return that missing piece? Why isn’t that helpful here? mtcars %&gt;% group_by(cyl) %&gt;% summarise(q = list(quantile(mpg))) %&gt;% unnest() #&gt; `summarise()` ungrouping output (override with `.groups` argument) #&gt; Warning: `cols` is now required when using unnest(). #&gt; Please use `cols = c(q)` #&gt; # A tibble: 15 x 2 #&gt; cyl q #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 21.4 #&gt; 2 4 22.8 #&gt; 3 4 26 #&gt; 4 4 30.4 #&gt; 5 4 33.9 #&gt; 6 6 17.8 #&gt; # … with 9 more rows The particular quantiles of the values are missing, e.g. 0%, 25%, 50%, 75%, 100%. quantile() returns these in the names of the vector. quantile(mtcars$mpg) #&gt; 0% 25% 50% 75% 100% #&gt; 10.4 15.4 19.2 22.8 33.9 Since the unnest function drops the names of the vector, they aren’t useful here. Exercise 25.4.4 What does this code do? Why might might it be useful? mtcars %&gt;% group_by(cyl) %&gt;% summarise_each(funs(list)) mtcars %&gt;% group_by(cyl) %&gt;% summarise_each(funs(list)) #&gt; Warning: `summarise_each_()` is deprecated as of dplyr 0.7.0. #&gt; Please use `across()` instead. #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_warnings()` to see where this warning was generated. #&gt; Warning: `funs()` is deprecated as of dplyr 0.8.0. #&gt; Please use a list of either functions or lambdas: #&gt; #&gt; # Simple named list: #&gt; list(mean = mean, median = median) #&gt; #&gt; # Auto named with `tibble::lst()`: #&gt; tibble::lst(mean, median) #&gt; #&gt; # Using lambdas #&gt; list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_warnings()` to see where this warning was generated. #&gt; # A tibble: 3 x 11 #&gt; cyl mpg disp hp drat wt qsec vs am gear carb #&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 4 &lt;dbl [… &lt;dbl [… &lt;dbl [… &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … #&gt; 2 6 &lt;dbl [… &lt;dbl [… &lt;dbl [… &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … #&gt; 3 8 &lt;dbl [… &lt;dbl [… &lt;dbl [… &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … &lt;dbl … It creates a data frame in which each row corresponds to a value of cyl, and each observation for each column (other than cyl) is a vector of all the values of that column for that value of cyl. It seems like it should be useful to have all the observations of each variable for each group, but off the top of my head, I can’t think of a specific use for this. But, it seems that it may do many things that dplyr::do does. 25.5 Simplifying list-columns Exercise 25.5.1 Why might the lengths() function be useful for creating atomic vector columns from list-columns? The lengths() function returns the lengths of each element in a list. It could be useful for testing whether all elements in a list-column are the same length. You could get the maximum length to determine how many atomic vector columns to create. It is also a replacement for something like map_int(x, length) or sapply(x, length). Exercise 25.5.2 List the most common types of vector found in a data frame. What makes lists different? The common types of vectors in data frames are: logical numeric integer character factor All of the common types of vectors in data frames are atomic. Lists are not atomic since they can contain other lists and other vectors. 25.6 Making tidy data with broom No exercises "],
["communicate-intro.html", "26 Introduction", " 26 Introduction No exercises "],
["r-markdown.html", "27 R Markdown 27.1 Introduction 27.2 R Markdown basics 27.3 Text formatting with Markdown 27.4 Code chunks 27.5 Troubleshooting 27.6 YAML header 27.7 Learning more", " 27 R Markdown 27.1 Introduction 27.2 R Markdown basics Exercise 27.2.1 Create a new notebook using File &gt; New File &gt; R Notebook. Read the instructions. Practice running the chunks. Verify that you can modify the code, re-run it, and see modified output. This exercise is left to the reader. Exercise 27.2.2 Create a new R Markdown document with File &gt; New File &gt; R Markdown …. Knit it by clicking the appropriate button. Knit it by using the appropriate keyboard short cut. Verify that you can modify the input and see the output update. This exercise is mostly left to the reader. Recall that the keyboard shortcut to knit a file is Cmd/Ctrl + Alt + K. Exercise 27.2.3 Compare and contrast the R notebook and R markdown files you created above. How are the outputs similar? How are they different? How are the inputs similar? How are they different? What happens if you copy the YAML header from one to the other? R notebook files show the output of code chunks inside the editor, while hiding the console, when they are edited in RStudio. This contrasts with R markdown files, which show their output inside the console, and do not show output inside the editor. This makes R notebook documents appealing for interactive exploration. In this R markdown file, the plot is displayed in the “Plot” tab, while the output of summary() is displayed in the tab. However, when this same file is converted to a R notebook, the plot and summary() output are displayed in the “Editor” below the chunk of code which created them. Both R notebooks and R markdown files and can be knit to produce HTML output. R markdown files can be knit to a variety of formats including HTML, PDF, and DOCX. However, R notebooks can only be knit to HTML files, which are given the extension .nb.html. However, unlike R markdown files knit to HTML, the HTML output of an R notebook includes copy of the original .Rmd source. If a .nb.html file is opened in RStudio, the source of the .Rmd file can be extracted and edited. In contrast, there is no way to recover the original source of an R markdown file from its output, except through the parts that are displayed in the output itself. R markdown files and R notebooks differ in the value of output in their YAML headers. The YAML header for the R notebook will have the line, --- ouptut: html_notebook --- For example, this is a R notebook, --- title: &quot;Diamond sizes&quot; date: 2016-08-25 output: html_notebook --- Text of the document. The YAML header for the R markdown file will have the line, ouptut: html_document For example, this is a R markdown file. --- title: &quot;Diamond sizes&quot; date: 2016-08-25 output: html_document --- Text of the document. Copying the YAML header from an R notebook to a R markdown file changes it to an R notebook, and vice-versa. More specifically, an .Rmd file can be changed to R markdown file or R notebook by changing the value of the output key in the header. The RStudio IDE and the rmarkdown package both use the YAML header of an .Rmd file to determine the document-type of the file. For more information on R markdown notebooks see the following sources: R Markdown: The Definitive Guide section), Chapter Notebook Difference between R MarkDown and R NoteBook StackOverflow thread. Exercise 27.2.4 Create one new R Markdown document for each of the three built-in formats: HTML, PDF and Word. Knit each of the three documents. How does the output differ? How does the input differ? (You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary.) They produce different outputs, both in the final documents and intermediate files (notably the type of plots produced). The only difference in the inputs is the value of output in the YAML header. The following .Rmd would be knit to HTML. --- title: &quot;Diamond sizes&quot; date: 2016-08-25 output: html_document --- Text of the document. If the value of the output key is changed to word_document, knitting the file will create a Word document (DOCX). --- title: &quot;Diamond sizes&quot; date: 2016-08-25 output: word_document --- Text of the document. Similarly, if the value of the output key is changed to pdf_document, knitting the file will create a PDF. --- title: &quot;Diamond sizes&quot; date: 2016-08-25 output: pdf_document --- Text of the document. If you click on the Knit menu button and then on one of Knit to HTML, Knit to PDF, or Knit to Word, you will see that the value of the output key will change to html_document, pdf_document, or word_document, respectively. You will see that the value of output will look a little different than the previous examples. It will add a new line with a value like, pdf_document: default. --- title: &quot;Diamond sizes&quot; date: 2016-08-25 output: pdf_document: default --- Text of the document. This format is more general, allows the document have multiple output formats as well as configuration settings that allow more fine-grained control over the look of the output format. The chapter R Markdown Formats discusses output formats for R markdown files in more detail. 27.3 Text formatting with Markdown Exercise 27.3.1 Practice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold. A minimal example is the following CV. --- title: &quot;Hadley Wickham&quot; --- ## Employment - Chief Scientist, Rstudio, **2013--present**. - Adjust Professor, Rice University, Houston, TX, **2013--present**. - Assistant Professor, Rice University, Houston, TX, **2008--12**. ## Education - Ph.D. in Statistics, Iowa State University, Ames, IA, **2008** - M.Sc. in Statistics, University of Auckland, New Zealand, **2004** - B.Sc. in Statistics and Computer Science, First Class Honours, The University of Auckland, New Zealand, **2002**. - Bachelor of Human Biology, First Class Honours, The University of Auckland, Auckland, New Zealand, **1999**. Your own example could be much more detailed. Exercise 27.3.2 Using the R Markdown quick reference, figure out how to: Add a footnote. Add a horizontal rule. Add a block quote. --- title: Horizontal Rules, Block Quotes, and Footnotes --- The quick brown fox jumped over the lazy dog.[^quick-fox] Use three or more `-` for a horizontal rule. For example, --- The horizontal rule uses the same syntax as a YAML block? So how does R markdown distinguish between the two? Three dashes (&quot;---&quot;) is only treated the start of a YAML block if it is at the start of the document. &gt; This would be a block quote. Generally, block quotes are used to indicate &gt; quotes longer than a three or four lines. [^quick-fox]: This is an example of a footnote. The sentence this is footnoting is often used for displaying fonts because it includes all 26 letters of the English alphabet. Exercise 27.3.3 Copy and paste the contents of diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown in to a local R markdown document. Check that you can run it, then add text after the frequency polygon that describes its most striking features. The following R markdown document answers this question as well as exercises Exercise 27.4.1, Exercise 27.4.2, and Exercise 27.4.3. --- title: &quot;Diamond sizes&quot; output: html_document date: &#39;2018-07-15&#39; --- ```{r knitr_opts, include = FALSE} knitr::opts_chunk$set(echo = FALSE) ``` ```{r setup, message = FALSE} library(&quot;ggplot2&quot;) library(&quot;dplyr&quot;) ``` ```{r} smaller &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) ``` ```{r include = FALSE, purl = FALSE} # Hide objects and functions ONLY used inline n_larger &lt;- nrow(diamonds) - nrow(smaller) pct_larger &lt;- n_larger / nrow(diamonds) * 100 comma &lt;- function(x) { format(x, digits = 2, big.mark = &quot;,&quot;) } ``` ## Size and Cut, Color, and Clarity Diamonds with lower quality cuts (cuts are ranked from &quot;Ideal&quot; to &quot;Fair&quot;) tend to be be larger. ```{r} ggplot(diamonds, aes(y = carat, x = cut)) + geom_boxplot() ``` Likewise, diamonds with worse color (diamond colors are ranked from J (worst) to D (best)) tend to be larger: ```{r} ggplot(diamonds, aes(y = carat, x = color)) + geom_boxplot() ``` The pattern present in cut and color is also present in clarity. Diamonds with worse clarity (I1 (worst), SI1, SI2, VS1, VS2, VVS1, VVS2, IF (best)) tend to be larger: ```{r} ggplot(diamonds, aes(y = carat, x = clarity)) + geom_boxplot() ``` These patterns are consistent with there being a profitability threshold for retail diamonds that is a function of carat, clarity, color, cut and other characteristics. A diamond may be profitable to sell if a poor value of one feature, for example, poor clarity, color, or cut, is be offset by a good value of another feature, such as a large size. This can be considered an example of [Berkson&#39;s paradox](https://en.wikipedia.org/wiki/Berkson%27s_paradox). ## Largest Diamonds We have data about `r comma(nrow(diamonds))` diamonds. Only `r n_larger` (`r round(pct_larger, 1)`%) are larger than 2.5 carats. The distribution of the remainder is shown below: ```{r} smaller %&gt;% ggplot(aes(carat)) + geom_freqpoly(binwidth = 0.01) ``` The frequency distribution of diamond sizes is marked by spikes at whole-number and half-carat values, as well as several other carat values corresponding to fractions. The largest twenty diamonds (by carat) in the datasets are, ```{r results = &quot;asis&quot;} diamonds %&gt;% arrange(desc(carat)) %&gt;% slice(1:20) %&gt;% select(carat, cut, color, clarity) %&gt;% knitr::kable( caption = &quot;The largest 20 diamonds in the `diamonds` dataset.&quot; ) ``` Most of the twenty largest datasets are in the lowest clarity category (&quot;I1&quot;), with one being in the second best category (&quot;VVS2&quot;) The top twenty diamonds have colors ranging from the worst, &quot;J&quot;, to best, &quot;D&quot;,categories, though most are in the lower categories &quot;J&quot; and &quot;I&quot;. The top twenty diamonds are more evenly distributed among the cut categories, from &quot;Fair&quot; to &quot;Ideal&quot;, although the worst category (Fair) is the most common. 27.4 Code chunks Exercise 27.4.1 Add a section that explores how diamond sizes vary by cut, color, and clarity. Assume you’re writing a report for someone who doesn’t know R, and instead of setting echo = FALSE on each chunk, set a global option. See the answer to Exercise 27.3.3. Exercise 27.4.2 Download diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown. Add a section that describes the largest 20 diamonds, including a table that displays their most important attributes. See the answer to Exercise 27.3.3. I use arrange() and slice() to select the largest twenty diamonds, and knitr::kable() to produce a formatted table. Exercise 27.4.3 Modify diamonds-sizes.Rmd to use comma() to produce nicely formatted output. Also include the percentage of diamonds that are larger than 2.5 carats. See the answer to Exercise 27.3.3. I moved the computation of the number larger and percent of diamonds larger than 2.5 carats into a code chunk. I find that it is best to keep inline R expressions simple, usually consisting of an object and a formatting function. This makes it both easier to read and test the R code, while simultaneously making the prose easier to read. It helps the readability of the code and document to keep the computation of objects used in prose close to their use. Calculating those objects in a code chunk with the include = FALSE option (as is done in diamonds-size.Rmd) is useful in this regard. Exercise 27.4.4 Set up a network of chunks where d depends on c and b, and both b and c depend on a. Have each chunk print lubridate::now(), set cache = TRUE, then verify your understanding of caching. --- title: &quot;Exercise 24.4.7.4&quot; author: &quot;Jeffrey Arnold&quot; date: &quot;2/1/2018&quot; output: html_document --- ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE, cache = TRUE) ``` The chunk `a` has no dependencies. ```{r a} print(lubridate::now()) x &lt;- 1 ``` The chunk `b` depends on `a`. ```{r b, dependson = c(&quot;a&quot;)} print(lubridate::now()) y &lt;- x + 1 ``` The chunk `c` depends on `a`. ```{r c, dependson = c(&quot;a&quot;)} print(lubridate::now()) z &lt;- x * 2 ``` The chunk `d` depends on `c` and `b`: ```{r d, dependson = c(&quot;c&quot;, &quot;b&quot;)} print(lubridate::now()) w &lt;- y + z ``` If this document is knit repeatedly, the value printed by `lubridate::now()` will be the same for all chunks, and the same as the first time the document was run with caching. 27.5 Troubleshooting No exercises 27.6 YAML header No exercises 27.7 Learning more No exercises "],
["graphics-for-communication.html", "28 Graphics for communication 28.1 Introduction 28.2 Label 28.3 Annotations 28.4 Scales 28.5 Zooming 28.6 Themes 28.7 Saving your plots 28.8 Learning more", " 28 Graphics for communication 28.1 Introduction library(&quot;tidyverse&quot;) library(&quot;modelr&quot;) library(&quot;lubridate&quot;) 28.2 Label Exercise 28.2.1 Create one plot on the fuel economy data with customized title, subtitle, caption, x, y, and colour labels. ggplot( data = mpg, mapping = aes(x = fct_reorder(class, hwy), y = hwy) ) + geom_boxplot() + coord_flip() + labs( title = &quot;Compact Cars have &gt; 10 Hwy MPG than Pickup Trucks&quot;, subtitle = &quot;Comparing the median highway mpg in each class&quot;, caption = &quot;Data from fueleconomy.gov&quot;, x = &quot;Car Class&quot;, y = &quot;Highway Miles per Gallon&quot; ) Exercise 28.2.2 The geom_smooth() is somewhat misleading because the hwy for large engines is skewed upwards due to the inclusion of lightweight sports cars with big engines. Use your modeling tools to fit and display a better model. First, I’ll plot the relationship between fuel efficiency and engine size (displacement) using all cars. The plot shows a strong negative relationship. ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs( title = &quot;Fuel Efficiency Decreases with Engine Size&quot;, caption = &quot;Data from fueleconomy.gov&quot;, y = &quot;Highway Miles per Gallon&quot;, x = &quot;Engine Displacement&quot; ) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; However, if I disaggregate by car class, and plot the relationship between fuel efficiency and engine displacement within each class, I see a different relationship. For all car class except subcompact cars, there is no relationship or only a small negative relationship between fuel efficiency and engine size. For subcompact cars, there is a strong negative relationship between fuel efficiency and engine size. As the question noted, this is because the subcompact car class includes both small cheap cars, and sports cars with large engines. ggplot(mpg, aes(displ, hwy, colour = class)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs( title = &quot;Fuel Efficiency Mostly Varies by Car Class&quot;, subtitle = &quot;Subcompact caries fuel efficiency varies by engine size&quot;, caption = &quot;Data from fueleconomy.gov&quot;, y = &quot;Highway Miles per Gallon&quot;, x = &quot;Engine Displacement&quot; ) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; Another way to model and visualize the relationship between fuel efficiency and engine displacement after accounting for car class is to regress fuel efficiency on car class, and plot the residuals of that regression against engine displacement. The residuals of the first regression are the variation in fuel efficiency not explained by engine displacement. The relationship between fuel efficiency and engine displacement is attenuated after accounting for car class. mod &lt;- lm(hwy ~ class, data = mpg) mpg %&gt;% add_residuals(mod) %&gt;% ggplot(aes(x = displ, y = resid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs( title = &quot;Engine size has little effect on fuel efficiency&quot;, subtitle = &quot;After accounting for car class&quot;, caption = &quot;Data from fueleconomy.gov&quot;, y = &quot;Highway MPG Relative to Class Average&quot;, x = &quot;Engine Displacement&quot; ) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; Exercise 28.2.3 Take an exploratory graphic that you’ve created in the last month, and add informative titles to make it easier for others to understand. By its very nature, this exercise is left to readers. 28.3 Annotations Exercise 28.3.1 Use geom_text() with infinite positions to place text at the four corners of the plot. I can use similar code as the example in the text. However, I need to use vjust and hjust in order for the text to appear in the plot, and these need to be different for each corner. But, geom_text() takes hjust and vjust as aesthetics, I can add them to the data and mappings, and use a single geom_text() call instead of four different geom_text() calls with four different data arguments, and four different values of hjust and vjust arguments. label &lt;- tribble( ~displ, ~hwy, ~label, ~vjust, ~hjust, Inf, Inf, &quot;Top right&quot;, &quot;top&quot;, &quot;right&quot;, Inf, -Inf, &quot;Bottom right&quot;, &quot;bottom&quot;, &quot;right&quot;, -Inf, Inf, &quot;Top left&quot;, &quot;top&quot;, &quot;left&quot;, -Inf, -Inf, &quot;Bottom left&quot;, &quot;bottom&quot;, &quot;left&quot; ) ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_text(aes(label = label, vjust = vjust, hjust = hjust), data = label) Exercise 28.3.2 Read the documentation for annotate(). How can you use it to add a text label to a plot without having to create a tibble? With annotate you use what would be aesthetic mappings directly as arguments: ggplot(mpg, aes(displ, hwy)) + geom_point() + annotate(&quot;text&quot;, x = Inf, y = Inf, label = &quot;Increasing engine size is \\nrelated to decreasing fuel economy.&quot;, vjust = &quot;top&quot;, hjust = &quot;right&quot; ) Exercise 28.3.3 How do labels with geom_text() interact with faceting? How can you add a label to a single facet? How can you put a different label in each facet? (Hint: think about the underlying data.) If the facet variable is not specified, the text is drawn in all facets. label &lt;- tibble( displ = Inf, hwy = Inf, label = &quot;Increasing engine size is \\nrelated to decreasing fuel economy.&quot; ) ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_text(aes(label = label), data = label, vjust = &quot;top&quot;, hjust = &quot;right&quot;, size = 2 ) + facet_wrap(~class) To draw the label in only one facet, add a column to the label data frame with the value of the faceting variable(s) in which to draw it. label &lt;- tibble( displ = Inf, hwy = Inf, class = &quot;2seater&quot;, label = &quot;Increasing engine size is \\nrelated to decreasing fuel economy.&quot; ) ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_text(aes(label = label), data = label, vjust = &quot;top&quot;, hjust = &quot;right&quot;, size = 2 ) + facet_wrap(~class) To draw labels in different plots, simply have the facetting variable(s): label &lt;- tibble( displ = Inf, hwy = Inf, class = unique(mpg$class), label = str_c(&quot;Label for &quot;, class) ) ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_text(aes(label = label), data = label, vjust = &quot;top&quot;, hjust = &quot;right&quot;, size = 3 ) + facet_wrap(~class) Exercise 28.3.4 What arguments to geom_label() control the appearance of the background box? label.padding: padding around label label.r: amount of rounding in the corners label.size: size of label border Exercise 28.3.5 What are the four arguments to arrow()? How do they work? Create a series of plots that demonstrate the most important options. The four arguments are (from the help for arrow()): angle : angle of arrow head length : length of the arrow head ends: ends of the line to draw arrow head type: &quot;open&quot; or &quot;close&quot;: whether the arrow head is a closed or open triangle 28.4 Scales Exercise 28.4.1 Why doesn’t the following code override the default scale? df &lt;- tibble( x = rnorm(10000), y = rnorm(10000) ) ggplot(df, aes(x, y)) + geom_hex() + scale_colour_gradient(low = &quot;white&quot;, high = &quot;red&quot;) + coord_fixed() It does not override the default scale because the colors in geom_hex() are set by the fill aesthetic, not the color aesthetic. ggplot(df, aes(x, y)) + geom_hex() + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red&quot;) + coord_fixed() Exercise 28.4.2 The first argument to every scale is the label for the scale. It is equivalent to using the labs function. ggplot(mpg, aes(displ, hwy)) + geom_point(aes(colour = class)) + geom_smooth(se = FALSE) + labs( x = &quot;Engine displacement (L)&quot;, y = &quot;Highway fuel economy (mpg)&quot;, colour = &quot;Car type&quot; ) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(mpg, aes(displ, hwy)) + geom_point(aes(colour = class)) + geom_smooth(se = FALSE) + scale_x_continuous(&quot;Engine displacement (L)&quot;) + scale_y_continuous(&quot;Highway fuel economy (mpg)&quot;) + scale_colour_discrete(&quot;Car type&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Exercise 28.4.3 Change the display of the presidential terms by: Combining the two variants shown above. Improving the display of the y axis. Labeling each term with the name of the president. Adding informative plot labels. Placing breaks every 4 years (this is trickier than it seems!). fouryears &lt;- lubridate::make_date(seq(year(min(presidential$start)), year(max(presidential$end)), by = 4 ), 1, 1) presidential %&gt;% mutate( id = 33 + row_number(), name_id = fct_inorder(str_c(name, &quot; (&quot;, id, &quot;)&quot;)) ) %&gt;% ggplot(aes(start, name_id, colour = party)) + geom_point() + geom_segment(aes(xend = end, yend = name_id)) + scale_colour_manual(&quot;Party&quot;, values = c(Republican = &quot;red&quot;, Democratic = &quot;blue&quot;)) + scale_y_discrete(NULL) + scale_x_date(NULL, breaks = presidential$start, date_labels = &quot;&#39;%y&quot;, minor_breaks = fouryears ) + ggtitle(&quot;Terms of US Presdients&quot;, subtitle = &quot;Roosevelth (34th) to Obama (44th)&quot; ) + theme( panel.grid.minor = element_blank(), axis.ticks.y = element_blank() ) To include both the start dates of presidential terms and every four years, I use different levels of emphasis. The presidential term start years are used as major breaks with thicker lines and x-axis labels. Lines for every four years is indicated with minor breaks that use thinner lines to distinguish them from presidential term start years and to avoid cluttering the plot. Exercise 28.4.4 Use override.aes to make the legend on the following plot easier to see. ggplot(diamonds, aes(carat, price)) + geom_point(aes(colour = cut), alpha = 1 / 20) The problem with the legend is that the alpha value make the colors hard to see. So I’ll override the alpha value to make the points solid in the legend. ggplot(diamonds, aes(carat, price)) + geom_point(aes(colour = cut), alpha = 1 / 20) + theme(legend.position = &quot;bottom&quot;) + guides(colour = guide_legend(nrow = 1, override.aes = list(alpha = 1))) 28.5 Zooming No exercises 28.6 Themes No exercises 28.7 Saving your plots No exercises 28.8 Learning more No exercises "],
["r-markdown-formats.html", "29 R Markdown formats", " 29 R Markdown formats No exercises "],
["r-markdown-workflow.html", "30 R Markdown workflow", " 30 R Markdown workflow No exercises "],
["references.html", "References", " References "]
]
